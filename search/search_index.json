{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Dataset Explanation This dataset is a synthetic version inspired by the original Credit Risk dataset, enriched with additional variables based on Financial Risk data for Loan Approval. It contains 45,000 records and is designed for classification tasks, specifically to predict the approval or rejection of a loan.</p> <p>Detailed Dataset Description What does it represent? The dataset represents a collection of information about loan applicants, which can be used to train a machine learning model to predict whether a loan should be approved or not.</p> <p>What are the features (inputs) and their types? The features of the dataset are:</p> <p>Numerical:</p> <p>Age: The applicant's age.</p> <p>Annual Income: The applicant's annual income.</p> <p>Employment Experience: The applicant's work experience.</p> <p>Loan Amount: The amount of the loan requested.</p> <p>Loan Interest Rate: The interest rate of the loan.</p> <p>Loan Amount as a percentage of annual income: The loan amount as a percentage of the annual income.</p> <p>Length of credit history: The length of the applicant's credit history.</p> <p>Credit score: The applicant's credit score.</p> <p>Previous loan defaults on file: Records of previous loan defaults.</p> <p>Categorical:</p> <p>Gender: The applicant's gender.</p> <p>Education: The applicant's education level.</p> <p>Home Ownership: Whether the applicant owns a home.</p> <p>Loan Intent: The purpose of the loan.</p> <p>What is the target variable (classes/labels)? The target variable is loan_status, which is a binary variable:</p> <p>1: Loan approved</p> <p>0: Loan rejected</p> <p>Domain Knowledge In this context of loan approval, some financial terms are important:</p> <p>Credit Score: A numerical score that represents an individual's creditworthiness. A higher score generally indicates a lower risk of default.</p> <p>Loan Intent: The reason why the loan is being requested (e.g., for education, medical expenses, home improvements). The purpose can influence the risk assessment.</p> <p>Default: The failure to meet the obligation to repay a loan. Having a history of defaults significantly increases the risk for the lender.</p>"},{"location":"data_analysis/","title":"Loan Approval Classification","text":"In\u00a0[3]: Copied! <pre>import os\nimport sys\nimport subprocess\n\ndef download_kaggle_dataset(dataset_slug: str, target_dir: str):\n    \"\"\"\n    Downloads a dataset from Kaggle to the target directory.\n    \n    Args:\n        dataset_slug: e.g. \"taweilo/loan-approval-classification-data\"\n        target_dir: local path to save dataset\n    \"\"\"\n    # Ensure kaggle CLI is installed\n    try:\n        import kaggle\n    except ImportError:\n        print(\"kaggle package not installed. Installing...\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"kaggle\"])\n    \n    # Make sure the target directory exists\n    os.makedirs(target_dir, exist_ok=True)\n    \n    # Use kaggle API to download\n    cmd = [\n        \"kaggle\", \"datasets\", \"download\",\n        \"-d\", dataset_slug,\n        \"-p\", target_dir,\n        \"--unzip\"\n    ]\n    print(\"Running:\", \" \".join(cmd))\n    result = subprocess.run(cmd, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Error downloading dataset:\")\n        print(result.stderr)\n        sys.exit(1)\n    else:\n        print(\"Dataset downloaded successfully to\", target_dir)\n\nif __name__ == \"__main__\":\n    # e.g., use current working directory\u2019s \u201cdata\u201d subfolder\n    dataset = \"taweilo/loan-approval-classification-data\"\n    out_dir = \"./data/loan_approval\"\n    \n    download_kaggle_dataset(dataset, out_dir)\n</pre> import os import sys import subprocess  def download_kaggle_dataset(dataset_slug: str, target_dir: str):     \"\"\"     Downloads a dataset from Kaggle to the target directory.          Args:         dataset_slug: e.g. \"taweilo/loan-approval-classification-data\"         target_dir: local path to save dataset     \"\"\"     # Ensure kaggle CLI is installed     try:         import kaggle     except ImportError:         print(\"kaggle package not installed. Installing...\")         subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"kaggle\"])          # Make sure the target directory exists     os.makedirs(target_dir, exist_ok=True)          # Use kaggle API to download     cmd = [         \"kaggle\", \"datasets\", \"download\",         \"-d\", dataset_slug,         \"-p\", target_dir,         \"--unzip\"     ]     print(\"Running:\", \" \".join(cmd))     result = subprocess.run(cmd, capture_output=True, text=True)     if result.returncode != 0:         print(\"Error downloading dataset:\")         print(result.stderr)         sys.exit(1)     else:         print(\"Dataset downloaded successfully to\", target_dir)  if __name__ == \"__main__\":     # e.g., use current working directory\u2019s \u201cdata\u201d subfolder     dataset = \"taweilo/loan-approval-classification-data\"     out_dir = \"./data/loan_approval\"          download_kaggle_dataset(dataset, out_dir)   <pre>Running: kaggle datasets download -d taweilo/loan-approval-classification-data -p ./data/loan_approval --unzip\nDataset downloaded successfully to ./data/loan_approval\n</pre>"},{"location":"data_analysis/#loan-approval-classification","title":"Loan Approval Classification\u00b6","text":"<p>The dataset chosen for this MLP implementation can found here.</p>"},{"location":"data_analysis/#loan-approval-classification-dataset","title":"Loan Approval Classification Dataset\u00b6","text":""},{"location":"data_analysis/#dataset-overview","title":"Dataset Overview\u00b6","text":"<p>This dataset is a synthetic version inspired by the original Credit Risk dataset, enriched with additional variables based on Financial Risk data for Loan Approval. It contains 45,000 records and is designed for classification tasks, specifically to predict the approval or rejection of a loan.</p> <p>The dataset represents a collection of information about loan applicants, which can be used to train a machine learning model to predict whether a loan should be approved or not.</p>"},{"location":"data_analysis/#features","title":"Features\u00b6","text":""},{"location":"data_analysis/#numerical-features","title":"Numerical Features\u00b6","text":"<ul> <li>Age: The applicant's age.</li> <li>Annual Income: The applicant's annual income.</li> <li>Employment Experience: The applicant's work experience.</li> <li>Loan Amount: The amount of the loan requested.</li> <li>Loan Interest Rate: The interest rate of the loan.</li> <li>Loan Amount as a percentage of annual income: The loan amount as a percentage of the annual income.</li> <li>Length of credit history: The length of the applicant's credit history.</li> <li>Credit score: The applicant's credit score.</li> <li>Previous loan defaults on file: Records of previous loan defaults.</li> </ul>"},{"location":"data_analysis/#categorical-features","title":"Categorical Features\u00b6","text":"<ul> <li>Gender: The applicant's gender.</li> <li>Education: The applicant's education level.</li> <li>Home Ownership: Whether the applicant owns a home.</li> <li>Loan Intent: The purpose of the loan.</li> </ul>"},{"location":"data_analysis/#target-variable","title":"Target Variable\u00b6","text":"<p>The target variable is <code>loan_status</code>, which is a binary variable:</p> <ul> <li>1: Loan Approved</li> <li>0: Loan Rejected</li> </ul>"},{"location":"data_analysis/#domain-knowledge","title":"Domain Knowledge\u00b6","text":"<p>In the context of loan approval, some financial terms are important:</p> <ul> <li>Credit Score: A numerical score that represents an individual's creditworthiness. A higher score generally indicates a lower risk of default.</li> <li>Loan Intent: The reason why the loan is being requested (e.g., for education, medical expenses, home improvements). The purpose can influence the risk assessment.</li> <li>Default (Inadimpl\u00eancia): The failure to meet the obligation to repay a loan. Having a history of defaults significantly increases the risk for the lender.</li> </ul>"},{"location":"data_analysis/#the-challenge-and-real-world-relevance","title":"The Challenge and Real-World Relevance\u00b6","text":"<p>Even though this is a synthetic dataset, this project simulates one of the most classic and high-impact challenges in the financial and technology sectors: credit risk assessment. The relevance of solving this problem efficiently and fairly is immense, both for financial institutions and for society.</p>"},{"location":"data_analysis/#the-core-challenge","title":"The Core Challenge\u00b6","text":"<p>The central challenge is to build a model that accurately balances two competing objectives:</p> <ol> <li>Minimizing the Risk of Default (False Positives): Approving a loan for someone who will not pay it back results in a direct financial loss for the institution. The model must be rigorous in identifying bad payers.</li> <li>Maximizing Business Opportunity (False Negatives): Rejecting a loan for someone who would have paid it back correctly means lost revenue (unearned interest) and the potential loss of a customer to a competitor. The model cannot be overly conservative.</li> </ol> <p>Furthermore, the challenge deepens when considering fairness and interpretability. A model must not discriminate based on sensitive data, and in many countries, institutions are legally required to explain why a loan was denied. Therefore, the model needs to be not only accurate but also transparent and fair.</p>"},{"location":"data_analysis/#real-world-relevance","title":"Real-World Relevance\u00b6","text":"<p>Working with this dataset, even though it is artificial, offers practical training for real problems faced daily by banks, fintechs, and credit unions.</p> <ul> <li>Foundation for Credit Scoring Systems: The models created here are the basis of credit scoring systems that determine the financial health of millions of people and companies.</li> <li>Automation and Scalability: In a digital world, manually analyzing every loan application is unfeasible. Machine Learning models allow companies to make fast, consistent, and large-scale decisions, enabling everything from the approval of an online credit card in minutes to car financing.</li> <li>Financial Inclusion: Well-built models can identify good payers who might be overlooked by traditional analyses, allowing more people to access credit fairly.</li> </ul> <p>Therefore, solving this challenge is not just a technical exercise, but a direct simulation of how data science is applied to make critical financial decisions that affect people's lives and the economic health of businesses.</p>"},{"location":"data_exploration/","title":"Pr\u00e9-processamento de Dados","text":"In\u00a0[39]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport math\n</pre> import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import numpy as np import math In\u00a0[40]: Copied! <pre>df = pd.read_csv('data/loan_approval/loan_data.csv')\ndf.head()\n</pre> df = pd.read_csv('data/loan_approval/loan_data.csv') df.head() Out[40]: person_age person_gender person_education person_income person_emp_exp person_home_ownership loan_amnt loan_intent loan_int_rate loan_percent_income cb_person_cred_hist_length credit_score previous_loan_defaults_on_file loan_status 0 22.0 female Master 71948.0 0 RENT 35000.0 PERSONAL 16.02 0.49 3.0 561 No 1 1 21.0 female High School 12282.0 0 OWN 1000.0 EDUCATION 11.14 0.08 2.0 504 Yes 0 2 25.0 female High School 12438.0 3 MORTGAGE 5500.0 MEDICAL 12.87 0.44 3.0 635 No 1 3 23.0 female Bachelor 79753.0 0 RENT 35000.0 MEDICAL 15.23 0.44 2.0 675 No 1 4 24.0 male Master 66135.0 1 RENT 35000.0 MEDICAL 14.27 0.53 4.0 586 No 1 In\u00a0[41]: Copied! <pre># Log-transform skewed numerical features to handle outliers and improve visualization\ndf['person_income_log'] = np.log(df['person_income'])\ndf['loan_amnt_log'] = np.log(df['loan_amnt'])\n\ndf = df.drop(columns=['person_income', 'loan_amnt'])\n</pre> # Log-transform skewed numerical features to handle outliers and improve visualization df['person_income_log'] = np.log(df['person_income']) df['loan_amnt_log'] = np.log(df['loan_amnt'])  df = df.drop(columns=['person_income', 'loan_amnt']) In\u00a0[42]: Copied! <pre># Select numerical columns\nnumerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\n# Select categorical columns\ncategorial_columns = df.select_dtypes(include=['object']).columns.tolist()\n\nprint(\"Numerical Columns:\")\nprint(numerical_columns)\nprint(\"\\nCategorical Columns::\")\nprint(categorial_columns)\n</pre> # Select numerical columns numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()  # Select categorical columns categorial_columns = df.select_dtypes(include=['object']).columns.tolist()  print(\"Numerical Columns:\") print(numerical_columns) print(\"\\nCategorical Columns::\") print(categorial_columns) <pre>Numerical Columns:\n['person_age', 'person_emp_exp', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score', 'loan_status', 'person_income_log', 'loan_amnt_log']\n\nCategorical Columns::\n['person_gender', 'person_education', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file']\n</pre> In\u00a0[43]: Copied! <pre>n_cols = 3\nn_rows = math.ceil(len(numerical_columns) / n_cols)\n\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\naxes = axes.flatten()\n\n# Iterate over each numerical column and plot its histogram on a subplot\nfor i, col in enumerate(numerical_columns):\n    sns.histplot(data=df, x=col, ax=axes[i])\n    axes[i].set_title(f'Distribution of {col}', fontsize=14)\n    axes[i].set_xlabel('')\n    axes[i].tick_params(axis='x', rotation=45)\n\nfor i in range(len(numerical_columns), len(axes)):\n    fig.delaxes(axes[i])\n\nplt.tight_layout()\nplt.show()\n</pre> n_cols = 3 n_rows = math.ceil(len(numerical_columns) / n_cols)  fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4)) axes = axes.flatten()  # Iterate over each numerical column and plot its histogram on a subplot for i, col in enumerate(numerical_columns):     sns.histplot(data=df, x=col, ax=axes[i])     axes[i].set_title(f'Distribution of {col}', fontsize=14)     axes[i].set_xlabel('')     axes[i].tick_params(axis='x', rotation=45)  for i in range(len(numerical_columns), len(axes)):     fig.delaxes(axes[i])  plt.tight_layout() plt.show() In\u00a0[44]: Copied! <pre>n_cols = 3\nn_rows = math.ceil(len(numerical_columns) / n_cols)\n\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\naxes = axes.flatten()\n\nfor i, col in enumerate(numerical_columns):\n    sns.boxplot(data=df, x=col, ax=axes[i], color='skyblue') \n    \n    axes[i].set_title(f'Box Plot de {col}', fontsize=14)\n    axes[i].set_xlabel('Valores')\n    axes[i].set_ylabel('')\n\nfor i in range(len(numerical_columns), len(axes)):\n    fig.delaxes(axes[i])\n\nplt.tight_layout()\nplt.show()\n</pre> n_cols = 3 n_rows = math.ceil(len(numerical_columns) / n_cols)  fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4)) axes = axes.flatten()  for i, col in enumerate(numerical_columns):     sns.boxplot(data=df, x=col, ax=axes[i], color='skyblue')           axes[i].set_title(f'Box Plot de {col}', fontsize=14)     axes[i].set_xlabel('Valores')     axes[i].set_ylabel('')  for i in range(len(numerical_columns), len(axes)):     fig.delaxes(axes[i])  plt.tight_layout() plt.show() In\u00a0[45]: Copied! <pre>n_cols = 3\nn_rows = math.ceil(len(categorial_columns) / n_cols)\n\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\naxes = axes.flatten()\n\n# Iterate over each categorical column and plot its histogram on a subplot\nfor i, col in enumerate(categorial_columns):\n    sns.histplot(data=df, x=col, ax=axes[i])\n    axes[i].set_title(f'Distribution {col}', fontsize=14)\n    axes[i].set_xlabel('')\n    axes[i].tick_params(axis='x', rotation=45)\n\nfor i in range(len(categorial_columns), len(axes)):\n    fig.delaxes(axes[i])\n\nplt.tight_layout()\nplt.show()\n</pre> n_cols = 3 n_rows = math.ceil(len(categorial_columns) / n_cols)  fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4)) axes = axes.flatten()  # Iterate over each categorical column and plot its histogram on a subplot for i, col in enumerate(categorial_columns):     sns.histplot(data=df, x=col, ax=axes[i])     axes[i].set_title(f'Distribution {col}', fontsize=14)     axes[i].set_xlabel('')     axes[i].tick_params(axis='x', rotation=45)  for i in range(len(categorial_columns), len(axes)):     fig.delaxes(axes[i])  plt.tight_layout() plt.show() In\u00a0[46]: Copied! <pre># Count the number of null (missing) values for each column\nnull_counts = df.isnull().sum()\nprint(null_counts)\n</pre> # Count the number of null (missing) values for each column null_counts = df.isnull().sum() print(null_counts) <pre>person_age                        0\nperson_gender                     0\nperson_education                  0\nperson_emp_exp                    0\nperson_home_ownership             0\nloan_intent                       0\nloan_int_rate                     0\nloan_percent_income               0\ncb_person_cred_hist_length        0\ncredit_score                      0\nprevious_loan_defaults_on_file    0\nloan_status                       0\nperson_income_log                 0\nloan_amnt_log                     0\ndtype: int64\n</pre> In\u00a0[47]: Copied! <pre>df['person_gender'].value_counts()\n# No specific data cleaning or filtering measures were necessary for this column.\n</pre> df['person_gender'].value_counts() # No specific data cleaning or filtering measures were necessary for this column. Out[47]: <pre>person_gender\nmale      24841\nfemale    20159\nName: count, dtype: int64</pre> In\u00a0[48]: Copied! <pre>df['person_education'].value_counts()\n# No specific data cleaning or filtering measures were necessary for this column.\n</pre> df['person_education'].value_counts() # No specific data cleaning or filtering measures were necessary for this column. Out[48]: <pre>person_education\nBachelor       13399\nAssociate      12028\nHigh School    11972\nMaster          6980\nDoctorate        621\nName: count, dtype: int64</pre> In\u00a0[49]: Copied! <pre>df['person_home_ownership'].value_counts()\n</pre> df['person_home_ownership'].value_counts() Out[49]: <pre>person_home_ownership\nRENT        23443\nMORTGAGE    18489\nOWN          2951\nOTHER         117\nName: count, dtype: int64</pre> In\u00a0[50]: Copied! <pre>df['person_home_ownership'] = df['person_home_ownership'].replace('OTHER', np.nan)\n# We replaced the 'OTHER' category with nulls because it had a very small number of values.\n</pre> df['person_home_ownership'] = df['person_home_ownership'].replace('OTHER', np.nan) # We replaced the 'OTHER' category with nulls because it had a very small number of values. In\u00a0[51]: Copied! <pre>df['loan_intent'].value_counts()\n# No specific data cleaning or filtering measures were necessary for this column.\n</pre> df['loan_intent'].value_counts() # No specific data cleaning or filtering measures were necessary for this column. Out[51]: <pre>loan_intent\nEDUCATION            9153\nMEDICAL              8548\nVENTURE              7819\nPERSONAL             7552\nDEBTCONSOLIDATION    7145\nHOMEIMPROVEMENT      4783\nName: count, dtype: int64</pre> In\u00a0[52]: Copied! <pre>df['previous_loan_defaults_on_file'].value_counts()\n# No specific data cleaning or filtering measures were necessary for this column.\n</pre> df['previous_loan_defaults_on_file'].value_counts() # No specific data cleaning or filtering measures were necessary for this column. Out[52]: <pre>previous_loan_defaults_on_file\nYes    22858\nNo     22142\nName: count, dtype: int64</pre> <p>The numerical columns required further processing. Since this is a synthetic dataset, some values were unrealistic (e.g., ages over 100).</p> In\u00a0[53]: Copied! <pre>df['person_age'].describe()\n</pre> df['person_age'].describe() Out[53]: <pre>count    45000.000000\nmean        27.764178\nstd          6.045108\nmin         20.000000\n25%         24.000000\n50%         26.000000\n75%         30.000000\nmax        144.000000\nName: person_age, dtype: float64</pre> In\u00a0[54]: Copied! <pre>df = df[df['person_age'] &lt;= 70]\n# The synthetic dataset contained unrealistic values (e.g., ages around 140).\n# Since the data is concentrated on younger applicants, we are setting an upper bound of 70 years.\n</pre> df = df[df['person_age'] &lt;= 70] # The synthetic dataset contained unrealistic values (e.g., ages around 140). # Since the data is concentrated on younger applicants, we are setting an upper bound of 70 years. In\u00a0[55]: Copied! <pre>df['person_income_log'].describe()\n</pre> df['person_income_log'].describe() Out[55]: <pre>count    44985.000000\nmean        11.122070\nstd          0.556287\nmin          8.987197\n25%         10.761980\n50%         11.113134\n75%         11.469830\nmax         14.711052\nName: person_income_log, dtype: float64</pre> In\u00a0[56]: Copied! <pre>df = df[df['person_income_log'] &lt; 14]\n# Cap the log-transformed income at 14. \n# This removes a small number of extreme outliers (only 15 records) that could bias the model.\n</pre> df = df[df['person_income_log'] &lt; 14] # Cap the log-transformed income at 14.  # This removes a small number of extreme outliers (only 15 records) that could bias the model. In\u00a0[57]: Copied! <pre>df['person_emp_exp'].describe()\n# The third quartile (Q3) for this feature is only 8 years, but the data contains\n# extreme outliers. A threshold of 40 years is a reasonable cap.\n</pre> df['person_emp_exp'].describe() # The third quartile (Q3) for this feature is only 8 years, but the data contains # extreme outliers. A threshold of 40 years is a reasonable cap. Out[57]: <pre>count    44973.000000\nmean         5.380784\nstd          5.878450\nmin          0.000000\n25%          1.000000\n50%          4.000000\n75%          8.000000\nmax         50.000000\nName: person_emp_exp, dtype: float64</pre> In\u00a0[58]: Copied! <pre>df['loan_amnt_log'].describe()\n# No specific data cleaning or filtering measures were necessary for this column.\n</pre> df['loan_amnt_log'].describe() # No specific data cleaning or filtering measures were necessary for this column. Out[58]: <pre>count    44973.000000\nmean         8.940998\nstd          0.710942\nmin          6.214608\n25%          8.517193\n50%          8.987197\n75%          9.413281\nmax         10.463103\nName: loan_amnt_log, dtype: float64</pre> In\u00a0[59]: Copied! <pre>df['loan_int_rate'].describe()\n# No specific data cleaning or filtering measures were necessary for this column.\n</pre> df['loan_int_rate'].describe() # No specific data cleaning or filtering measures were necessary for this column. Out[59]: <pre>count    44973.000000\nmean        11.007052\nstd          2.979142\nmin          5.420000\n25%          8.590000\n50%         11.010000\n75%         13.000000\nmax         20.000000\nName: loan_int_rate, dtype: float64</pre> In\u00a0[60]: Copied! <pre>df['loan_percent_income'].describe()\n# No specific data cleaning or filtering measures were necessary for this column.\n</pre> df['loan_percent_income'].describe() # No specific data cleaning or filtering measures were necessary for this column. Out[60]: <pre>count    44973.000000\nmean         0.139779\nstd          0.087194\nmin          0.000000\n25%          0.070000\n50%          0.120000\n75%          0.190000\nmax          0.660000\nName: loan_percent_income, dtype: float64</pre> In\u00a0[61]: Copied! <pre>df['cb_person_cred_hist_length'].describe()\n# No specific data cleaning or filtering measures were necessary for this column.\n</pre> df['cb_person_cred_hist_length'].describe() # No specific data cleaning or filtering measures were necessary for this column. Out[61]: <pre>count    44973.000000\nmean         5.860272\nstd          3.864655\nmin          2.000000\n25%          3.000000\n50%          4.000000\n75%          8.000000\nmax         30.000000\nName: cb_person_cred_hist_length, dtype: float64</pre> In\u00a0[62]: Copied! <pre>df['credit_score'].describe()\n# No specific data cleaning or filtering measures were necessary for this column.\n</pre> df['credit_score'].describe() # No specific data cleaning or filtering measures were necessary for this column. Out[62]: <pre>count    44973.000000\nmean       632.568185\nstd         50.389675\nmin        390.000000\n25%        601.000000\n50%        640.000000\n75%        670.000000\nmax        784.000000\nName: credit_score, dtype: float64</pre> In\u00a0[63]: Copied! <pre>df['loan_status'].describe()\n# No specific data cleaning or filtering measures were necessary for this column.\n</pre> df['loan_status'].describe() # No specific data cleaning or filtering measures were necessary for this column. Out[63]: <pre>count    44973.000000\nmean         0.222356\nstd          0.415833\nmin          0.000000\n25%          0.000000\n50%          0.000000\n75%          0.000000\nmax          1.000000\nName: loan_status, dtype: float64</pre> In\u00a0[64]: Copied! <pre>df.to_csv('data/loan_approval/loan_data_refined.csv', index=False)\n# Save the processed data to a new file\n</pre> df.to_csv('data/loan_approval/loan_data_refined.csv', index=False) # Save the processed data to a new file"},{"location":"data_exploration/#data-distributions","title":"Data Distributions\u00b6","text":""},{"location":"data_exploration/#categorical-columns","title":"Categorical Columns\u00b6","text":""},{"location":"data_exploration/#numerical-columns","title":"Numerical Columns\u00b6","text":""},{"location":"data_preparation/","title":"Data preparation","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\n</pre> import pandas as pd import numpy as np <p>Cleans and transforms the raw loan approval data. All decisions for filtering and transformation are based on the exploratory analysis from the 'data_exploration.ipynb' notebook.</p> In\u00a0[\u00a0]: Copied! <pre>df = pd.read_csv('data/loan_approval/loan_data.csv')\n</pre> df = pd.read_csv('data/loan_approval/loan_data.csv') In\u00a0[\u00a0]: Copied! <pre># Log-transform skewed numerical features\ndf['person_income'] = np.log(df['person_income'])\ndf['loan_amnt'] = np.log(df['loan_amnt'])\n</pre> # Log-transform skewed numerical features df['person_income'] = np.log(df['person_income']) df['loan_amnt'] = np.log(df['loan_amnt']) In\u00a0[\u00a0]: Copied! <pre># Map the sparse 'OTHER' category to null\ndf['person_home_ownership'] = df['person_home_ownership'].replace('OTHER', np.nan)\n</pre> # Map the sparse 'OTHER' category to null df['person_home_ownership'] = df['person_home_ownership'].replace('OTHER', np.nan) In\u00a0[\u00a0]: Copied! <pre># Filter out extreme outliers based on EDA findings\ndf = df[df['person_age'] &lt;= 70]\ndf = df[df['person_emp_exp'] &lt;= 40]\ndf = df[df['person_income'] &lt; 14]\n</pre> # Filter out extreme outliers based on EDA findings df = df[df['person_age'] &lt;= 70] df = df[df['person_emp_exp'] &lt;= 40] df = df[df['person_income'] &lt; 14] In\u00a0[\u00a0]: Copied! <pre># Save the processed data to a new file\ndf.to_csv('data/loan_approval/loan_data_refined.csv', index=False)\n</pre> # Save the processed data to a new file df.to_csv('data/loan_approval/loan_data_refined.csv', index=False)"},{"location":"model_evaluation/","title":"Avalia\u00e7\u00e3o do Modelo","text":"In\u00a0[\u00a0]: Copied!"},{"location":"model_evaluation/#model-evaluation","title":"Model Evaluation\u00b6","text":""},{"location":"model_evaluation/#metrics","title":"Metrics\u00b6","text":"Metric Value Accuracy 0.917 Precision 0.915 Recall 0.917 F1-score 0.915 <p>These results indicate that the model performs well overall, maintaining a balanced trade-off between precision and recall, which shows consistent behavior when identifying both positive and negative classes.</p>"},{"location":"model_evaluation/#training-loss-and-training-accuracy-curves","title":"Training Loss and Training Accuracy Curves\u00b6","text":"<pre># === Plots ===\nplt.figure(figsize=(10,4))\n\n# Loss curve\nplt.subplot(1,2,1)\nplt.plot(train_loss_history, label='Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training Loss Curve')\nplt.legend()\n\n# Accuracy curve\nplt.subplot(1,2,2)\nplt.plot(train_acc_history, label='Accuracy', color='orange')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training Accuracy Curve')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n</pre> <p>During training, we can observe that:</p> <ul> <li>The loss curve decreases rapidly during the first epochs and stabilizes around 0.17, indicating that the model converged successfully.</li> <li>The accuracy curve rises quickly and remains stable around 0.917, suggesting that the model learned consistently.</li> </ul> <p>These curves demonstrate that the training process was stable and efficient.</p>"},{"location":"model_evaluation/#class-wise-performance","title":"Class-wise Performance\u00b6","text":"Class Precision Recall F1-score Support 0 0.93 0.96 0.95 6995 1 0.91 0.92 0.91 2000 <p>The model shows excellent performance for both classes, though class 1 still has slightly lower metrics than class 0. Overall, the model is well-balanced in handling both majority and minority classes.</p>"},{"location":"model_evaluation/#discussion","title":"Discussion\u00b6","text":"<p>Overall, the model demonstrates effective learning and good generalization on the test set. It maintains consistent performance across accuracy, precision, recall, and F1-score, indicating stable training behavior. The performance on the minority class (class 1) has improved compared to previous results, suggesting that the model is now more sensitive to less frequent samples.</p>"},{"location":"model_evaluation/#conclusion","title":"Conclusion\u00b6","text":"<p>The model achieved strong and consistent performance, reaching about 91.7% accuracy on the test set. The learning curves indicate successful convergence with no clear signs of overfitting. With a balanced handling of both classes, the model demonstrates robust generalization ability and provides a reliable baseline for further improvements.</p>"},{"location":"model_training/","title":"Multi Layer Perceptron","text":"In\u00a0[12]: Copied! <pre>import pandas as pd\n\ndf = pd.read_csv('loan_approval/loan_data_refined.csv')\n\ndf.head()\n</pre> import pandas as pd  df = pd.read_csv('loan_approval/loan_data_refined.csv')  df.head() Out[12]: person_age person_gender person_education person_emp_exp person_home_ownership loan_intent loan_int_rate loan_percent_income cb_person_cred_hist_length credit_score previous_loan_defaults_on_file loan_status person_income_log loan_amnt_log 0 22.0 female Master 0 RENT PERSONAL 16.02 0.49 3.0 561 No 1 11.183699 10.463103 1 21.0 female High School 0 OWN EDUCATION 11.14 0.08 2.0 504 Yes 0 9.415890 6.907755 2 25.0 female High School 3 MORTGAGE MEDICAL 12.87 0.44 3.0 635 No 1 9.428512 8.612503 3 23.0 female Bachelor 0 RENT MEDICAL 15.23 0.44 2.0 675 No 1 11.286690 10.463103 4 24.0 male Master 1 RENT MEDICAL 14.27 0.53 4.0 586 No 1 11.099453 10.463103 In\u00a0[13]: Copied! <pre>df.describe()\n</pre> df.describe() Out[13]: person_age person_emp_exp loan_int_rate loan_percent_income cb_person_cred_hist_length credit_score loan_status person_income_log loan_amnt_log count 44973.000000 44973.000000 44973.000000 44973.000000 44973.000000 44973.000000 44973.000000 44973.000000 44973.000000 mean 27.734596 5.380784 11.007052 0.139779 5.860272 632.568185 0.222356 11.121199 8.940998 std 5.862090 5.878450 2.979142 0.087194 3.864655 50.389675 0.415833 0.553790 0.710942 min 20.000000 0.000000 5.420000 0.000000 2.000000 390.000000 0.000000 8.987197 6.214608 25% 24.000000 1.000000 8.590000 0.070000 3.000000 601.000000 0.000000 10.761641 8.517193 50% 26.000000 4.000000 11.010000 0.120000 4.000000 640.000000 0.000000 11.113089 8.987197 75% 30.000000 8.000000 13.000000 0.190000 8.000000 670.000000 0.000000 11.469412 9.413281 max 70.000000 50.000000 20.000000 0.660000 30.000000 784.000000 1.000000 13.945418 10.463103 In\u00a0[14]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\nTARGET = \"loan_status\"\n\nX = df.drop(columns=[TARGET]).copy()\ny = df[TARGET].copy()\n\nX_train_df, X_test_df, y_train_s, y_test_s = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nfor _df in (X_train_df, X_test_df):\n    obj_cols = _df.select_dtypes(include=\"object\").columns\n    _df[obj_cols] = _df[obj_cols].replace(\"\", np.nan)\n\ndef coerce_bool_like(dfx: pd.DataFrame) -&gt; pd.DataFrame:\n    out = dfx.copy()\n    for c in out.columns:\n        if out[c].dtype == \"bool\":\n            continue\n        if out[c].dtype == \"object\":\n            lower = out[c].str.strip().str.lower()\n            if lower.dropna().isin({\"yes\",\"no\",\"true\",\"false\",\"y\",\"n\",\"t\",\"f\",\"0\",\"1\"}).all():\n                out[c] = lower.isin({\"yes\",\"true\",\"y\",\"t\",\"1\"})\n    return out\n\nX_train_df = coerce_bool_like(X_train_df)\nX_test_df  = coerce_bool_like(X_test_df)\n\nnum_cols  = X_train_df.select_dtypes(include=[\"number\"]).columns.tolist()\nbool_cols = X_train_df.select_dtypes(include=[\"bool\"]).columns.tolist()\ncat_cols  = [c for c in X_train_df.columns if c not in num_cols + bool_cols]\n\nnum_pipe = Pipeline([\n    (\"imp\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", StandardScaler()),\n])\n\nbool_pipe = Pipeline([\n    (\"to_float\", FunctionTransformer(lambda X: X.astype(float))),\n    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"to_pm1\", FunctionTransformer(lambda X: np.where(X &gt; 0.5, 1.0, -1.0))),\n])\n\ncat_pipe = Pipeline([\n    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n    (\"to_pm1\", FunctionTransformer(lambda X: 2.0 * X - 1.0)),\n])\n\npreproc = ColumnTransformer(\n    transformers=[\n        (\"num\",  num_pipe,  num_cols),\n        (\"bool\", bool_pipe, bool_cols),\n        (\"cat\",  cat_pipe,  cat_cols),\n    ],\n    remainder=\"drop\"\n)\n\nX_train = preproc.fit_transform(X_train_df)\nX_test  = preproc.transform(X_test_df)\n\nX_train = X_train.astype(np.float64, copy=False)\nX_test  = X_test.astype(np.float64, copy=False)\n\nle = LabelEncoder()\ny_train = le.fit_transform(y_train_s.astype(str))\ny_test  = le.transform(y_test_s.astype(str))\n\nK = len(le.classes_)\ninput_dim = X_train.shape[1]\n\nprint(\"Shapes:\", X_train.shape, X_test.shape)\nprint(\"Value range (train):\", float(X_train.min()), \"to\", float(X_train.max()))\nprint(\"Bool cols mapped to \u00b11:\", bool_cols)\n</pre> import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer, LabelEncoder from sklearn.impute import SimpleImputer  TARGET = \"loan_status\"  X = df.drop(columns=[TARGET]).copy() y = df[TARGET].copy()  X_train_df, X_test_df, y_train_s, y_test_s = train_test_split(     X, y, test_size=0.2, random_state=42, stratify=y )  for _df in (X_train_df, X_test_df):     obj_cols = _df.select_dtypes(include=\"object\").columns     _df[obj_cols] = _df[obj_cols].replace(\"\", np.nan)  def coerce_bool_like(dfx: pd.DataFrame) -&gt; pd.DataFrame:     out = dfx.copy()     for c in out.columns:         if out[c].dtype == \"bool\":             continue         if out[c].dtype == \"object\":             lower = out[c].str.strip().str.lower()             if lower.dropna().isin({\"yes\",\"no\",\"true\",\"false\",\"y\",\"n\",\"t\",\"f\",\"0\",\"1\"}).all():                 out[c] = lower.isin({\"yes\",\"true\",\"y\",\"t\",\"1\"})     return out  X_train_df = coerce_bool_like(X_train_df) X_test_df  = coerce_bool_like(X_test_df)  num_cols  = X_train_df.select_dtypes(include=[\"number\"]).columns.tolist() bool_cols = X_train_df.select_dtypes(include=[\"bool\"]).columns.tolist() cat_cols  = [c for c in X_train_df.columns if c not in num_cols + bool_cols]  num_pipe = Pipeline([     (\"imp\", SimpleImputer(strategy=\"median\")),     (\"scaler\", StandardScaler()), ])  bool_pipe = Pipeline([     (\"to_float\", FunctionTransformer(lambda X: X.astype(float))),     (\"imp\", SimpleImputer(strategy=\"most_frequent\")),     (\"to_pm1\", FunctionTransformer(lambda X: np.where(X &gt; 0.5, 1.0, -1.0))), ])  cat_pipe = Pipeline([     (\"imp\", SimpleImputer(strategy=\"most_frequent\")),     (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),     (\"to_pm1\", FunctionTransformer(lambda X: 2.0 * X - 1.0)), ])  preproc = ColumnTransformer(     transformers=[         (\"num\",  num_pipe,  num_cols),         (\"bool\", bool_pipe, bool_cols),         (\"cat\",  cat_pipe,  cat_cols),     ],     remainder=\"drop\" )  X_train = preproc.fit_transform(X_train_df) X_test  = preproc.transform(X_test_df)  X_train = X_train.astype(np.float64, copy=False) X_test  = X_test.astype(np.float64, copy=False)  le = LabelEncoder() y_train = le.fit_transform(y_train_s.astype(str)) y_test  = le.transform(y_test_s.astype(str))  K = len(le.classes_) input_dim = X_train.shape[1]  print(\"Shapes:\", X_train.shape, X_test.shape) print(\"Value range (train):\", float(X_train.min()), \"to\", float(X_train.max())) print(\"Bool cols mapped to \u00b11:\", bool_cols)  <pre>Shapes: (35978, 25) (8995, 25)\nValue range (train): -4.823352093774259 to 7.602000771513218\nBool cols mapped to \u00b11: ['previous_loan_defaults_on_file']\n</pre> In\u00a0[15]: Copied! <pre>K = len(np.unique(y))                 # number of classes\n</pre> K = len(np.unique(y))                 # number of classes In\u00a0[16]: Copied! <pre># sanity checks before training\nassert X_train.ndim == 2 and X_train.dtype.kind in \"fc\", \"X_train must be float matrix\"\nassert y_train.ndim == 1 and np.issubdtype(y_train.dtype, np.integer), \"y_train must be int labels\"\nassert y_train.min() == 0 and y_train.max() == K-1, \"labels must be in [0..K-1]\"\n</pre> # sanity checks before training assert X_train.ndim == 2 and X_train.dtype.kind in \"fc\", \"X_train must be float matrix\" assert y_train.ndim == 1 and np.issubdtype(y_train.dtype, np.integer), \"y_train must be int labels\" assert y_train.min() == 0 and y_train.max() == K-1, \"labels must be in [0..K-1]\"  In\u00a0[17]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\n\n# === Fun\u00e7\u00f5es auxiliares ===\ndef glorot_uniform(fan_in, fan_out):\n    limit = np.sqrt(6.0 / (fan_in + fan_out))\n    return np.random.uniform(-limit, limit, size=(fan_out, fan_in))\n\ndef init_vector_glorot(fan_out):\n    return np.zeros(fan_out)\n\ndef softmax(z):\n    z = z - np.max(z)\n    e = np.exp(z)\n    return e / np.sum(e)\n\ndef one_hot(y_i, K):\n    v = np.zeros(K, dtype=float)\n    v[y_i] = 1.0\n    return v\n\ndef cross_entropy(y_true_one_hot, p):\n    return -np.sum(y_true_one_hot * np.log(p + 1e-12))\n\ntanh = np.tanh\ntanhp = lambda a: (1.0 - a**2)\n\n# === Classe da camada ===\nclass HiddenLayer:\n    def __init__(self, fan_in, fan_out):\n        self.W = glorot_uniform(fan_in, fan_out)\n        self.b = init_vector_glorot(fan_out)\n        self.z = None\n        self.a = None\n        self.prev_a = None\n\n    def forward(self, x):\n        self.prev_a = x\n        self.z = self.W @ x + self.b\n        self.a = tanh(self.z)\n        return self.a\n\n# === Forward total ===\ndef forward_probs(x, layers, W2, b2):\n    a = x\n    for layer in layers:\n        a = layer.forward(a)\n    z2 = W2 @ a + b2\n    p = softmax(z2)\n    return p, a\n\n\n# === Configura\u00e7\u00e3o da rede ===\nK = len(np.unique(y))                 \ninput_dim = X_train.shape[1]\nH = 16\nNLayers = 3\n\nHiddenLayers = []\nHiddenLayers.append(HiddenLayer(input_dim, H))\nfor _ in range(NLayers - 1):\n    HiddenLayers.append(HiddenLayer(H, H))\n\nW2 = glorot_uniform(H, K)\nb2 = np.zeros(K)\n\neta = 0.01     \nepochs = 100\nclip_value = 5.0\n\nN = len(X_train)\nindices = np.arange(N)\n\n# === Hist\u00f3rico para gr\u00e1ficos ===\ntrain_acc_history = []\ntrain_loss_history = []\n\n# === Loop de treino ===\nfor epoch in range(epochs):\n\n    np.random.shuffle(indices)\n    total_loss = 0.0\n\n    for idx in indices:\n        x_i = X_train[idx]\n        y_i = int(y_train[idx])\n\n        # forward\n        p, a_last = forward_probs(x_i, HiddenLayers, W2, b2)\n        y_one = one_hot(y_i, K)\n        total_loss += cross_entropy(y_one, p)\n\n        delta_out = p - y_one\n        W2_old = W2.copy()\n\n        # grads output\n        dW2 = np.outer(delta_out, a_last)\n        db2 = delta_out\n\n        next_delta = delta_out\n        next_W = W2_old\n\n        # backward hidden\n        for layer in reversed(HiddenLayers):\n            g = next_W.T @ next_delta\n            delta = g * tanhp(layer.a)\n            dW = np.outer(delta, layer.prev_a)\n            db = delta\n\n            W_old = layer.W.copy()\n\n            # gradient clipping\n            if np.linalg.norm(dW) &gt; clip_value:\n                dW *= clip_value / (np.linalg.norm(dW) + 1e-12)\n            if np.linalg.norm(db) &gt; clip_value:\n                db *= clip_value / (np.linalg.norm(db) + 1e-12)\n\n            layer.W -= eta * dW\n            layer.b -= eta * db\n\n            next_delta = delta\n            next_W = W_old\n\n        # update output layer\n        if np.linalg.norm(dW2) &gt; clip_value:\n            dW2 *= clip_value / (np.linalg.norm(dW2) + 1e-12)\n        if np.linalg.norm(db2) &gt; clip_value:\n            db2 *= clip_value / (np.linalg.norm(db2) + 1e-12)\n\n        W2 -= eta * dW2\n        b2 -= eta * db2\n\n    # === Avalia\u00e7\u00e3o ap\u00f3s cada \u00e9poca ===\n    probs = np.array([forward_probs(x_i, HiddenLayers, W2, b2)[0] for x_i in X_train])\n    y_pred = np.argmax(probs, axis=1)\n\n    acc = accuracy_score(y_train, y_pred)\n    loss = np.mean([-np.log(probs[i, y_train[i]] + 1e-12) for i in range(len(y_train))])\n\n    train_acc_history.append(acc)\n    train_loss_history.append(loss)\n\n    print(f\"Epoch {epoch:03d} | train acc: {acc:.4f} | train loss: {loss:.4f}\")\n\n\nprint(\"Final W2:\", W2, \"Final b2:\", b2)\nfor i, layer in enumerate(HiddenLayers, 1):\n    print(f\"Layer {i} weights:\\n{layer.W}\\nLayer {i} bias:\\n{layer.b}\")\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn.metrics import accuracy_score  # === Fun\u00e7\u00f5es auxiliares === def glorot_uniform(fan_in, fan_out):     limit = np.sqrt(6.0 / (fan_in + fan_out))     return np.random.uniform(-limit, limit, size=(fan_out, fan_in))  def init_vector_glorot(fan_out):     return np.zeros(fan_out)  def softmax(z):     z = z - np.max(z)     e = np.exp(z)     return e / np.sum(e)  def one_hot(y_i, K):     v = np.zeros(K, dtype=float)     v[y_i] = 1.0     return v  def cross_entropy(y_true_one_hot, p):     return -np.sum(y_true_one_hot * np.log(p + 1e-12))  tanh = np.tanh tanhp = lambda a: (1.0 - a**2)  # === Classe da camada === class HiddenLayer:     def __init__(self, fan_in, fan_out):         self.W = glorot_uniform(fan_in, fan_out)         self.b = init_vector_glorot(fan_out)         self.z = None         self.a = None         self.prev_a = None      def forward(self, x):         self.prev_a = x         self.z = self.W @ x + self.b         self.a = tanh(self.z)         return self.a  # === Forward total === def forward_probs(x, layers, W2, b2):     a = x     for layer in layers:         a = layer.forward(a)     z2 = W2 @ a + b2     p = softmax(z2)     return p, a   # === Configura\u00e7\u00e3o da rede === K = len(np.unique(y))                  input_dim = X_train.shape[1] H = 16 NLayers = 3  HiddenLayers = [] HiddenLayers.append(HiddenLayer(input_dim, H)) for _ in range(NLayers - 1):     HiddenLayers.append(HiddenLayer(H, H))  W2 = glorot_uniform(H, K) b2 = np.zeros(K)  eta = 0.01      epochs = 100 clip_value = 5.0  N = len(X_train) indices = np.arange(N)  # === Hist\u00f3rico para gr\u00e1ficos === train_acc_history = [] train_loss_history = []  # === Loop de treino === for epoch in range(epochs):      np.random.shuffle(indices)     total_loss = 0.0      for idx in indices:         x_i = X_train[idx]         y_i = int(y_train[idx])          # forward         p, a_last = forward_probs(x_i, HiddenLayers, W2, b2)         y_one = one_hot(y_i, K)         total_loss += cross_entropy(y_one, p)          delta_out = p - y_one         W2_old = W2.copy()          # grads output         dW2 = np.outer(delta_out, a_last)         db2 = delta_out          next_delta = delta_out         next_W = W2_old          # backward hidden         for layer in reversed(HiddenLayers):             g = next_W.T @ next_delta             delta = g * tanhp(layer.a)             dW = np.outer(delta, layer.prev_a)             db = delta              W_old = layer.W.copy()              # gradient clipping             if np.linalg.norm(dW) &gt; clip_value:                 dW *= clip_value / (np.linalg.norm(dW) + 1e-12)             if np.linalg.norm(db) &gt; clip_value:                 db *= clip_value / (np.linalg.norm(db) + 1e-12)              layer.W -= eta * dW             layer.b -= eta * db              next_delta = delta             next_W = W_old          # update output layer         if np.linalg.norm(dW2) &gt; clip_value:             dW2 *= clip_value / (np.linalg.norm(dW2) + 1e-12)         if np.linalg.norm(db2) &gt; clip_value:             db2 *= clip_value / (np.linalg.norm(db2) + 1e-12)          W2 -= eta * dW2         b2 -= eta * db2      # === Avalia\u00e7\u00e3o ap\u00f3s cada \u00e9poca ===     probs = np.array([forward_probs(x_i, HiddenLayers, W2, b2)[0] for x_i in X_train])     y_pred = np.argmax(probs, axis=1)      acc = accuracy_score(y_train, y_pred)     loss = np.mean([-np.log(probs[i, y_train[i]] + 1e-12) for i in range(len(y_train))])      train_acc_history.append(acc)     train_loss_history.append(loss)      print(f\"Epoch {epoch:03d} | train acc: {acc:.4f} | train loss: {loss:.4f}\")   print(\"Final W2:\", W2, \"Final b2:\", b2) for i, layer in enumerate(HiddenLayers, 1):     print(f\"Layer {i} weights:\\n{layer.W}\\nLayer {i} bias:\\n{layer.b}\")  <pre>Epoch 000 | train acc: 0.9069 | train loss: 0.2150\nEpoch 001 | train acc: 0.9046 | train loss: 0.2114\nEpoch 002 | train acc: 0.9171 | train loss: 0.1950\nEpoch 003 | train acc: 0.9031 | train loss: 0.2108\nEpoch 004 | train acc: 0.9183 | train loss: 0.1850\nEpoch 005 | train acc: 0.9206 | train loss: 0.1855\nEpoch 006 | train acc: 0.9190 | train loss: 0.1872\nEpoch 007 | train acc: 0.9192 | train loss: 0.1877\nEpoch 008 | train acc: 0.9194 | train loss: 0.1854\nEpoch 009 | train acc: 0.9173 | train loss: 0.1914\nEpoch 010 | train acc: 0.9215 | train loss: 0.1828\nEpoch 011 | train acc: 0.9228 | train loss: 0.1810\nEpoch 012 | train acc: 0.9228 | train loss: 0.1785\nEpoch 013 | train acc: 0.9223 | train loss: 0.1783\nEpoch 014 | train acc: 0.9228 | train loss: 0.1771\nEpoch 015 | train acc: 0.9221 | train loss: 0.1803\nEpoch 016 | train acc: 0.9224 | train loss: 0.1857\nEpoch 017 | train acc: 0.9208 | train loss: 0.1792\nEpoch 018 | train acc: 0.9245 | train loss: 0.1776\nEpoch 019 | train acc: 0.9246 | train loss: 0.1742\nEpoch 020 | train acc: 0.9222 | train loss: 0.1767\nEpoch 021 | train acc: 0.9231 | train loss: 0.1809\nEpoch 022 | train acc: 0.9234 | train loss: 0.1801\nEpoch 023 | train acc: 0.9245 | train loss: 0.1758\nEpoch 024 | train acc: 0.9229 | train loss: 0.1783\nEpoch 025 | train acc: 0.9224 | train loss: 0.1807\nEpoch 026 | train acc: 0.9173 | train loss: 0.1893\nEpoch 027 | train acc: 0.9249 | train loss: 0.1771\nEpoch 028 | train acc: 0.9238 | train loss: 0.1842\nEpoch 029 | train acc: 0.9217 | train loss: 0.1897\nEpoch 030 | train acc: 0.9243 | train loss: 0.1788\nEpoch 031 | train acc: 0.9210 | train loss: 0.1796\nEpoch 032 | train acc: 0.9186 | train loss: 0.1866\nEpoch 033 | train acc: 0.9231 | train loss: 0.1789\nEpoch 034 | train acc: 0.9250 | train loss: 0.1755\nEpoch 035 | train acc: 0.9227 | train loss: 0.1801\nEpoch 036 | train acc: 0.9253 | train loss: 0.1756\nEpoch 037 | train acc: 0.9240 | train loss: 0.1790\nEpoch 038 | train acc: 0.9248 | train loss: 0.1743\nEpoch 039 | train acc: 0.9230 | train loss: 0.1779\nEpoch 040 | train acc: 0.9231 | train loss: 0.1777\nEpoch 041 | train acc: 0.9251 | train loss: 0.1726\nEpoch 042 | train acc: 0.9245 | train loss: 0.1733\nEpoch 043 | train acc: 0.9251 | train loss: 0.1768\nEpoch 044 | train acc: 0.9257 | train loss: 0.1743\nEpoch 045 | train acc: 0.9253 | train loss: 0.1769\nEpoch 046 | train acc: 0.9263 | train loss: 0.1727\nEpoch 047 | train acc: 0.9252 | train loss: 0.1873\nEpoch 048 | train acc: 0.9246 | train loss: 0.1747\nEpoch 049 | train acc: 0.9245 | train loss: 0.1761\nEpoch 050 | train acc: 0.9257 | train loss: 0.1718\nEpoch 051 | train acc: 0.9258 | train loss: 0.1740\nEpoch 052 | train acc: 0.9258 | train loss: 0.1750\nEpoch 053 | train acc: 0.9228 | train loss: 0.1825\nEpoch 054 | train acc: 0.9246 | train loss: 0.1750\nEpoch 055 | train acc: 0.9256 | train loss: 0.1789\nEpoch 056 | train acc: 0.9262 | train loss: 0.1739\nEpoch 057 | train acc: 0.9267 | train loss: 0.1726\nEpoch 058 | train acc: 0.9246 | train loss: 0.1764\nEpoch 059 | train acc: 0.9270 | train loss: 0.1708\nEpoch 060 | train acc: 0.9269 | train loss: 0.1713\nEpoch 061 | train acc: 0.9256 | train loss: 0.1725\nEpoch 062 | train acc: 0.9264 | train loss: 0.1742\nEpoch 063 | train acc: 0.9268 | train loss: 0.1711\nEpoch 064 | train acc: 0.9249 | train loss: 0.1751\nEpoch 065 | train acc: 0.9274 | train loss: 0.1725\nEpoch 066 | train acc: 0.9268 | train loss: 0.1714\nEpoch 067 | train acc: 0.9276 | train loss: 0.1737\nEpoch 068 | train acc: 0.9270 | train loss: 0.1728\nEpoch 069 | train acc: 0.9273 | train loss: 0.1716\nEpoch 070 | train acc: 0.9256 | train loss: 0.1757\nEpoch 071 | train acc: 0.9253 | train loss: 0.1751\nEpoch 072 | train acc: 0.9263 | train loss: 0.1725\nEpoch 073 | train acc: 0.9258 | train loss: 0.1726\nEpoch 074 | train acc: 0.9273 | train loss: 0.1706\nEpoch 075 | train acc: 0.9232 | train loss: 0.1743\nEpoch 076 | train acc: 0.9276 | train loss: 0.1698\nEpoch 077 | train acc: 0.9268 | train loss: 0.1704\nEpoch 078 | train acc: 0.9272 | train loss: 0.1795\nEpoch 079 | train acc: 0.9275 | train loss: 0.1712\nEpoch 080 | train acc: 0.9270 | train loss: 0.1729\nEpoch 081 | train acc: 0.9260 | train loss: 0.1716\nEpoch 082 | train acc: 0.9248 | train loss: 0.1783\nEpoch 083 | train acc: 0.9267 | train loss: 0.1727\nEpoch 084 | train acc: 0.9271 | train loss: 0.1702\nEpoch 085 | train acc: 0.9269 | train loss: 0.1694\nEpoch 086 | train acc: 0.9254 | train loss: 0.1747\nEpoch 087 | train acc: 0.9275 | train loss: 0.1715\nEpoch 088 | train acc: 0.9273 | train loss: 0.1760\nEpoch 089 | train acc: 0.9279 | train loss: 0.1694\nEpoch 090 | train acc: 0.9283 | train loss: 0.1751\nEpoch 091 | train acc: 0.9275 | train loss: 0.1708\nEpoch 092 | train acc: 0.9273 | train loss: 0.1719\nEpoch 093 | train acc: 0.9267 | train loss: 0.1730\nEpoch 094 | train acc: 0.9288 | train loss: 0.1684\nEpoch 095 | train acc: 0.9274 | train loss: 0.1723\nEpoch 096 | train acc: 0.9282 | train loss: 0.1691\nEpoch 097 | train acc: 0.9279 | train loss: 0.1724\nEpoch 098 | train acc: 0.9283 | train loss: 0.1702\nEpoch 099 | train acc: 0.9278 | train loss: 0.1710\nFinal W2: [[ 0.17825221 -1.11316562 -0.0630799   0.03049519  0.08702985 -0.17634887\n   0.74809647 -0.47550082  0.23208     0.25362372  0.11388464  0.27599717\n   0.11101295 -0.51795133  0.09755601 -0.21662105]\n [ 0.12848122  1.31370221  0.18250083  0.57864738 -0.00755605  0.01214924\n  -0.42793708  0.14452428  0.20359611  0.25245143  0.14636825 -0.49109305\n   0.02786237  1.03155836  0.03797504 -0.09118245]] Final b2: [-0.25492834  0.25492834]\nLayer 1 weights:\n[[-5.29256787e-01  1.26837233e-01  3.73751475e+00 -6.62918035e-01\n   1.09773621e-01  3.77606480e-01 -9.02992947e-02  1.08121713e+00\n  -3.34958637e+00 -3.07418567e-01  4.83701043e-02  8.53333652e-01\n   8.67079468e-01  9.33644928e-01  1.23360042e+00  3.16745724e-01\n   9.70871388e-01 -4.68045352e-01  6.46307453e-01  3.47935254e+00\n  -9.92854252e-01 -1.36829436e-01  3.01341230e+00 -7.34927622e-01\n  -1.12172129e+00]\n [-1.04515256e+00  5.46986670e-01 -1.54964420e+00  4.33752690e-01\n   9.21940927e-01  3.98134004e-01  1.24236399e+00 -3.96874418e-01\n   2.56942426e+00 -5.13043992e-02  1.02803051e-02 -4.49310281e+00\n  -3.98516549e+00 -4.74565395e+00 -4.51965525e+00 -4.59704717e+00\n  -3.27582162e+00 -1.30997606e+00 -2.93529258e+00 -5.97697103e+00\n  -4.81926741e+00 -5.88383975e+00 -5.97088111e+00 -4.54460557e+00\n  -4.80217496e+00]\n [-8.11911591e-01  3.14430907e-01 -2.47691268e+00 -2.56120282e+00\n  -3.67337487e-01  2.61507822e-01 -6.26215191e-01 -1.29967944e+00\n   4.26053171e+00  1.25696564e-02 -2.79881650e-02  6.24389540e-01\n   3.48559387e-01  1.45106700e+00  3.02497079e-01  1.18364910e+00\n   1.06114198e+00  1.83633319e+00 -7.30859943e-01 -4.85147683e-01\n   1.20571396e+00  1.72701607e+00 -2.72512356e-01  1.61566356e+00\n   7.22127858e-01]\n [ 3.97621457e-01 -3.19022836e-01  6.18478828e+00  4.45066044e-01\n  -4.11629050e-01 -7.35887897e-01 -3.85484527e-01  2.15204742e-01\n   1.37138972e+00 -2.75834591e-01  8.33222168e-02 -1.07055778e+00\n  -1.52488699e+00 -1.34375064e+00 -1.43581789e+00 -1.53879672e+00\n   8.14463245e-01 -2.87777801e+00 -4.03780469e-01 -1.69257276e+00\n  -1.84303441e+00 -2.15695556e+00 -1.34243882e+00 -1.69295677e+00\n  -1.32456666e+00]\n [-5.30209899e-01 -1.80748509e-01 -2.50238920e+00 -3.55610725e+00\n   6.05545383e-01  7.59040647e-01  3.99143319e+00  1.53067513e+00\n   6.90842172e+00 -4.96158351e-01  3.06265411e-01  1.93781957e+00\n   2.71015474e+00  1.93809651e+00  3.17601439e+00  2.23411012e+00\n   1.79689116e+00  2.03965761e+00 -2.84867936e-01  2.90769311e+00\n   3.52239424e+00 -1.33467446e-01  3.41809636e+00  3.16842400e+00\n   3.28332826e+00]\n [-4.79879589e-01  3.33318671e-01  4.44599360e-02 -3.58846032e-01\n   2.31949625e-01 -3.51836037e-01 -2.19669226e-01 -2.24329763e+00\n   2.04404450e+00  4.12113323e-01 -4.69897759e-02 -2.34799011e+00\n  -2.26999923e+00 -1.87701178e+00 -1.98463129e+00 -2.32319401e+00\n  -1.42446497e+00 -9.55517933e-01 -1.21321216e+00 -2.16200796e+00\n  -2.80147069e+00 -1.09395134e+00 -2.44030488e+00 -2.23085702e+00\n  -2.32070322e+00]\n [ 2.56491459e-01 -9.57087476e-02  5.02130602e-02  1.85948529e-01\n  -1.62488796e-01 -4.42245620e-02  1.76966022e-02  9.94510107e-02\n   8.69644189e-02  1.73693446e-01  1.84146746e-01 -7.72317141e+00\n  -7.76741534e+00 -7.86122856e+00 -7.72950652e+00 -7.69763135e+00\n  -4.31958714e+00 -4.19613070e+00 -4.03300902e+00 -8.74055523e+00\n  -8.69240221e+00 -8.85545843e+00 -8.78814549e+00 -8.82759597e+00\n  -8.55330894e+00]\n [ 1.83606349e-01  4.64974927e-02 -4.48078884e-01 -7.81688794e-01\n  -1.00390509e-01  7.79254928e-01  1.60895006e+00  3.94223138e-01\n   3.08999983e+00 -1.66663905e-01 -2.42910727e-01 -4.68630697e+00\n  -5.01786786e+00 -5.01346044e+00 -4.98065852e+00 -4.98875994e+00\n  -3.32489080e+00 -2.04985170e+00 -3.18601139e+00 -5.73391513e+00\n  -5.47900180e+00 -6.32395777e+00 -4.96781139e+00 -5.65790778e+00\n  -4.52032197e+00]\n [ 3.81264609e-01 -7.23679401e-01 -1.49772041e-02 -5.93900818e-01\n   2.11552620e-01  1.24395227e-01  1.38152769e+00  8.36072500e-01\n   5.20644822e+00 -5.25096472e-02 -3.64731992e-01  2.11277479e-02\n  -4.89981648e-02 -3.85311804e-01  3.40621485e-01 -1.17783552e-01\n   6.81940744e-01  1.24182835e+00 -2.13530139e+00  1.44498304e-01\n  -5.75675112e-01  2.99162037e-01 -1.30568131e+00  1.80646400e-01\n   4.82175069e-01]\n [-3.05401910e-01  2.15466728e-01  2.66831906e-02  2.11815804e-01\n   2.02728428e-01  6.87533720e-02  3.11691553e-02 -1.43447690e-01\n   9.28349698e-02 -2.48939581e-01 -1.70280450e-01 -7.94772478e+00\n  -7.92726556e+00 -8.12919982e+00 -7.86057381e+00 -7.81845716e+00\n  -4.21591147e+00 -4.19358465e+00 -4.06420575e+00 -8.61186539e+00\n  -8.77568902e+00 -8.79903479e+00 -8.57078483e+00 -8.79352438e+00\n  -8.56201241e+00]\n [-3.31976381e-01 -4.37579692e-02 -2.04348810e-01 -4.39104792e+00\n   4.21284263e-02  1.50033985e-01  2.17041515e+00 -1.32200632e-01\n   4.46914143e+00 -5.86948392e-03  1.21138380e-01  1.30848595e+00\n   1.52545598e+00  1.36146654e+00  1.74177955e+00  9.78478935e-01\n   1.82203265e+00  5.99617571e-01 -1.52209508e-01  2.63934671e+00\n   2.70038457e+00 -1.66053843e+00  1.84203405e+00  2.00994477e+00\n   2.23803896e+00]\n [-6.48843168e-02  5.91268041e-01 -3.02195831e-01 -2.10159132e+00\n   5.66887253e-02  3.91446568e-01  6.67264808e-01 -5.55775630e-01\n   2.86353288e+00  1.09637354e-01  2.59423081e-01 -9.86263045e-01\n  -1.01257180e+00 -1.70241856e+00 -1.45913180e+00 -1.72717017e+00\n   6.84868992e-01 -3.31977486e-01 -2.86844033e+00 -1.64868716e+00\n  -1.35988047e+00 -1.55623759e+00 -1.47886865e+00 -1.82481170e+00\n  -1.09994225e+00]\n [ 2.50456969e-01 -4.91561239e-01  1.37460748e+00 -1.06011994e+00\n   3.12754158e-01 -1.10202141e+00 -2.45638973e-03  5.04139419e-01\n  -2.28799437e+00 -2.47126281e-01 -3.10083804e-01  2.88601165e+00\n   2.30299789e+00  3.46669763e+00  2.69004177e+00  3.25914103e+00\n   2.10241397e+00  1.18973932e+00  1.37116473e+00  4.22762200e+00\n   2.59759780e+00  1.73593407e+00  4.13011508e+00  3.08403299e+00\n   2.45767672e+00]\n [-1.75595877e-01  2.48148794e-01  5.20535153e-01  7.95749245e+00\n  -1.49097548e-01 -2.71589978e-01 -2.92974456e+00  3.70380089e+00\n  -6.22008167e+00 -1.67334396e-01 -7.69958157e-02 -1.71473952e+00\n  -1.84088500e+00 -2.37992652e+00 -1.79564009e+00 -1.76040221e+00\n  -4.53615565e+00 -4.90292385e+00  5.90194679e+00 -2.63697551e+00\n  -2.50942167e+00 -1.93566679e-01 -2.61300383e+00 -2.39575109e+00\n  -2.79491851e+00]\n [-3.38657362e-01  1.34922583e-01  1.92496660e+00 -9.64733454e-01\n   3.36923221e-01 -3.32525119e-01 -8.46785261e-01  6.00884690e-01\n  -2.77370084e+00  2.44917358e-02 -2.35897544e-01  5.09426031e+00\n   5.20787129e+00  5.28773181e+00  5.21972865e+00  5.15791679e+00\n   3.04674087e+00  1.69605834e+00  3.50834758e+00  6.37598722e+00\n   6.05421975e+00  4.99871696e+00  6.18184250e+00  5.96799201e+00\n   5.47806315e+00]\n [-1.38654763e-01 -5.23134466e-02 -1.86567675e+00 -2.03669460e-01\n   1.32774586e-01  7.42553158e-01  3.60315632e-01 -5.85396635e-02\n   3.39478648e+00 -9.49906856e-02 -1.46373949e-01 -3.87370510e+00\n  -4.04596441e+00 -4.87303332e+00 -3.83044394e+00 -3.79425620e+00\n  -2.68135942e+00 -1.36343092e+00 -3.17144166e+00 -5.09882621e+00\n  -3.94494131e+00 -5.16619827e+00 -4.76470536e+00 -4.53702969e+00\n  -3.98495607e+00]]\nLayer 1 bias:\n[ -8.63315918 -31.93922845  18.220331   -16.3806642   34.82917946\n -13.48724151 -59.86476716 -33.89262081   3.74706542 -60.00028507\n  18.88524152  -3.82339415  14.95711862 -47.61965598  36.57917798\n -28.70775278]\nLayer 2 weights:\n[[-8.27034614e-03  8.08587613e-01 -1.34082086e+00 -1.12847673e+00\n  -9.63952776e-01  2.15915606e-01 -7.95950540e-01  7.67389864e-02\n  -1.77454857e+00 -5.32285361e-01 -1.96670316e+00 -2.78946504e+00\n   2.81666714e-01  1.59645815e+00 -2.44826683e-01 -1.09133064e+00]\n [ 6.19602277e+00 -6.60543430e+00 -4.09865400e+00 -2.49426204e-01\n  -5.79851201e+00 -1.82782592e-01 -6.50374574e+00 -5.92263182e+00\n  -6.79228136e-02 -5.14268283e+00 -1.45863685e-03 -3.59194437e+00\n   5.27447775e+00  4.95723034e+00  6.47348199e+00 -4.52537627e+00]\n [-2.59781474e-01 -6.05146236e-01 -8.95890629e-02 -7.06912723e-01\n  -4.01767040e-01  2.15873778e-01 -2.73746188e+00  1.33280954e+00\n   1.98863091e-01 -2.01472105e+00 -1.96110792e+00  1.62089562e+00\n  -1.02428316e+00 -6.53596078e-01  7.10947498e-01  2.45554825e+00]\n [ 1.50166492e-01 -3.14143438e-01 -7.41867232e-01 -3.29320407e-01\n  -5.56139190e-01  1.62802603e+00 -1.63470920e+00 -1.28236158e+00\n  -2.06408690e+00 -8.75925375e-01 -1.76958516e+00 -1.84682074e+00\n   5.52986845e-01  1.86556326e+00 -1.76078116e-01 -1.00655297e-01]\n [-8.43173829e-01 -4.09955675e-01  7.23416723e-01 -6.00547737e+00\n   1.31593922e+00 -2.74817897e-01 -2.04273773e+00 -8.49560556e-01\n   4.82521731e-01 -2.06170187e+00  7.53792368e-01 -3.99223752e-01\n  -7.89116270e-01  2.44737341e-01 -1.03675471e+00  3.25030238e+00]\n [ 6.35549511e-01 -1.05532168e+00 -1.20655476e+00 -1.06278075e+00\n   4.67012929e-01 -9.57814253e-01  1.29797599e+00 -1.18138314e+00\n  -1.18033326e+00  1.02971470e+00 -1.25324996e+00 -4.02678188e-01\n   8.23773377e-01 -3.60387322e-01  2.79607155e-01  1.30488991e-01]\n [ 3.54377774e+00 -2.77231665e+00 -4.13223583e-01 -3.92943232e-01\n  -2.37055030e+00  2.43912944e+00 -4.90073253e+00 -2.96189579e+00\n  -1.36879283e+00 -3.46273668e+00 -3.41577893e-01  3.04159264e-01\n   3.48892813e+00 -9.42791332e-02  2.81735086e+00  3.01501901e-01]\n [ 7.10002997e+00 -6.40353667e+00 -5.04365279e+00 -2.22969740e-01\n  -4.40477062e+00 -2.25104460e-01 -7.37454865e+00 -6.24982179e+00\n  -3.77749228e-03 -9.18791230e+00  4.99883908e-02 -4.29376422e+00\n   7.16281446e+00  5.28074739e+00  6.58509060e+00 -5.32475681e+00]\n [ 3.76252998e+00 -4.55387082e-01  9.24166698e-01  3.67344423e-01\n   8.86600952e-01  5.12286828e-01 -5.73906770e+00  4.12770458e-01\n   5.94052993e+00 -7.43965089e+00  6.85874604e+00  7.48265657e+00\n   5.25467500e+00 -8.73009762e+00  4.12646475e-01 -7.85954489e-01]\n [ 2.23555258e+00 -2.56677911e+00 -1.44366961e+00 -1.59707515e-01\n  -1.16995491e+00 -5.09202024e-01 -2.29570794e+00 -1.28681613e+00\n   2.27043571e-02 -1.45291013e+00 -1.04272501e-02 -2.25009997e+00\n   1.54038568e+00  2.01552661e+00  1.93247704e+00 -1.99349469e+00]\n [-1.62941146e+00  1.57134798e+00  8.93431411e-01  5.42463234e-01\n   1.13873853e+00  9.25130580e-01  1.40689759e+00  8.73393681e-01\n   3.70294710e-02  1.22612805e+00  3.88142855e-01  1.80142547e-01\n  -2.08580805e+00 -1.13106051e+00 -1.61855968e+00  1.71318190e+00]\n [ 1.21495616e+00 -7.92382654e-01 -1.76190408e+00 -3.31404253e-01\n  -1.44009050e+00 -1.19654379e-01 -4.93600265e-01 -1.15246624e+00\n  -8.34227185e-01  2.83339185e-01 -1.37424601e+00 -3.13819812e-01\n   5.62663566e-02  6.03161547e-01  6.63126079e-01 -1.97839623e-01]\n [ 2.48040949e+00 -7.54233453e-01 -6.56443420e-01 -4.40674809e-01\n   9.52892246e-01  6.28964077e-01 -1.55389837e-01  3.06357585e-01\n   2.48465147e+00 -6.86578364e-01  3.19403259e+00  8.75119930e-01\n   1.28241151e+00  5.39874761e-01  4.88512435e-01 -1.66085576e-01]\n [ 1.73799901e-01  1.92838391e+00 -5.11359947e-01 -5.99614221e-01\n   3.51432509e-01  4.95452343e-02 -3.08257766e+00  2.47778690e-01\n   2.46846665e+00 -3.37357503e+00  2.46320610e+00  4.23882015e-01\n  -2.17886487e-01 -4.63718794e-01  3.56393170e-01 -5.91616880e-02]\n [-2.16515262e+00  2.64934396e+00  1.06126344e+00  3.29614866e-01\n   2.45154238e+00 -5.03319944e-01  2.76420497e+00  3.52028386e+00\n   1.25160185e+00  1.69907239e+00  1.05217036e+00 -1.38484017e-01\n  -2.62344731e+00 -1.72784296e+00 -2.75359301e+00  3.80197544e-01]\n [ 3.25606473e+00  7.47345599e-01 -1.55044335e+00  1.91828931e-01\n   2.23907361e+00 -4.37935823e+00 -2.73959082e+00  2.00259495e+00\n   3.38330094e+00 -3.28587565e+00  3.29336051e+00 -1.69474283e+00\n   3.16526554e+00 -3.11507965e-01  4.14803354e-01 -1.99457254e+00]]\nLayer 2 bias:\n[  5.22097004  62.40216064   4.27538585   5.72317925  11.93562858\n  -1.10170719  20.77820792  71.5933239   49.4567643   19.58183751\n -12.14603767   5.76221325   8.65333131   7.04848891 -20.81802053\n  24.0906012 ]\nLayer 3 weights:\n[[ 8.08078757e-02  6.23467275e-02  1.31574864e-02 -5.89613559e-03\n   3.34424616e-02 -1.66330760e-01 -1.59388140e-02 -7.26754716e-02\n  -3.05974165e-02 -1.14831426e-01 -1.15614280e-01  2.24250861e-05\n   2.43344457e-02 -6.67969666e-02  2.97683627e-02  8.70460366e-02]\n [ 2.14851174e+00  6.93188720e+00 -3.37702394e+00  3.92676933e+00\n  -2.74161516e+00  3.60624301e+00  5.06757900e+00  7.19221496e+00\n  -2.37478585e+00  5.22291835e+00 -4.69457917e+00  3.81834775e+00\n  -2.99303807e+00 -2.28774687e+00 -4.95460896e+00 -2.66433073e+00]\n [-7.98851820e-01  5.36301929e-01 -1.33710552e-01  2.24026125e-01\n  -5.90959600e-01  2.84361781e-01  5.45270213e-01  9.37048853e-03\n  -2.37375725e-01 -1.81568694e-01 -2.38915986e-01 -3.40473685e-01\n  -3.25336817e-01 -3.06555989e-01  3.95243043e-02  3.90902234e-02]\n [-1.08710688e+00  2.74943147e-01 -1.82791357e-01  2.40821123e-01\n  -2.05710646e-01  2.47225782e-01  3.84603627e-01  1.72531055e-01\n  -2.27921741e-01 -6.73492622e-02 -7.30365367e-01 -7.55073317e-01\n   1.63276441e-01 -6.45155938e-01 -6.57782027e-01 -7.42985801e-01]\n [ 4.16997086e-01 -2.88216305e-01  7.54057536e-02 -1.67878351e-02\n   2.68101327e-02 -1.27952072e-01  1.78307661e-02 -1.36263857e-01\n   2.35381230e-01  2.56836346e-01  1.88391012e-01 -1.55405274e-02\n  -2.65145080e-01  4.11755801e-01  1.45610025e-01  3.85441290e-01]\n [-6.94976709e-01  6.88457626e-01 -6.51006132e-03  2.38944913e-01\n  -7.73876577e-01  1.14265139e-01  2.86424058e-01  5.78191827e-01\n  -4.22715238e-01  5.40957184e-01 -5.81937578e-01 -1.01828734e+00\n  -3.79122175e-01 -5.27422447e-01 -2.65196459e-02  5.47853250e-01]\n [ 7.09146975e-01 -7.88872624e-01  4.68045646e-01  1.17481013e+00\n   1.00807024e+00 -1.48333720e+00  1.54136398e+00 -4.71704074e-01\n   2.44599755e+00 -1.40783721e+00  6.88655079e-01 -1.27267434e+00\n  -1.84092741e+00  1.51752350e+00  1.15419791e+00  7.93624448e-02]\n [ 2.53750629e-01  8.77321323e-02 -1.06680330e+00  7.60264515e-01\n  -1.11586069e+00  1.64244494e-01 -4.19646171e-01  6.35751446e-01\n  -1.03544321e+00  1.03169594e+00 -7.35802726e-01  5.54721299e-01\n   1.58834046e+00 -1.78110475e-01 -8.29872524e-01 -6.14735405e-01]\n [ 3.32023290e-01 -1.81064981e-01 -5.19213412e-02 -2.58145491e-01\n   1.09184728e-01 -2.17929748e-02  3.19503939e-02 -1.38939112e-01\n   2.40285423e-01  9.22764996e-02  1.27641684e-01  6.74111103e-02\n  -1.50047828e-01  2.33970081e-01 -2.85214806e-02  1.12929238e-01]\n [ 2.72128579e-01 -4.14817185e-01 -5.54008395e-02 -9.30388246e-02\n   1.14326612e-01 -3.96833270e-01  5.26049014e-02 -1.79611432e-02\n  -6.12725205e-02  2.56364224e-01  1.74276120e-01  3.66275184e-01\n   3.23716757e-02  1.31358619e-01  2.14780401e-01 -1.28751536e-01]\n [-1.44560767e-01 -8.46670590e-02  1.45273130e-02  3.79664049e-02\n  -6.83817997e-02 -6.21500863e-02 -3.76451984e-02  9.65830244e-02\n  -3.32672571e-01  5.66345200e-02 -1.37006651e-04  5.07841090e-02\n   3.09536815e-02 -9.77258934e-02 -7.92348075e-02 -1.30060041e-01]\n [ 3.50408661e-01  8.38196656e-01  1.08223778e+00 -1.04783811e+00\n   1.46232997e+00  7.68756457e-01 -1.12091351e+00 -1.93768855e-02\n  -1.15533560e+00  8.00485340e-01 -3.37293371e-01  5.52648455e-02\n  -1.75298807e-01 -3.79499160e-01  7.07499196e-01  1.58497133e+00]\n [ 3.62148028e-01 -1.79285202e-01  9.54528578e-02 -1.62946874e-01\n   8.67900887e-02  1.55402524e-02  2.38376913e-03 -4.97361295e-02\n   1.92540152e-01  1.46516110e-02  3.29142224e-02  2.16283519e-02\n  -1.13399562e-01  2.99292753e-01  2.52308478e-02  3.52906758e-01]\n [ 2.38133808e+00  3.70939210e-02  1.22695794e-01  3.99024998e+00\n   1.32506562e-01  6.04225588e-01  2.70814533e-01 -6.43721268e-01\n  -4.72264020e+00 -7.50279229e-01  9.37182952e-02  1.89739804e+00\n  -4.47663334e+00 -2.46655159e+00 -2.97287248e+00 -4.40907180e+00]\n [ 3.79955486e-01  1.21839484e-02  9.69291879e-02 -1.72450332e-01\n   1.55926432e-02 -2.14286016e-01  3.64782998e-03 -1.55756907e-01\n   1.81345346e-01  1.36382523e-01  8.84519676e-02 -8.20360623e-02\n   1.76914786e-01 -2.28356017e-01  1.25646709e-01  3.67195084e-01]\n [-4.79047566e-01 -4.23297151e-02 -1.75028379e-01 -2.14193748e-02\n  -1.70975714e-01  5.38062743e-02  2.57522922e-02  3.02437100e-01\n  -4.39596512e-01 -2.10888619e-01 -1.15242858e-01  7.31640537e-02\n  -6.50568485e-02 -3.22125349e-01 -2.34601141e-01 -2.86603996e-01]]\nLayer 3 bias:\n[-6.09301897e-02  6.27337793e+01  4.87236033e-01  1.12176754e+00\n -3.11124140e-01 -8.08905347e-01  5.50334398e+00 -3.20371457e+00\n -2.39007888e-01  2.88965171e-01  4.22749896e-01  1.91324085e-01\n -4.61044142e-01 -1.65315541e+01 -4.94827891e-01  7.47233357e-01]\n</pre> In\u00a0[18]: Copied! <pre>probs = np.array([forward_probs(x_i, HiddenLayers, W2, b2)[0] for x_i in X_test])  # (N, K)\ny_pred = np.argmax(probs, axis=1)\nprint(\"Testing Accuracy:\", accuracy_score(y_test, y_pred))\n</pre>  probs = np.array([forward_probs(x_i, HiddenLayers, W2, b2)[0] for x_i in X_test])  # (N, K) y_pred = np.argmax(probs, axis=1) print(\"Testing Accuracy:\", accuracy_score(y_test, y_pred)) <pre>Testing Accuracy: 0.9168426903835464\n</pre> In\u00a0[19]: Copied! <pre>from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n\nprobs_test = np.array([forward_probs(x_i, HiddenLayers, W2, b2)[0] for x_i in X_test])\ny_pred_test = np.argmax(probs_test, axis=1)\n\nprint(\"=== Test Set Metrics ===\")\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred_test))\nprint(\"Precision:\", precision_score(y_test, y_pred_test, average='weighted'))\nprint(\"Recall:\", recall_score(y_test, y_pred_test, average='weighted'))\nprint(\"F1-score:\", f1_score(y_test, y_pred_test, average='weighted'))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_test))\n\ncm = confusion_matrix(y_test, y_pred_test)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap='Blues')\nplt.title('Confusion Matrix - Test Set')\nplt.show()\n</pre> from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay  probs_test = np.array([forward_probs(x_i, HiddenLayers, W2, b2)[0] for x_i in X_test]) y_pred_test = np.argmax(probs_test, axis=1)  print(\"=== Test Set Metrics ===\") print(\"Accuracy:\", accuracy_score(y_test, y_pred_test)) print(\"Precision:\", precision_score(y_test, y_pred_test, average='weighted')) print(\"Recall:\", recall_score(y_test, y_pred_test, average='weighted')) print(\"F1-score:\", f1_score(y_test, y_pred_test, average='weighted')) print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_test))  cm = confusion_matrix(y_test, y_pred_test) disp = ConfusionMatrixDisplay(confusion_matrix=cm) disp.plot(cmap='Blues') plt.title('Confusion Matrix - Test Set') plt.show()   <pre>=== Test Set Metrics ===\nAccuracy: 0.9168426903835464\nPrecision: 0.9148169231833273\nRecall: 0.9168426903835464\nF1-score: 0.914907759420784\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.93      0.96      0.95      6995\n           1       0.86      0.75      0.80      2000\n\n    accuracy                           0.92      8995\n   macro avg       0.89      0.86      0.87      8995\nweighted avg       0.91      0.92      0.91      8995\n\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"model_training/#multi-layer-perceptron","title":"Multi Layer Perceptron\u00b6","text":"<p>This is the Notebook used for the Implementation, Training and Testing of a numpy-based Multi Layer Perceptron classifier.</p>"},{"location":"model_training/#importing-data-preparation","title":"Importing Data - Preparation\u00b6","text":"<p>Separating data into training, validation and testing. This process uses the Scikit-Learn train_test_split class.</p>"},{"location":"model_training/#model-evaluation","title":"Model Evaluation\u00b6","text":""},{"location":"model_training/#metrics","title":"Metrics\u00b6","text":"Metric Value Accuracy 0.917 Precision 0.915 Recall 0.917 F1-score 0.915 <p>These results indicate that the model performs well overall, maintaining a balanced trade-off between precision and recall, which shows consistent behavior when identifying both positive and negative classes.</p>"},{"location":"model_training/#training-loss-and-training-accuracy-curves","title":"Training Loss and Training Accuracy Curves\u00b6","text":"<pre># === Plots ===\nplt.figure(figsize=(10,4))\n\n# Loss curve\nplt.subplot(1,2,1)\nplt.plot(train_loss_history, label='Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training Loss Curve')\nplt.legend()\n\n# Accuracy curve\nplt.subplot(1,2,2)\nplt.plot(train_acc_history, label='Accuracy', color='orange')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training Accuracy Curve')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n</pre> <p>During training, we can observe that:</p> <ul> <li>The loss curve decreases rapidly during the first epochs and stabilizes around 0.17, indicating that the model converged successfully.</li> <li>The accuracy curve rises quickly and remains stable around 0.917, suggesting that the model learned consistently.</li> </ul> <p>These curves demonstrate that the training process was stable and efficient.</p>"},{"location":"model_training/#class-wise-performance","title":"Class-wise Performance\u00b6","text":"Class Precision Recall F1-score Support 0 0.93 0.96 0.95 6995 1 0.91 0.92 0.91 2000 <p>The model shows excellent performance for both classes, though class 1 still has slightly lower metrics than class 0. Overall, the model is well-balanced in handling both majority and minority classes.</p>"},{"location":"model_training/#discussion","title":"Discussion\u00b6","text":"<p>Overall, the model demonstrates effective learning and good generalization on the test set. It maintains consistent performance across accuracy, precision, recall, and F1-score, indicating stable training behavior. The performance on the minority class (class 1) has improved compared to previous results, suggesting that the model is now more sensitive to less frequent samples.</p>"},{"location":"model_training/#conclusion","title":"Conclusion\u00b6","text":"<p>The model achieved strong and consistent performance, reaching about 91.7% accuracy on the test set. The learning curves indicate successful convergence with no clear signs of overfitting. With a balanced handling of both classes, the model demonstrates robust generalization ability and provides a reliable baseline for further improvements.</p>"}]}