{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Dataset Explanation This dataset is a synthetic version inspired by the original Credit Risk dataset, enriched with additional variables based on Financial Risk data for Loan Approval. It contains 45,000 records and is designed for classification tasks, specifically to predict the approval or rejection of a loan.</p> <p>Detailed Dataset Description What does it represent? The dataset represents a collection of information about loan applicants, which can be used to train a machine learning model to predict whether a loan should be approved or not.</p> <p>What are the features (inputs) and their types? The features of the dataset are:</p> <p>Numerical:</p> <p>Age: The applicant's age.</p> <p>Annual Income: The applicant's annual income.</p> <p>Employment Experience: The applicant's work experience.</p> <p>Loan Amount: The amount of the loan requested.</p> <p>Loan Interest Rate: The interest rate of the loan.</p> <p>Loan Amount as a percentage of annual income: The loan amount as a percentage of the annual income.</p> <p>Length of credit history: The length of the applicant's credit history.</p> <p>Credit score: The applicant's credit score.</p> <p>Previous loan defaults on file: Records of previous loan defaults.</p> <p>Categorical:</p> <p>Gender: The applicant's gender.</p> <p>Education: The applicant's education level.</p> <p>Home Ownership: Whether the applicant owns a home.</p> <p>Loan Intent: The purpose of the loan.</p> <p>What is the target variable (classes/labels)? The target variable is loan_status, which is a binary variable:</p> <p>1: Loan approved</p> <p>0: Loan rejected</p> <p>Domain Knowledge In this context of loan approval, some financial terms are important:</p> <p>Credit Score: A numerical score that represents an individual's creditworthiness. A higher score generally indicates a lower risk of default.</p> <p>Loan Intent: The reason why the loan is being requested (e.g., for education, medical expenses, home improvements). The purpose can influence the risk assessment.</p> <p>Default: The failure to meet the obligation to repay a loan. Having a history of defaults significantly increases the risk for the lender.</p>"},{"location":"data_analysis/","title":"Loan Approval Classification","text":"In\u00a0[3]: Copied! <pre>import os\nimport sys\nimport subprocess\n\ndef download_kaggle_dataset(dataset_slug: str, target_dir: str):\n    \"\"\"\n    Downloads a dataset from Kaggle to the target directory.\n    \n    Args:\n        dataset_slug: e.g. \"taweilo/loan-approval-classification-data\"\n        target_dir: local path to save dataset\n    \"\"\"\n    # Ensure kaggle CLI is installed\n    try:\n        import kaggle\n    except ImportError:\n        print(\"kaggle package not installed. Installing...\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"kaggle\"])\n    \n    # Make sure the target directory exists\n    os.makedirs(target_dir, exist_ok=True)\n    \n    # Use kaggle API to download\n    cmd = [\n        \"kaggle\", \"datasets\", \"download\",\n        \"-d\", dataset_slug,\n        \"-p\", target_dir,\n        \"--unzip\"\n    ]\n    print(\"Running:\", \" \".join(cmd))\n    result = subprocess.run(cmd, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Error downloading dataset:\")\n        print(result.stderr)\n        sys.exit(1)\n    else:\n        print(\"Dataset downloaded successfully to\", target_dir)\n\nif __name__ == \"__main__\":\n    # e.g., use current working directory\u2019s \u201cdata\u201d subfolder\n    dataset = \"taweilo/loan-approval-classification-data\"\n    out_dir = \"./data/loan_approval\"\n    \n    download_kaggle_dataset(dataset, out_dir)\n</pre> import os import sys import subprocess  def download_kaggle_dataset(dataset_slug: str, target_dir: str):     \"\"\"     Downloads a dataset from Kaggle to the target directory.          Args:         dataset_slug: e.g. \"taweilo/loan-approval-classification-data\"         target_dir: local path to save dataset     \"\"\"     # Ensure kaggle CLI is installed     try:         import kaggle     except ImportError:         print(\"kaggle package not installed. Installing...\")         subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"kaggle\"])          # Make sure the target directory exists     os.makedirs(target_dir, exist_ok=True)          # Use kaggle API to download     cmd = [         \"kaggle\", \"datasets\", \"download\",         \"-d\", dataset_slug,         \"-p\", target_dir,         \"--unzip\"     ]     print(\"Running:\", \" \".join(cmd))     result = subprocess.run(cmd, capture_output=True, text=True)     if result.returncode != 0:         print(\"Error downloading dataset:\")         print(result.stderr)         sys.exit(1)     else:         print(\"Dataset downloaded successfully to\", target_dir)  if __name__ == \"__main__\":     # e.g., use current working directory\u2019s \u201cdata\u201d subfolder     dataset = \"taweilo/loan-approval-classification-data\"     out_dir = \"./data/loan_approval\"          download_kaggle_dataset(dataset, out_dir)   <pre>Running: kaggle datasets download -d taweilo/loan-approval-classification-data -p ./data/loan_approval --unzip\nDataset downloaded successfully to ./data/loan_approval\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"data_analysis/#loan-approval-classification","title":"Loan Approval Classification\u00b6","text":"<p>The dataset chosen for this MLP implementation can found here.</p>"},{"location":"data_exploration/","title":"Pr\u00e9-processamento de Dados","text":"In\u00a0[165]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport math\n</pre> import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import numpy as np import math In\u00a0[166]: Copied! <pre>df = pd.read_csv('data/loan_approval/loan_data.csv')\ndf.head()\n</pre> df = pd.read_csv('data/loan_approval/loan_data.csv') df.head() Out[166]: person_age person_gender person_education person_income person_emp_exp person_home_ownership loan_amnt loan_intent loan_int_rate loan_percent_income cb_person_cred_hist_length credit_score previous_loan_defaults_on_file loan_status 0 22.0 female Master 71948.0 0 RENT 35000.0 PERSONAL 16.02 0.49 3.0 561 No 1 1 21.0 female High School 12282.0 0 OWN 1000.0 EDUCATION 11.14 0.08 2.0 504 Yes 0 2 25.0 female High School 12438.0 3 MORTGAGE 5500.0 MEDICAL 12.87 0.44 3.0 635 No 1 3 23.0 female Bachelor 79753.0 0 RENT 35000.0 MEDICAL 15.23 0.44 2.0 675 No 1 4 24.0 male Master 66135.0 1 RENT 35000.0 MEDICAL 14.27 0.53 4.0 586 No 1 In\u00a0[167]: Copied! <pre># Log-transform skewed numerical features to handle outliers and improve visualization\ndf['person_income_log'] = np.log(df['person_income'])\ndf['loan_amnt_log'] = np.log(df['loan_amnt'])\n\ndf = df.drop(columns=['person_income', 'loan_amnt'])\n</pre> # Log-transform skewed numerical features to handle outliers and improve visualization df['person_income_log'] = np.log(df['person_income']) df['loan_amnt_log'] = np.log(df['loan_amnt'])  df = df.drop(columns=['person_income', 'loan_amnt']) In\u00a0[168]: Copied! <pre># Select numerical columns\nnumerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\n# Select categorical columns\ncategorial_columns = df.select_dtypes(include=['object']).columns.tolist()\n\nprint(\"Numerical Columns:\")\nprint(numerical_columns)\nprint(\"\\nCategorical Columns::\")\nprint(categorial_columns)\n</pre> # Select numerical columns numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()  # Select categorical columns categorial_columns = df.select_dtypes(include=['object']).columns.tolist()  print(\"Numerical Columns:\") print(numerical_columns) print(\"\\nCategorical Columns::\") print(categorial_columns) <pre>Numerical Columns:\n['person_age', 'person_emp_exp', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length', 'credit_score', 'loan_status', 'person_income_log', 'loan_amnt_log']\n\nCategorical Columns::\n['person_gender', 'person_education', 'person_home_ownership', 'loan_intent', 'previous_loan_defaults_on_file']\n</pre> In\u00a0[169]: Copied! <pre>n_cols = 3\nn_rows = math.ceil(len(numerical_columns) / n_cols)\n\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\naxes = axes.flatten()\n\n# Iterate over each numerical column and plot its histogram on a subplot\nfor i, col in enumerate(numerical_columns):\n    sns.histplot(data=df, x=col, ax=axes[i])\n    axes[i].set_title(f'Distribution of {col}', fontsize=14)\n    axes[i].set_xlabel('')\n    axes[i].tick_params(axis='x', rotation=45)\n\nfor i in range(len(numerical_columns), len(axes)):\n    fig.delaxes(axes[i])\n\nplt.tight_layout()\nplt.show()\n</pre> n_cols = 3 n_rows = math.ceil(len(numerical_columns) / n_cols)  fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4)) axes = axes.flatten()  # Iterate over each numerical column and plot its histogram on a subplot for i, col in enumerate(numerical_columns):     sns.histplot(data=df, x=col, ax=axes[i])     axes[i].set_title(f'Distribution of {col}', fontsize=14)     axes[i].set_xlabel('')     axes[i].tick_params(axis='x', rotation=45)  for i in range(len(numerical_columns), len(axes)):     fig.delaxes(axes[i])  plt.tight_layout() plt.show() In\u00a0[170]: Copied! <pre>n_cols = 3\nn_rows = math.ceil(len(numerical_columns) / n_cols)\n\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\naxes = axes.flatten()\n\nfor i, col in enumerate(numerical_columns):\n    sns.boxplot(data=df, x=col, ax=axes[i], color='skyblue') \n    \n    axes[i].set_title(f'Box Plot de {col}', fontsize=14)\n    axes[i].set_xlabel('Valores')\n    axes[i].set_ylabel('')\n\nfor i in range(len(numerical_columns), len(axes)):\n    fig.delaxes(axes[i])\n\nplt.tight_layout()\nplt.show()\n</pre> n_cols = 3 n_rows = math.ceil(len(numerical_columns) / n_cols)  fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4)) axes = axes.flatten()  for i, col in enumerate(numerical_columns):     sns.boxplot(data=df, x=col, ax=axes[i], color='skyblue')           axes[i].set_title(f'Box Plot de {col}', fontsize=14)     axes[i].set_xlabel('Valores')     axes[i].set_ylabel('')  for i in range(len(numerical_columns), len(axes)):     fig.delaxes(axes[i])  plt.tight_layout() plt.show() In\u00a0[171]: Copied! <pre>n_cols = 3\nn_rows = math.ceil(len(categorial_columns) / n_cols)\n\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\naxes = axes.flatten()\n\n# Iterate over each categorical column and plot its histogram on a subplot\nfor i, col in enumerate(categorial_columns):\n    sns.histplot(data=df, x=col, ax=axes[i])\n    axes[i].set_title(f'Distribution {col}', fontsize=14)\n    axes[i].set_xlabel('')\n    axes[i].tick_params(axis='x', rotation=45)\n\nfor i in range(len(categorial_columns), len(axes)):\n    fig.delaxes(axes[i])\n\nplt.tight_layout()\nplt.show()\n</pre> n_cols = 3 n_rows = math.ceil(len(categorial_columns) / n_cols)  fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4)) axes = axes.flatten()  # Iterate over each categorical column and plot its histogram on a subplot for i, col in enumerate(categorial_columns):     sns.histplot(data=df, x=col, ax=axes[i])     axes[i].set_title(f'Distribution {col}', fontsize=14)     axes[i].set_xlabel('')     axes[i].tick_params(axis='x', rotation=45)  for i in range(len(categorial_columns), len(axes)):     fig.delaxes(axes[i])  plt.tight_layout() plt.show() In\u00a0[172]: Copied! <pre># Count the number of null (missing) values for each column\nnull_counts = df.isnull().sum()\nprint(null_counts)\n</pre> # Count the number of null (missing) values for each column null_counts = df.isnull().sum() print(null_counts) <pre>person_age                        0\nperson_gender                     0\nperson_education                  0\nperson_emp_exp                    0\nperson_home_ownership             0\nloan_intent                       0\nloan_int_rate                     0\nloan_percent_income               0\ncb_person_cred_hist_length        0\ncredit_score                      0\nprevious_loan_defaults_on_file    0\nloan_status                       0\nperson_income_log                 0\nloan_amnt_log                     0\ndtype: int64\n</pre> In\u00a0[173]: Copied! <pre>df['person_gender'].value_counts()\n# No specific data cleaning or filtering measures were necessary for this column.\n</pre> df['person_gender'].value_counts() # No specific data cleaning or filtering measures were necessary for this column. Out[173]: <pre>person_gender\nmale      24841\nfemale    20159\nName: count, dtype: int64</pre> In\u00a0[174]: Copied! <pre>df['person_education'].value_counts()\n# No specific data cleaning or filtering measures were necessary for this column.\n</pre> df['person_education'].value_counts() # No specific data cleaning or filtering measures were necessary for this column. Out[174]: <pre>person_education\nBachelor       13399\nAssociate      12028\nHigh School    11972\nMaster          6980\nDoctorate        621\nName: count, dtype: int64</pre> In\u00a0[175]: Copied! <pre>df['person_home_ownership'].value_counts()\n# We replaced the 'OTHER' category with nulls because it had a very small number of values.\n</pre> df['person_home_ownership'].value_counts() # We replaced the 'OTHER' category with nulls because it had a very small number of values. Out[175]: <pre>person_home_ownership\nRENT        23443\nMORTGAGE    18489\nOWN          2951\nOTHER         117\nName: count, dtype: int64</pre> In\u00a0[176]: Copied! <pre>df['loan_intent'].value_counts()\n# No specific data cleaning or filtering measures were necessary for this column.\n</pre> df['loan_intent'].value_counts() # No specific data cleaning or filtering measures were necessary for this column. Out[176]: <pre>loan_intent\nEDUCATION            9153\nMEDICAL              8548\nVENTURE              7819\nPERSONAL             7552\nDEBTCONSOLIDATION    7145\nHOMEIMPROVEMENT      4783\nName: count, dtype: int64</pre> In\u00a0[177]: Copied! <pre>df['previous_loan_defaults_on_file'].value_counts()\n# No specific data cleaning or filtering measures were necessary for this column.\n</pre> df['previous_loan_defaults_on_file'].value_counts() # No specific data cleaning or filtering measures were necessary for this column. Out[177]: <pre>previous_loan_defaults_on_file\nYes    22858\nNo     22142\nName: count, dtype: int64</pre> <p>The numerical columns required further processing. Since this is a synthetic dataset, some values were unrealistic (e.g., ages over 100).</p> In\u00a0[178]: Copied! <pre>df['person_age'].describe()\n# The synthetic dataset contained unrealistic values (e.g., ages around 140).\n# Since the data is concentrated on younger applicants, we are setting an upper bound of 70 years.\n</pre> df['person_age'].describe() # The synthetic dataset contained unrealistic values (e.g., ages around 140). # Since the data is concentrated on younger applicants, we are setting an upper bound of 70 years. Out[178]: <pre>count    45000.000000\nmean        27.764178\nstd          6.045108\nmin         20.000000\n25%         24.000000\n50%         26.000000\n75%         30.000000\nmax        144.000000\nName: person_age, dtype: float64</pre> In\u00a0[179]: Copied! <pre>df['person_income'].describe()\n# Cap the log-transformed income at 14. \n# This removes a small number of extreme outliers (only 15 records) that could bias the model.\n</pre> df['person_income'].describe() # Cap the log-transformed income at 14.  # This removes a small number of extreme outliers (only 15 records) that could bias the model. <pre>\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile c:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\mlp-project\\env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812, in Index.get_loc(self, key)\n   3811 try:\n-&gt; 3812     return self._engine.get_loc(casted_key)\n   3813 except KeyError as err:\n\nFile pandas/_libs/index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n\nFile pandas/_libs/index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n\nFile pandas/_libs/hashtable_class_helper.pxi:7088, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nFile pandas/_libs/hashtable_class_helper.pxi:7096, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nKeyError: 'person_income'\n\nThe above exception was the direct cause of the following exception:\n\nKeyError                                  Traceback (most recent call last)\nCell In[179], line 1\n----&gt; 1 df['person_income'].describe()\n      2 # Cap the log-transformed income at 14. \n      3 # This removes a small number of extreme outliers (only 15 records) that could bias the model.\n\nFile c:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\mlp-project\\env\\Lib\\site-packages\\pandas\\core\\frame.py:4107, in DataFrame.__getitem__(self, key)\n   4105 if self.columns.nlevels &gt; 1:\n   4106     return self._getitem_multilevel(key)\n-&gt; 4107 indexer = self.columns.get_loc(key)\n   4108 if is_integer(indexer):\n   4109     indexer = [indexer]\n\nFile c:\\Users\\erikb\\OneDrive\\\u00c1rea de Trabalho\\mlp-project\\env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819, in Index.get_loc(self, key)\n   3814     if isinstance(casted_key, slice) or (\n   3815         isinstance(casted_key, abc.Iterable)\n   3816         and any(isinstance(x, slice) for x in casted_key)\n   3817     ):\n   3818         raise InvalidIndexError(key)\n-&gt; 3819     raise KeyError(key) from err\n   3820 except TypeError:\n   3821     # If we have a listlike key, _check_indexing_error will raise\n   3822     #  InvalidIndexError. Otherwise we fall through and re-raise\n   3823     #  the TypeError.\n   3824     self._check_indexing_error(key)\n\nKeyError: 'person_income'</pre> In\u00a0[\u00a0]: Copied! <pre>df['person_emp_exp'].describe()\n# The third quartile (Q3) for this feature is only 8 years, but the data contains\n# extreme outliers. A threshold of 40 years is a reasonable cap.\n</pre> df['person_emp_exp'].describe() # The third quartile (Q3) for this feature is only 8 years, but the data contains # extreme outliers. A threshold of 40 years is a reasonable cap. Out[\u00a0]: <pre>count    45000.000000\nmean         5.410333\nstd          6.063532\nmin          0.000000\n25%          1.000000\n50%          4.000000\n75%          8.000000\nmax        125.000000\nName: person_emp_exp, dtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>df['loan_amnt'].describe()\n# No specific data cleaning or filtering measures were necessary for this column.\n</pre> df['loan_amnt'].describe() # No specific data cleaning or filtering measures were necessary for this column. Out[\u00a0]: <pre>count    45000.000000\nmean         8.940948\nstd          0.710887\nmin          6.214608\n25%          8.517193\n50%          8.987197\n75%          9.412240\nmax         10.463103\nName: loan_amnt, dtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>df['loan_int_rate'].describe()\n# No specific data cleaning or filtering measures were necessary for this column.\n</pre> df['loan_int_rate'].describe() # No specific data cleaning or filtering measures were necessary for this column. Out[\u00a0]: <pre>count    45000.000000\nmean        11.006606\nstd          2.978808\nmin          5.420000\n25%          8.590000\n50%         11.010000\n75%         12.990000\nmax         20.000000\nName: loan_int_rate, dtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>df['loan_percent_income'].describe()\n# No specific data cleaning or filtering measures were necessary for this column.\n</pre> df['loan_percent_income'].describe() # No specific data cleaning or filtering measures were necessary for this column. Out[\u00a0]: <pre>count    45000.000000\nmean         0.139725\nstd          0.087212\nmin          0.000000\n25%          0.070000\n50%          0.120000\n75%          0.190000\nmax          0.660000\nName: loan_percent_income, dtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>df['cb_person_cred_hist_length'].describe()\n# No specific data cleaning or filtering measures were necessary for this column.\n</pre> df['cb_person_cred_hist_length'].describe() # No specific data cleaning or filtering measures were necessary for this column. Out[\u00a0]: <pre>count    45000.000000\nmean         5.867489\nstd          3.879702\nmin          2.000000\n25%          3.000000\n50%          4.000000\n75%          8.000000\nmax         30.000000\nName: cb_person_cred_hist_length, dtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>df['credit_score'].describe()\n# No specific data cleaning or filtering measures were necessary for this column.\n</pre> df['credit_score'].describe() # No specific data cleaning or filtering measures were necessary for this column. Out[\u00a0]: <pre>count    45000.000000\nmean       632.608756\nstd         50.435865\nmin        390.000000\n25%        601.000000\n50%        640.000000\n75%        670.000000\nmax        850.000000\nName: credit_score, dtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>df['loan_status'].describe()\n# No specific data cleaning or filtering measures were necessary for this column.\n</pre> df['loan_status'].describe() # No specific data cleaning or filtering measures were necessary for this column. Out[\u00a0]: <pre>count    45000.000000\nmean         0.222222\nstd          0.415744\nmin          0.000000\n25%          0.000000\n50%          0.000000\n75%          0.000000\nmax          1.000000\nName: loan_status, dtype: float64</pre>"},{"location":"data_exploration/#data-distributions","title":"Data Distributions\u00b6","text":""},{"location":"data_exploration/#categorical-columns","title":"Categorical Columns\u00b6","text":""},{"location":"data_exploration/#numerical-columns","title":"Numerical Columns\u00b6","text":""},{"location":"data_preparation/","title":"Data preparation","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\n</pre> import pandas as pd import numpy as np <p>Cleans and transforms the raw loan approval data. All decisions for filtering and transformation are based on the exploratory analysis from the 'data_exploration.ipynb' notebook.</p> In\u00a0[\u00a0]: Copied! <pre>df = pd.read_csv('data/loan_approval/loan_data.csv')\n</pre> df = pd.read_csv('data/loan_approval/loan_data.csv') In\u00a0[\u00a0]: Copied! <pre># Log-transform skewed numerical features\ndf['person_income'] = np.log(df['person_income'])\ndf['loan_amnt'] = np.log(df['loan_amnt'])\n</pre> # Log-transform skewed numerical features df['person_income'] = np.log(df['person_income']) df['loan_amnt'] = np.log(df['loan_amnt']) In\u00a0[\u00a0]: Copied! <pre># Map the sparse 'OTHER' category to null\ndf['person_home_ownership'] = df['person_home_ownership'].replace('OTHER', np.nan)\n</pre> # Map the sparse 'OTHER' category to null df['person_home_ownership'] = df['person_home_ownership'].replace('OTHER', np.nan) In\u00a0[\u00a0]: Copied! <pre># Filter out extreme outliers based on EDA findings\ndf = df[df['person_age'] &lt;= 70]\ndf = df[df['person_emp_exp'] &lt;= 40]\ndf = df[df['person_income'] &lt; 14]\n</pre> # Filter out extreme outliers based on EDA findings df = df[df['person_age'] &lt;= 70] df = df[df['person_emp_exp'] &lt;= 40] df = df[df['person_income'] &lt; 14] In\u00a0[\u00a0]: Copied! <pre># Save the processed data to a new file\ndf.to_csv('data/loan_approval/loan_data_refined.csv', index=False)\n</pre> # Save the processed data to a new file df.to_csv('data/loan_approval/loan_data_refined.csv', index=False)"},{"location":"model_training/","title":"Multi Layer Perceptron","text":"In\u00a0[3]: Copied! <pre>import pandas as pd\n\ndf = pd.read_csv('../data/loan_approval/loan_data_refined.csv')\n\ndf.head()\n</pre> import pandas as pd  df = pd.read_csv('../data/loan_approval/loan_data_refined.csv')  df.head() Out[3]: Unnamed: 0 person_age person_gender person_education person_income person_emp_exp person_home_ownership loan_amnt loan_intent loan_int_rate loan_percent_income cb_person_cred_hist_length credit_score previous_loan_defaults_on_file loan_status 0 0 22.0 female Master 11.183699 0 RENT 10.463103 PERSONAL 16.02 0.49 3.0 561 No 1 1 1 21.0 female High School 9.415890 0 OWN 6.907755 EDUCATION 11.14 0.08 2.0 504 Yes 0 2 2 25.0 female High School 9.428512 3 MORTGAGE 8.612503 MEDICAL 12.87 0.44 3.0 635 No 1 3 3 23.0 female Bachelor 11.286690 0 RENT 10.463103 MEDICAL 15.23 0.44 2.0 675 No 1 4 4 24.0 male Master 11.099453 1 RENT 10.463103 MEDICAL 14.27 0.53 4.0 586 No 1 In\u00a0[4]: Copied! <pre>df.describe()\n</pre> df.describe() Out[4]: Unnamed: 0 person_age person_income person_emp_exp loan_amnt loan_int_rate loan_percent_income cb_person_cred_hist_length credit_score loan_status count 44924.000000 44924.000000 44924.000000 44924.000000 44924.000000 44924.000000 44924.000000 44924.000000 44924.000000 44924.000000 mean 22485.167616 27.693193 11.121073 5.338527 8.940718 11.006575 0.139758 5.840397 632.498108 0.222353 std 12993.058492 5.728904 0.553768 5.740106 0.710852 2.978941 0.087184 3.818092 50.353701 0.415832 min 0.000000 20.000000 8.987197 0.000000 6.214608 5.420000 0.000000 2.000000 390.000000 0.000000 25% 11234.750000 24.000000 10.761492 1.000000 8.517193 8.590000 0.070000 3.000000 601.000000 0.000000 50% 22466.500000 26.000000 11.113089 4.000000 8.987197 11.010000 0.120000 4.000000 639.000000 0.000000 75% 33759.250000 30.000000 11.469151 8.000000 9.412097 13.000000 0.190000 8.000000 670.000000 0.000000 max 44999.000000 62.000000 13.945418 40.000000 10.463103 20.000000 0.660000 30.000000 772.000000 1.000000 In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\nTARGET = \"loan_status\"\n\nX = df.drop(columns=[TARGET]).copy()\ny = df[TARGET].copy()\n\nX_train_df, X_test_df, y_train_s, y_test_s = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nfor _df in (X_train_df, X_test_df):\n    obj_cols = _df.select_dtypes(include=\"object\").columns\n    _df[obj_cols] = _df[obj_cols].replace(\"\", np.nan)\n\ndef coerce_bool_like(dfx: pd.DataFrame) -&gt; pd.DataFrame:\n    out = dfx.copy()\n    for c in out.columns:\n        if out[c].dtype == \"bool\":\n            continue\n        if out[c].dtype == \"object\":\n            lower = out[c].str.strip().str.lower()\n            if lower.dropna().isin({\"yes\",\"no\",\"true\",\"false\",\"y\",\"n\",\"t\",\"f\",\"0\",\"1\"}).all():\n                out[c] = lower.isin({\"yes\",\"true\",\"y\",\"t\",\"1\"})\n    return out\n\nX_train_df = coerce_bool_like(X_train_df)\nX_test_df  = coerce_bool_like(X_test_df)\n\nnum_cols  = X_train_df.select_dtypes(include=[\"number\"]).columns.tolist()\nbool_cols = X_train_df.select_dtypes(include=[\"bool\"]).columns.tolist()\ncat_cols  = [c for c in X_train_df.columns if c not in num_cols + bool_cols]\n\nnum_pipe = Pipeline([\n    (\"imp\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", StandardScaler()),\n])\n\nbool_pipe = Pipeline([\n    (\"to_float\", FunctionTransformer(lambda X: X.astype(float))),\n    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"to_pm1\", FunctionTransformer(lambda X: np.where(X &gt; 0.5, 1.0, -1.0))),\n])\n\ncat_pipe = Pipeline([\n    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n    (\"to_pm1\", FunctionTransformer(lambda X: 2.0 * X - 1.0)),\n])\n\npreproc = ColumnTransformer(\n    transformers=[\n        (\"num\",  num_pipe,  num_cols),\n        (\"bool\", bool_pipe, bool_cols),\n        (\"cat\",  cat_pipe,  cat_cols),\n    ],\n    remainder=\"drop\"\n)\n\nX_train = preproc.fit_transform(X_train_df)\nX_test  = preproc.transform(X_test_df)\n\nX_train = X_train.astype(np.float64, copy=False)\nX_test  = X_test.astype(np.float64, copy=False)\n\nle = LabelEncoder()\ny_train = le.fit_transform(y_train_s.astype(str))\ny_test  = le.transform(y_test_s.astype(str))\n\nK = len(le.classes_)\ninput_dim = X_train.shape[1]\n\nprint(\"Shapes:\", X_train.shape, X_test.shape)\nprint(\"Value range (train):\", float(X_train.min()), \"to\", float(X_train.max()))\nprint(\"Bool cols mapped to \u00b11:\", bool_cols)\n</pre> import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer, LabelEncoder from sklearn.impute import SimpleImputer  TARGET = \"loan_status\"  X = df.drop(columns=[TARGET]).copy() y = df[TARGET].copy()  X_train_df, X_test_df, y_train_s, y_test_s = train_test_split(     X, y, test_size=0.2, random_state=42, stratify=y )  for _df in (X_train_df, X_test_df):     obj_cols = _df.select_dtypes(include=\"object\").columns     _df[obj_cols] = _df[obj_cols].replace(\"\", np.nan)  def coerce_bool_like(dfx: pd.DataFrame) -&gt; pd.DataFrame:     out = dfx.copy()     for c in out.columns:         if out[c].dtype == \"bool\":             continue         if out[c].dtype == \"object\":             lower = out[c].str.strip().str.lower()             if lower.dropna().isin({\"yes\",\"no\",\"true\",\"false\",\"y\",\"n\",\"t\",\"f\",\"0\",\"1\"}).all():                 out[c] = lower.isin({\"yes\",\"true\",\"y\",\"t\",\"1\"})     return out  X_train_df = coerce_bool_like(X_train_df) X_test_df  = coerce_bool_like(X_test_df)  num_cols  = X_train_df.select_dtypes(include=[\"number\"]).columns.tolist() bool_cols = X_train_df.select_dtypes(include=[\"bool\"]).columns.tolist() cat_cols  = [c for c in X_train_df.columns if c not in num_cols + bool_cols]  num_pipe = Pipeline([     (\"imp\", SimpleImputer(strategy=\"median\")),     (\"scaler\", StandardScaler()), ])  bool_pipe = Pipeline([     (\"to_float\", FunctionTransformer(lambda X: X.astype(float))),     (\"imp\", SimpleImputer(strategy=\"most_frequent\")),     (\"to_pm1\", FunctionTransformer(lambda X: np.where(X &gt; 0.5, 1.0, -1.0))), ])  cat_pipe = Pipeline([     (\"imp\", SimpleImputer(strategy=\"most_frequent\")),     (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),     (\"to_pm1\", FunctionTransformer(lambda X: 2.0 * X - 1.0)), ])  preproc = ColumnTransformer(     transformers=[         (\"num\",  num_pipe,  num_cols),         (\"bool\", bool_pipe, bool_cols),         (\"cat\",  cat_pipe,  cat_cols),     ],     remainder=\"drop\" )  X_train = preproc.fit_transform(X_train_df) X_test  = preproc.transform(X_test_df)  X_train = X_train.astype(np.float64, copy=False) X_test  = X_test.astype(np.float64, copy=False)  le = LabelEncoder() y_train = le.fit_transform(y_train_s.astype(str)) y_test  = le.transform(y_test_s.astype(str))  K = len(le.classes_) input_dim = X_train.shape[1]  print(\"Shapes:\", X_train.shape, X_test.shape) print(\"Value range (train):\", float(X_train.min()), \"to\", float(X_train.max())) print(\"Bool cols mapped to \u00b11:\", bool_cols)  <pre>Shapes: (35939, 26) (8985, 26)\nValue range (train): -4.813166360666915 to 6.345548280596645\nBool cols mapped to \u00b11: ['previous_loan_defaults_on_file']\n</pre> In\u00a0[26]: Copied! <pre>K = len(np.unique(y))                 # number of classes\n</pre> K = len(np.unique(y))                 # number of classes In\u00a0[27]: Copied! <pre># sanity checks before training\nassert X_train.ndim == 2 and X_train.dtype.kind in \"fc\", \"X_train must be float matrix\"\nassert y_train.ndim == 1 and np.issubdtype(y_train.dtype, np.integer), \"y_train must be int labels\"\nassert y_train.min() == 0 and y_train.max() == K-1, \"labels must be in [0..K-1]\"\n</pre> # sanity checks before training assert X_train.ndim == 2 and X_train.dtype.kind in \"fc\", \"X_train must be float matrix\" assert y_train.ndim == 1 and np.issubdtype(y_train.dtype, np.integer), \"y_train must be int labels\" assert y_train.min() == 0 and y_train.max() == K-1, \"labels must be in [0..K-1]\"  In\u00a0[28]: Copied! <pre>import numpy as np\nfrom sklearn.metrics import accuracy_score\n\n\ndef glorot_uniform(fan_in, fan_out):\n    limit = np.sqrt(6.0 / (fan_in + fan_out))\n    return np.random.uniform(-limit, limit, size=(fan_out, fan_in))\n\ndef init_vector_glorot(fan_out):\n    return np.zeros(fan_out)\n\ndef softmax(z):\n    z = z - np.max(z)\n    e = np.exp(z)\n    return e / np.sum(e)\n\ndef one_hot(y_i, K):\n    v = np.zeros(K, dtype=float)\n    v[y_i] = 1.0\n    return v\n\ntanh = np.tanh\ntanhp = lambda a: (1.0 - a**2) \n\n\nclass HiddenLayer:\n    def __init__(self, fan_in, fan_out):\n        self.W = glorot_uniform(fan_in, fan_out)  # shape (fan_out, fan_in)\n        self.b = init_vector_glorot(fan_out)      # shape (fan_out,)\n        self.z = None\n        self.a = None\n        self.prev_a = None\n\n    def forward(self, x):\n        self.prev_a = x\n        self.z = self.W @ x + self.b\n        self.a = tanh(self.z)\n        return self.a\n\n\ndef forward_probs(x, layers, W2, b2):\n    a = x\n    for layer in layers:\n        a = layer.forward(a)\n    z2 = W2 @ a + b2\n    p = softmax(z2)\n    return p, a\n\n\nK = len(np.unique(y))                 # classes\ninput_dim = X_train.shape[1]\nH = 16\nNLayers = 3\n\nHiddenLayers = []\nHiddenLayers.append(HiddenLayer(input_dim, H))\nfor _ in range(NLayers - 1):\n    HiddenLayers.append(HiddenLayer(H, H))\n\nW2 = glorot_uniform(H, K)   # shape (K, H)\nb2 = np.zeros(K)\n\neta = 0.01     \nepochs = 100\nclip_value = 5.0    # gradient clip (optional, but helps stability)\n\nN = len(X_train)\nindices = np.arange(N)\n\nfor epoch in range(epochs):\n\n    np.random.shuffle(indices)\n\n    for idx in indices:\n        x_i = X_train[idx]          # shape (D,)\n        y_i = int(y_train[idx])     # scalar int\n\n        # forward\n        p, a_last = forward_probs(x_i, HiddenLayers, W2, b2)\n\n        y_one = one_hot(y_i, K)\n        delta_out = p - y_one                               # (K,)\n\n        W2_old = W2.copy()\n\n        # grads output\n        dW2 = np.outer(delta_out, a_last)                   # (K, H)\n        db2 = delta_out                                     # (K,)\n\n        next_delta = delta_out            # current delta at output\n        next_W = W2_old                   # use cached W2 for first step\n\n        for layer in reversed(HiddenLayers):\n\n            g = next_W.T @ next_delta                       # (H,)\n            delta = g * tanhp(layer.a)                      # (H,)\n\n            dW = np.outer(delta, layer.prev_a)              # (H, in_dim)\n            db = delta                                      # (H,)\n\n            W_old = layer.W.copy()\n\n            if np.linalg.norm(dW) &gt; clip_value:\n                dW = dW * (clip_value / (np.linalg.norm(dW) + 1e-12))\n            if np.linalg.norm(db) &gt; clip_value:\n                db = db * (clip_value / (np.linalg.norm(db) + 1e-12))\n\n            layer.W -= eta * dW\n            layer.b -= eta * db\n\n            next_delta = delta\n            next_W = W_old\n\n        if np.linalg.norm(dW2) &gt; clip_value:\n            dW2 = dW2 * (clip_value / (np.linalg.norm(dW2) + 1e-12))\n        if np.linalg.norm(db2) &gt; clip_value:\n            db2 = db2 * (clip_value / (np.linalg.norm(db2) + 1e-12))\n\n        W2 -= eta * dW2\n        b2 -= eta * db2\n\n    probs = np.array([forward_probs(x_i, HiddenLayers, W2, b2)[0] for x_i in X_train])\n    y_pred = np.argmax(probs, axis=1)\n    print(f\"Epoch {epoch:03d} | train acc: {accuracy_score(y_train, y_pred):.4f}\")\n\n\n\n\nprint(\"Final W2:\", W2, \"Final b2:\", b2)\nfor i, layer in enumerate(HiddenLayers, 1):\n    print(f\"Layer {i} weights:\\n{layer.W}\\nLayer {i} bias:\\n{layer.b}\")\n</pre> import numpy as np from sklearn.metrics import accuracy_score   def glorot_uniform(fan_in, fan_out):     limit = np.sqrt(6.0 / (fan_in + fan_out))     return np.random.uniform(-limit, limit, size=(fan_out, fan_in))  def init_vector_glorot(fan_out):     return np.zeros(fan_out)  def softmax(z):     z = z - np.max(z)     e = np.exp(z)     return e / np.sum(e)  def one_hot(y_i, K):     v = np.zeros(K, dtype=float)     v[y_i] = 1.0     return v  tanh = np.tanh tanhp = lambda a: (1.0 - a**2)    class HiddenLayer:     def __init__(self, fan_in, fan_out):         self.W = glorot_uniform(fan_in, fan_out)  # shape (fan_out, fan_in)         self.b = init_vector_glorot(fan_out)      # shape (fan_out,)         self.z = None         self.a = None         self.prev_a = None      def forward(self, x):         self.prev_a = x         self.z = self.W @ x + self.b         self.a = tanh(self.z)         return self.a   def forward_probs(x, layers, W2, b2):     a = x     for layer in layers:         a = layer.forward(a)     z2 = W2 @ a + b2     p = softmax(z2)     return p, a   K = len(np.unique(y))                 # classes input_dim = X_train.shape[1] H = 16 NLayers = 3  HiddenLayers = [] HiddenLayers.append(HiddenLayer(input_dim, H)) for _ in range(NLayers - 1):     HiddenLayers.append(HiddenLayer(H, H))  W2 = glorot_uniform(H, K)   # shape (K, H) b2 = np.zeros(K)  eta = 0.01      epochs = 100 clip_value = 5.0    # gradient clip (optional, but helps stability)  N = len(X_train) indices = np.arange(N)  for epoch in range(epochs):      np.random.shuffle(indices)      for idx in indices:         x_i = X_train[idx]          # shape (D,)         y_i = int(y_train[idx])     # scalar int          # forward         p, a_last = forward_probs(x_i, HiddenLayers, W2, b2)          y_one = one_hot(y_i, K)         delta_out = p - y_one                               # (K,)          W2_old = W2.copy()          # grads output         dW2 = np.outer(delta_out, a_last)                   # (K, H)         db2 = delta_out                                     # (K,)          next_delta = delta_out            # current delta at output         next_W = W2_old                   # use cached W2 for first step          for layer in reversed(HiddenLayers):              g = next_W.T @ next_delta                       # (H,)             delta = g * tanhp(layer.a)                      # (H,)              dW = np.outer(delta, layer.prev_a)              # (H, in_dim)             db = delta                                      # (H,)              W_old = layer.W.copy()              if np.linalg.norm(dW) &gt; clip_value:                 dW = dW * (clip_value / (np.linalg.norm(dW) + 1e-12))             if np.linalg.norm(db) &gt; clip_value:                 db = db * (clip_value / (np.linalg.norm(db) + 1e-12))              layer.W -= eta * dW             layer.b -= eta * db              next_delta = delta             next_W = W_old          if np.linalg.norm(dW2) &gt; clip_value:             dW2 = dW2 * (clip_value / (np.linalg.norm(dW2) + 1e-12))         if np.linalg.norm(db2) &gt; clip_value:             db2 = db2 * (clip_value / (np.linalg.norm(db2) + 1e-12))          W2 -= eta * dW2         b2 -= eta * db2      probs = np.array([forward_probs(x_i, HiddenLayers, W2, b2)[0] for x_i in X_train])     y_pred = np.argmax(probs, axis=1)     print(f\"Epoch {epoch:03d} | train acc: {accuracy_score(y_train, y_pred):.4f}\")     print(\"Final W2:\", W2, \"Final b2:\", b2) for i, layer in enumerate(HiddenLayers, 1):     print(f\"Layer {i} weights:\\n{layer.W}\\nLayer {i} bias:\\n{layer.b}\")   <pre>Epoch 000 | train acc: 0.9122\nEpoch 001 | train acc: 0.9153\nEpoch 002 | train acc: 0.9140\nEpoch 003 | train acc: 0.9230\nEpoch 004 | train acc: 0.9223\nEpoch 005 | train acc: 0.9299\nEpoch 006 | train acc: 0.9316\nEpoch 007 | train acc: 0.9353\nEpoch 008 | train acc: 0.9358\nEpoch 009 | train acc: 0.9369\nEpoch 010 | train acc: 0.9370\nEpoch 011 | train acc: 0.9372\nEpoch 012 | train acc: 0.9361\nEpoch 013 | train acc: 0.9350\nEpoch 014 | train acc: 0.9359\nEpoch 015 | train acc: 0.9367\nEpoch 016 | train acc: 0.9348\nEpoch 017 | train acc: 0.9344\nEpoch 018 | train acc: 0.9336\nEpoch 019 | train acc: 0.9373\nEpoch 020 | train acc: 0.9366\nEpoch 021 | train acc: 0.9326\nEpoch 022 | train acc: 0.9321\nEpoch 023 | train acc: 0.9371\nEpoch 024 | train acc: 0.9351\nEpoch 025 | train acc: 0.9376\nEpoch 026 | train acc: 0.9373\nEpoch 027 | train acc: 0.9376\nEpoch 028 | train acc: 0.9363\nEpoch 029 | train acc: 0.9368\nEpoch 030 | train acc: 0.9367\nEpoch 031 | train acc: 0.9366\nEpoch 032 | train acc: 0.9370\nEpoch 033 | train acc: 0.9383\nEpoch 034 | train acc: 0.9372\nEpoch 035 | train acc: 0.9382\nEpoch 036 | train acc: 0.9385\nEpoch 037 | train acc: 0.9391\nEpoch 038 | train acc: 0.9378\nEpoch 039 | train acc: 0.9360\nEpoch 040 | train acc: 0.9376\nEpoch 041 | train acc: 0.9390\nEpoch 042 | train acc: 0.9329\nEpoch 043 | train acc: 0.9340\nEpoch 044 | train acc: 0.9360\nEpoch 045 | train acc: 0.9379\nEpoch 046 | train acc: 0.9371\nEpoch 047 | train acc: 0.9392\nEpoch 048 | train acc: 0.9384\nEpoch 049 | train acc: 0.9363\nEpoch 050 | train acc: 0.9390\nEpoch 051 | train acc: 0.9388\nEpoch 052 | train acc: 0.9381\nEpoch 053 | train acc: 0.9358\nEpoch 054 | train acc: 0.9371\nEpoch 055 | train acc: 0.9387\nEpoch 056 | train acc: 0.9376\nEpoch 057 | train acc: 0.9391\nEpoch 058 | train acc: 0.9393\nEpoch 059 | train acc: 0.9368\nEpoch 060 | train acc: 0.9380\nEpoch 061 | train acc: 0.9393\nEpoch 062 | train acc: 0.9316\nEpoch 063 | train acc: 0.9365\nEpoch 064 | train acc: 0.9395\nEpoch 065 | train acc: 0.9371\nEpoch 066 | train acc: 0.9351\nEpoch 067 | train acc: 0.9383\nEpoch 068 | train acc: 0.9372\nEpoch 069 | train acc: 0.9383\nEpoch 070 | train acc: 0.9379\nEpoch 071 | train acc: 0.9398\nEpoch 072 | train acc: 0.9383\nEpoch 073 | train acc: 0.9395\nEpoch 074 | train acc: 0.9376\nEpoch 075 | train acc: 0.9395\nEpoch 076 | train acc: 0.9394\nEpoch 077 | train acc: 0.9399\nEpoch 078 | train acc: 0.9383\nEpoch 079 | train acc: 0.9388\nEpoch 080 | train acc: 0.9382\nEpoch 081 | train acc: 0.9389\nEpoch 082 | train acc: 0.9381\nEpoch 083 | train acc: 0.9383\nEpoch 084 | train acc: 0.9381\nEpoch 085 | train acc: 0.9385\nEpoch 086 | train acc: 0.9390\nEpoch 087 | train acc: 0.9369\nEpoch 088 | train acc: 0.9392\nEpoch 089 | train acc: 0.9347\nEpoch 090 | train acc: 0.9400\nEpoch 091 | train acc: 0.9390\nEpoch 092 | train acc: 0.9374\nEpoch 093 | train acc: 0.9390\nEpoch 094 | train acc: 0.9346\nEpoch 095 | train acc: 0.9392\nEpoch 096 | train acc: 0.9374\nEpoch 097 | train acc: 0.9390\nEpoch 098 | train acc: 0.9364\nEpoch 099 | train acc: 0.9389\nFinal W2: [[ 0.14147249  1.20279893 -0.76100632  0.14000992  0.37719965 -0.39579763\n   0.33729468 -0.21822679 -0.94437897  0.0397664   0.13323904 -0.16682448\n   0.06893071  0.3975554   0.06977943  0.05351374]\n [ 0.34223173 -1.11689336  0.90481111  0.18490016  0.39567042  0.45251057\n   0.35148436 -0.21679351  1.24112781  0.01738311  0.38679676 -0.17046552\n   0.07256706 -0.05020689  0.0183485   0.01620554]] Final b2: [-1.2876251  1.2876251]\nLayer 1 weights:\n[[ 1.39144571e-01 -4.44488080e-01  1.73973412e-01  3.03703702e-01\n  -3.92557346e-02 -1.25598893e-01 -5.51646153e-02  4.86265082e-02\n   9.92968872e-02  2.32777683e-01  5.18246587e-02  1.26548941e-02\n  -1.11682475e+01 -1.11031656e+01 -1.13109379e+01 -1.11575859e+01\n  -1.11148575e+01 -6.16301208e+00 -6.01024062e+00 -6.26211148e+00\n  -1.23196656e+01 -1.24057665e+01 -1.24537648e+01 -1.23534714e+01\n  -1.23290310e+01 -1.21945651e+01]\n [ 1.14110573e+00 -2.42910908e-01 -6.97813794e-01 -4.39369211e-01\n  -4.00773221e-01  5.58635943e-01  2.53552667e+00  3.80011829e-02\n  -3.64569295e-01  4.41330755e+00 -1.81797186e-01  2.21596647e-01\n  -4.82969222e+00 -4.55503114e+00 -6.14415331e+00 -4.79073728e+00\n  -4.42508884e+00 -2.77582499e+00 -4.85326440e+00 -1.12811820e+00\n  -5.29666103e+00 -5.96362408e+00 -5.46962152e+00 -5.24080512e+00\n  -5.72490858e+00 -5.49592660e+00]\n [ 1.22040072e-01 -1.40567889e-01  1.13821599e-01  8.25228025e-02\n  -1.78276926e-02 -7.02509246e-02 -4.38089015e-02 -1.81824325e-02\n   6.17603220e-02  1.32872121e-01  1.30346643e-01  1.38547344e-01\n  -1.15269573e+01 -1.15138482e+01 -1.16082935e+01 -1.15222579e+01\n  -1.14819394e+01 -6.18924567e+00 -6.03527906e+00 -6.25003905e+00\n  -1.26802019e+01 -1.27569768e+01 -1.27016429e+01 -1.27040139e+01\n  -1.27312684e+01 -1.25714393e+01]\n [-1.03246180e+00  1.88366304e-01 -3.21319095e+00 -2.05982791e-02\n   3.85836967e+00  3.49887042e-01  7.07695005e+00  3.64696716e-01\n  -2.15905349e-01 -5.52042900e+00 -1.30642768e-01  5.59756181e-02\n  -1.38628963e+00 -1.65916969e+00 -2.40694496e+00 -1.46389772e+00\n  -1.73677828e+00 -5.64354333e+00 -5.12975966e+00  6.99311775e+00\n  -2.39201093e+00 -2.31873682e+00 -3.06481209e-01 -2.29675446e+00\n  -2.00357446e+00 -2.63064561e+00]\n [-9.25357582e-01 -4.05251062e-01 -1.61658585e+00  3.32316583e-01\n   3.82164107e-02  7.17572512e-01 -1.60120353e-01  3.21024901e-01\n  -5.14555729e-01 -2.11022768e+00  6.12946246e-02 -1.14824074e-01\n   3.93919082e+00  3.75556173e+00  4.15099479e+00  3.90040396e+00\n   3.99365566e+00  2.49106095e+00  1.21336342e+00  2.79235403e+00\n   4.40934742e+00  4.04164312e+00  5.38403835e+00  3.69214081e+00\n   4.47238453e+00  3.52885685e+00]\n [-1.41870636e+00  4.73179965e-01 -4.64846367e-01  3.65820671e-01\n   7.97526534e-03  4.77122445e+00  2.12818665e-01 -1.72631838e-02\n  -6.87799691e-01 -5.31600844e+00 -2.63025495e-01 -1.40368207e-01\n   2.12318313e+00  2.08289465e+00  2.01375719e+00  2.08513306e+00\n   1.84160671e+00  1.80466990e+00 -4.64515741e-01  2.04884871e+00\n   2.59590898e+00  1.62385352e+00  2.35019426e+00  2.68574334e+00\n   1.66264073e+00  2.17796168e+00]\n [ 1.74858261e-01  8.69540474e-02  6.63666125e-02 -2.80700081e-02\n  -2.75406844e-02 -1.28375748e-01  7.79762701e-03 -6.04220935e-02\n   1.90528230e-01  2.42661417e-01  2.34529732e-01  2.84839215e-01\n  -1.55491255e+01 -1.56055746e+01 -1.57357213e+01 -1.55784426e+01\n  -1.55360153e+01 -8.62813284e+00 -8.36344367e+00 -8.51517744e+00\n  -1.71405042e+01 -1.71618318e+01 -1.72683827e+01 -1.72083378e+01\n  -1.71830582e+01 -1.70194482e+01]\n [ 1.35391170e-01  4.16031709e-02  7.92255820e-02  2.32308780e-02\n   9.03563315e-03 -7.81417701e-02 -3.86927743e-02 -1.09488801e-01\n   7.93899316e-02  1.44006979e-01 -2.17915694e-01 -1.66834209e-01\n  -8.12011466e+00 -8.14702538e+00 -8.23693674e+00 -8.15129705e+00\n  -8.10263359e+00 -4.50425024e+00 -4.31353902e+00 -4.50208150e+00\n  -9.19419839e+00 -9.21382908e+00 -9.20489890e+00 -9.22980522e+00\n  -9.23998479e+00 -9.04878570e+00]\n [ 1.29915090e-01  2.27284848e-01  1.69786532e-01 -2.29256503e-02\n  -9.07655845e-02 -5.28976730e-02  1.05321521e-01 -2.63358484e-01\n   1.37572304e-01  1.52196169e-01 -2.11369515e-01 -1.74111973e-01\n  -1.09490323e+01 -1.10824895e+01 -1.11288341e+01 -1.10193604e+01\n  -1.09930276e+01 -6.19836898e+00 -6.03508555e+00 -6.20248854e+00\n  -1.23153189e+01 -1.22815251e+01 -1.22741932e+01 -1.22892838e+01\n  -1.22804433e+01 -1.20844267e+01]\n [ 1.08559427e+01  4.12141001e-02 -1.01661450e-01 -6.78092358e-02\n  -5.73120002e-02  3.73050865e-01  1.19000590e-01 -2.44996253e-02\n  -2.31083517e-01 -6.83130635e-01  1.07020205e-01  1.49315194e-01\n   3.93311467e+00  3.98035664e+00  4.17491975e+00  3.89481651e+00\n   4.03420310e+00  1.96671766e+00  1.90898801e+00  2.18395089e+00\n   4.67440672e+00  4.45673058e+00  4.59338171e+00  4.61000966e+00\n   4.58820610e+00  4.39951117e+00]\n [ 1.21766279e-01 -4.41244848e-01 -3.78269223e-03  2.21786092e-01\n   1.33223871e-01 -5.32216159e-02 -2.20001356e-01  2.73648451e-01\n   3.39520845e-02  1.68762538e-01  1.90832228e-01  1.79396159e-01\n  -1.23045161e+01 -1.21244916e+01 -1.23749438e+01 -1.22360823e+01\n  -1.21845534e+01 -7.09457748e+00 -6.88472209e+00 -7.05189523e+00\n  -1.35293837e+01 -1.36557871e+01 -1.37118287e+01 -1.37223249e+01\n  -1.36384716e+01 -1.34851283e+01]\n [ 9.38349023e-01 -7.39510534e-01  6.30194679e+00  1.40763953e-01\n  -5.24033264e-01 -6.15269546e-03 -6.11993482e-01  3.23582863e-02\n   1.50171460e-01  4.32070606e+00  5.39330637e-02  8.62791750e-02\n  -2.40994764e+00 -2.50728069e+00 -2.29051255e+00 -2.44024793e+00\n  -2.59348730e+00 -1.53468328e+00 -1.08613379e+00 -1.34348290e+00\n  -2.99879550e+00 -2.88591489e+00 -3.16306962e+00 -2.68641269e+00\n  -3.03021141e+00 -2.76534654e+00]\n [ 9.34144277e-01 -7.69910983e-01  1.79916477e-01  3.00081040e-01\n  -2.13970746e-01 -1.79926412e+00 -4.91368854e-01 -1.46005551e-01\n   5.53944565e-01  2.26582480e+00  3.19275545e-01  3.81434080e-01\n  -7.15667208e+00 -7.16344481e+00 -7.59470420e+00 -7.33378100e+00\n  -7.31983291e+00 -4.44683102e+00 -3.50921414e+00 -4.24714224e+00\n  -9.11388671e+00 -7.51984559e+00 -7.91879133e+00 -9.03468817e+00\n  -7.12946157e+00 -7.32626864e+00]\n [-1.53274422e-01 -1.39770357e-01 -6.55865209e-02  3.41160660e-02\n   8.37497288e-03  1.11270300e-01  4.61402060e-02  1.28351863e-01\n  -1.09926392e-01 -1.89683149e-01 -1.53785988e-01 -1.97806315e-01\n   1.43982918e+01  1.44392392e+01  1.45478934e+01  1.44171496e+01\n   1.43806970e+01  8.10800000e+00  7.87235464e+00  8.06616544e+00\n   1.61208575e+01  1.61094273e+01  1.62022889e+01  1.61567653e+01\n   1.61703581e+01  1.59656899e+01]\n [-1.64413403e-01 -7.81716617e-02  1.75932623e-02  8.93681120e-02\n   3.14232776e-02  1.44350625e-01  9.86002078e-02 -5.42189562e-02\n  -9.34416409e-02 -2.15384702e-01  2.09527792e-01  1.41931484e-01\n   1.06052606e+01  1.06353425e+01  1.07517788e+01  1.05489722e+01\n   1.05102218e+01  6.15572200e+00  5.85898928e+00  5.96880929e+00\n   1.17765416e+01  1.17669037e+01  1.19436224e+01  1.18047599e+01\n   1.19538272e+01  1.17297386e+01]\n [ 1.39397335e-01  1.80247289e-01  1.29907460e-01  4.23312935e-02\n  -4.02636921e-02 -9.14657514e-02  7.06986657e-02 -2.60888946e-01\n   9.19360373e-02  1.50358442e-01 -2.13558616e-01 -1.35586125e-01\n  -1.13621344e+01 -1.14166692e+01 -1.15148108e+01 -1.14550850e+01\n  -1.13863467e+01 -6.35984178e+00 -6.17893279e+00 -6.35884709e+00\n  -1.26688123e+01 -1.26308167e+01 -1.26145281e+01 -1.27013224e+01\n  -1.26561456e+01 -1.24741943e+01]]\nLayer 1 bias:\n[ -86.00826629  -39.9088024   -88.37162609  -48.67682447   24.3066671\n    3.23136686 -120.92117441  -62.52898324  -85.11108714   14.1234343\n  -95.04352829   -4.39297823  -50.88782371  112.8428128    81.9845441\n  -87.92021303]\nLayer 2 weights:\n[[ 1.42839417e-01  1.79127402e-01  5.00818741e-01  3.99976068e-02\n  -4.12744035e-01  4.03100206e+00  5.82980564e-01  9.22693732e-01\n   1.60162153e+00 -2.03014238e-01  1.15399706e+00  1.03503222e+00\n  -4.02522804e-01 -4.13862653e-01 -1.59130702e+00  2.75249711e-02]\n [ 1.92759491e+00  4.02060074e-01  2.27218149e+00 -3.80941460e+00\n  -2.79277690e+00 -3.17270137e+00  2.98127761e+00  2.53828099e+00\n   3.02663539e+00 -8.69514439e-01  2.16123752e+00  1.26643496e+00\n   2.16440923e+00 -3.04615291e+00 -2.53720538e+00  2.82111926e+00]\n [-9.27415379e-01 -6.76007434e-01  9.89012315e-01  1.20305632e+00\n   2.34070062e+00  1.26835402e-01 -1.41189244e+00  9.49873047e-01\n   8.58601909e-01  6.85429491e+00  4.64869218e-01 -2.60200900e+00\n   9.33781252e-01  1.03866555e+00 -2.73530269e-01  1.48685204e+00]\n [-4.80868812e-02 -2.50268619e-01  4.67917917e-01 -3.27704088e+00\n  -4.85550960e-02  2.60112456e-02  4.17740453e-01  7.31515303e-01\n   4.98758354e-01  2.27748001e-01 -7.80790702e-02 -1.03068868e-01\n  -1.40784250e-01 -5.60387074e-02 -1.26202479e-01  3.36746160e-01]\n [ 2.40614037e+00  1.40530727e+00  2.29516771e+00 -1.89358254e+00\n  -1.42293103e+00 -8.14279042e-01  2.52642580e+00  2.01142775e+00\n   1.82568050e+00 -7.38477980e-01  2.43638060e+00  8.12878367e-01\n   8.13896341e-02 -3.10232840e+00 -2.65948605e+00  2.14872938e+00]\n [ 1.30760183e+00 -3.54389388e+00 -1.35591605e+00 -4.79208233e+00\n   6.77894968e-01  4.72040579e-01 -1.47728724e+00 -1.86700879e+00\n  -1.81221709e+00  8.30714205e-02 -1.66482712e+00 -1.13600492e+00\n  -1.92583205e-01 -1.11414677e+00  1.90551778e+00 -1.43975216e+00]\n [ 2.15849294e+00  5.41043307e-01  2.81016700e+00 -3.74189730e+00\n  -2.35354115e+00 -5.28845928e-01  2.79379015e+00  2.46421763e+00\n   2.34910939e+00 -4.16755993e+00  2.86775804e+00  3.27387613e+00\n   3.18641610e+00 -2.89885367e+00 -2.49234133e+00  2.09294828e+00]\n [-2.11682406e-01  1.18059555e+00  1.75362772e-02  3.52378763e+00\n  -2.09473229e-01 -2.85099814e-01  6.89687771e-01  8.53182752e-03\n   4.68899049e-02 -5.24947307e-01  4.36780679e-01  1.20166100e+00\n   2.07597173e-01 -2.91334421e-01  3.50421460e-01  1.34834707e-01]\n [-5.02690797e-01 -1.28834787e+00 -5.52887027e-02 -3.66367098e+00\n   8.56490462e-02  3.67878642e-01 -5.47948553e-01 -4.89526083e-01\n  -4.29077510e-01  4.38871512e-01 -4.89408761e-01 -7.73585622e-01\n   7.82417580e-02  6.35178608e-01  2.98156204e-01 -8.65510913e-01]\n [ 1.23721100e+00 -5.98786328e-01 -9.35744898e-04 -2.29916167e+00\n  -7.56978769e-01 -1.69412233e+00 -3.20385517e-01 -3.35252529e-01\n  -5.19522497e-01 -2.49516822e+00 -1.17263221e+00  2.69205214e-01\n   1.21013629e+00 -4.79410684e-02  5.51994908e-01  8.35463459e-02]\n [ 5.15334451e+00  5.37001362e-01  6.07775112e+00 -2.09080363e+00\n  -6.57667231e+00 -4.69813359e+00  5.99042019e+00  6.04741495e+00\n   5.00452413e+00 -7.56772590e+00  5.33797742e+00  6.12527758e+00\n   6.66980457e+00 -4.19707836e+00 -5.39849016e+00  5.59038263e+00]\n [-1.03909372e+00 -1.49498095e-01 -3.05866076e-01  2.50512220e+00\n   1.77639226e+00  5.81004583e-01  6.02288250e-02 -1.73704684e-01\n   5.69945636e-02  1.11654373e+00 -2.43551572e-02 -7.08245028e-01\n  -1.71864120e+00  9.44881027e-01  1.58480039e-01 -3.28228257e-01]\n [ 8.72023411e-01  4.73647502e-01 -2.97763681e-01 -5.32089525e-01\n  -5.23023276e-01 -8.26168671e-01 -7.08487329e-02 -3.43794741e-01\n   1.65206863e-01 -4.86676978e+00  4.12004686e-01  2.28772676e+00\n   1.57289456e+00  6.14088754e-02 -2.07893568e-01 -2.93203620e-01]\n [-1.47888955e+01 -4.24945828e-01 -1.86368306e+01  1.58496613e+01\n   1.55400250e+01  1.72211259e+01 -1.81828976e+01 -1.74400425e+01\n  -1.77441298e+01  1.09486926e+01 -1.88810501e+01 -1.68287891e+01\n  -1.83882950e+01  1.40923455e+01  1.87742598e+01 -1.79234689e+01]\n [-1.55433022e+00 -3.25831092e-01 -2.44505846e+00  2.05421779e+00\n   1.30827605e+00  1.90468968e+00 -3.37099543e+00 -2.81413077e+00\n  -2.39477462e+00  4.06508860e+00 -2.90275722e+00 -3.77121600e+00\n  -2.51319607e+00  2.15833179e+00  2.77009730e+00 -2.83665675e+00]\n [-7.75388121e+00 -3.38411565e-01 -8.26457368e+00  5.21475320e+00\n   7.87464571e+00  7.96275903e+00 -8.83619755e+00 -8.10496305e+00\n  -9.36861753e+00  4.18238737e+00 -8.69507665e+00 -8.86065278e+00\n  -8.18812669e+00  8.37018046e+00  9.16982201e+00 -8.57343156e+00]]\nLayer 2 bias:\n[-1.12725228e+01 -3.44862034e+01  7.46290461e+00 -1.88629242e-01\n -2.27379383e+01  1.51198836e+01 -3.71885354e+01 -6.21798558e+00\n  8.70641261e+00 -2.66305079e+00 -7.93735266e+01  7.76118341e+00\n -8.17981173e+00  2.47360378e+02  3.54148664e+01  1.16560355e+02]\nLayer 3 weights:\n[[ 4.42313795e-01  4.29359042e-01  7.07713526e-01  3.85735182e-01\n   1.35953461e-01 -4.26910766e-01 -1.51293922e-01  3.50228148e-01\n   2.40214962e-01 -5.96973694e-01 -1.10033675e+00  2.91067189e-01\n  -6.43095959e-01  3.12570244e-01 -9.60425662e-02  1.73086096e-01]\n [ 4.29095936e-02  8.46839338e-01  1.08096028e-01  1.76849407e+00\n  -6.27413560e-02  2.66589480e+00  6.89841083e-01 -1.83609160e+00\n   2.32926925e+00  6.52403135e-01 -4.37778501e-01 -9.86898396e-01\n   4.18449057e-01  6.87147134e-01 -1.99442414e-01  2.32185419e-01]\n [ 6.19997731e-01  6.23704056e-01  1.29036990e+00  1.03189723e-01\n   9.88913107e-01 -9.28793579e-01 -1.78682552e+00 -6.99990538e-01\n  -1.00210475e-02 -4.26070072e-01 -1.18464938e+00  3.12113637e-01\n  -2.71893255e+00  1.77870548e-01  8.15509038e-01 -1.01068205e+00]\n [ 2.02174775e-01 -3.60653479e-01 -5.91708684e-01 -3.04722837e-01\n  -5.92695924e-02  6.57212893e-01 -1.61387272e-01  2.92027578e-01\n   1.02780110e-01  4.99810913e-01 -1.16654917e-01  2.08138156e-01\n   3.63173219e-01  6.95458881e-02 -1.07576074e-01 -2.61082270e-01]\n [ 2.95372315e-01  1.42441484e-01  1.48145764e-01  1.33950256e-01\n  -3.16570782e-01 -2.77853961e-01  3.46029633e-03 -1.32506493e-01\n  -1.63577065e-01  7.08446322e-02 -1.50214507e-02 -4.88177623e-02\n   5.17852960e-02 -1.14286682e-02 -7.99655782e-02  4.20906773e-03]\n [ 1.83265353e+00  4.08299700e-01  2.47244530e+00 -1.85360897e-01\n   7.70491378e-01  3.43952964e-01  4.67935902e-01 -7.46869545e-01\n   3.33887047e-01 -1.08997223e+00 -1.39180001e+00  5.47264423e-01\n  -4.74480905e-01  2.28059005e-01 -1.13611143e+00 -1.50432956e+00]\n [ 1.47703331e-01  8.02160721e-02  4.01607946e-02 -1.89082904e-01\n   2.13464892e-01 -8.31830726e-02 -1.67232778e-01  3.73092338e-02\n   3.08136938e-01  5.34581171e-02 -6.21608769e-02  9.43539873e-02\n   1.16868249e-01 -9.07408026e-02 -6.53189059e-02  1.36517830e-01]\n [-6.55416978e-02 -1.71753608e-01 -4.04176777e-01 -3.19566347e-01\n  -3.89552318e-02  6.88608938e-01 -1.57838232e-01  5.90474534e-02\n  -1.92412506e-01  3.80132489e-01  6.29003766e-02  1.58681566e-01\n   2.06575158e-01 -3.33706579e-02  1.51795572e-01 -8.74416374e-02]\n [ 9.03955891e+00 -1.04651580e+01  8.72356397e+00 -1.01713142e+01\n  -9.03966789e+00 -9.36329605e+00 -1.10576361e+01  9.51496703e+00\n  -9.17615594e+00 -9.50440223e+00 -1.27129955e+01  9.67264896e+00\n  -1.00610595e+01  1.78624568e+01  1.14857756e+01  1.33251874e+01]\n [-1.26588750e-01 -1.14760096e-01 -2.85589263e-03  1.68379074e-01\n   2.70742707e-01 -8.42786765e-02 -1.29909116e-02  3.28726700e-04\n   4.87679330e-02 -1.07109606e-01  2.62292482e-01  2.56131126e-02\n  -3.23733743e-01 -5.62087041e-03 -2.14007121e-01  2.71605086e-01]\n [ 4.55131772e-01  1.70228906e-01  6.20644715e-01 -2.86820583e-01\n   4.81102373e-01  4.94290655e-01 -2.14354794e-01 -1.84888917e-02\n  -2.19936117e-02 -1.28897883e-01 -2.14087970e+00  4.13041095e-01\n  -4.24273279e-01  6.79836722e-01  1.01965946e+00  2.53733027e-01]\n [-9.06922987e-02 -4.56587767e-02  1.87767066e-02 -1.86528187e-01\n   2.26952818e-02 -2.83244624e-02  1.37228067e-01 -2.76321116e-01\n  -1.44729131e-01 -4.76676482e-02 -5.63096738e-03 -8.28712940e-03\n  -1.48245141e-01 -3.30177350e-02 -4.59066873e-02  5.98807479e-02]\n [ 1.66894902e-01  2.17031501e-01  5.24960691e-02 -2.79125065e-01\n  -1.50749751e-01  2.71873882e-01  1.00030111e-01  1.95145646e-01\n  -2.63902674e-01  2.30962091e-01  1.70676273e-01  1.43385653e-01\n  -2.03697050e-01 -7.91813260e-02  5.16175821e-02  3.09689709e-01]\n [-7.86226416e-01  3.53134152e-01 -1.24706263e+00 -1.89436192e-01\n  -7.06684597e-01  6.32517390e-01  1.24290631e-01 -5.98137304e-01\n  -2.29256450e-02 -3.09074162e-01  7.74449675e-01  3.57459177e-02\n   5.85746508e-01 -4.48544199e-01 -8.30911469e-01 -3.22050569e-01]\n [ 2.99076408e-01 -4.06119263e-01  3.67951309e-01 -5.95968595e-01\n  -4.05139840e-01 -2.23723984e+00 -7.19627772e-01  7.49184904e-01\n  -1.31792618e+00 -1.40281799e-01  4.55131798e-01  4.75129253e-01\n  -2.65810679e-01 -5.48843926e-01 -3.10386993e-01  1.22312190e-01]\n [-9.93778582e-02 -1.57380150e-02 -1.19757383e-01 -8.44306615e-02\n   1.61654102e-01 -2.45444427e-01  2.59438877e-01 -3.10298697e-01\n  -1.95528218e-03  1.79359964e-01 -1.56221181e-01  3.12427582e-01\n  -2.55907747e-01 -6.13720476e-02 -1.49095669e-01  2.79348378e-02]]\nLayer 3 bias:\n[ 9.62424617e-01  9.00978506e+00 -5.52095284e+00 -7.39086714e-01\n  6.08671271e-01 -3.64110423e-01 -1.48646549e-01 -5.54169187e-01\n  1.70113183e+02 -2.29344037e-01  2.83391042e+00  1.12855385e-01\n  7.09291721e-01 -1.74398052e-01 -3.35021049e+00 -6.81277375e-02]\n</pre> In\u00a0[29]: Copied! <pre>probs = np.array([forward_probs(x_i, HiddenLayers, W2, b2)[0] for x_i in X_test])  # (N, K)\ny_pred = np.argmax(probs, axis=1)\nprint(\"Testing Accuracy:\", accuracy_score(y_test, y_pred))\n</pre>  probs = np.array([forward_probs(x_i, HiddenLayers, W2, b2)[0] for x_i in X_test])  # (N, K) y_pred = np.argmax(probs, axis=1) print(\"Testing Accuracy:\", accuracy_score(y_test, y_pred)) <pre>Testing Accuracy: 0.9364496382860322\n</pre>"},{"location":"model_training/#multi-layer-perceptron","title":"Multi Layer Perceptron\u00b6","text":"<p>This is the Notebook used for the Implementation, Training and Testing of a numpy-based Multi Layer Perceptron classifier.</p>"},{"location":"model_training/#importing-data-preparation","title":"Importing Data - Preparation\u00b6","text":"<p>Separating data into training, validation and testing. This process uses the Scikit-Learn train_test_split class.</p>"}]}