{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1018c4cc",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron\n",
    "\n",
    "This is the Notebook used for the Implementation, Training and Testing of a numpy-based Multi Layer Perceptron classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4184a4d8",
   "metadata": {},
   "source": [
    "## Importing Data - Preparation\n",
    "\n",
    "Separating data into training, validation and testing. This process uses the Scikit-Learn [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba2c5dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_age</th>\n",
       "      <th>person_gender</th>\n",
       "      <th>person_education</th>\n",
       "      <th>person_emp_exp</th>\n",
       "      <th>person_home_ownership</th>\n",
       "      <th>loan_intent</th>\n",
       "      <th>loan_int_rate</th>\n",
       "      <th>loan_percent_income</th>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>previous_loan_defaults_on_file</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>person_income_log</th>\n",
       "      <th>loan_amnt_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>female</td>\n",
       "      <td>Master</td>\n",
       "      <td>0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>PERSONAL</td>\n",
       "      <td>16.02</td>\n",
       "      <td>0.49</td>\n",
       "      <td>3.0</td>\n",
       "      <td>561</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>11.183699</td>\n",
       "      <td>10.463103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.0</td>\n",
       "      <td>female</td>\n",
       "      <td>High School</td>\n",
       "      <td>0</td>\n",
       "      <td>OWN</td>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>11.14</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>504</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>9.415890</td>\n",
       "      <td>6.907755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.0</td>\n",
       "      <td>female</td>\n",
       "      <td>High School</td>\n",
       "      <td>3</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>MEDICAL</td>\n",
       "      <td>12.87</td>\n",
       "      <td>0.44</td>\n",
       "      <td>3.0</td>\n",
       "      <td>635</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>9.428512</td>\n",
       "      <td>8.612503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.0</td>\n",
       "      <td>female</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>MEDICAL</td>\n",
       "      <td>15.23</td>\n",
       "      <td>0.44</td>\n",
       "      <td>2.0</td>\n",
       "      <td>675</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>11.286690</td>\n",
       "      <td>10.463103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.0</td>\n",
       "      <td>male</td>\n",
       "      <td>Master</td>\n",
       "      <td>1</td>\n",
       "      <td>RENT</td>\n",
       "      <td>MEDICAL</td>\n",
       "      <td>14.27</td>\n",
       "      <td>0.53</td>\n",
       "      <td>4.0</td>\n",
       "      <td>586</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>11.099453</td>\n",
       "      <td>10.463103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   person_age person_gender person_education  person_emp_exp  \\\n",
       "0        22.0        female           Master               0   \n",
       "1        21.0        female      High School               0   \n",
       "2        25.0        female      High School               3   \n",
       "3        23.0        female         Bachelor               0   \n",
       "4        24.0          male           Master               1   \n",
       "\n",
       "  person_home_ownership loan_intent  loan_int_rate  loan_percent_income  \\\n",
       "0                  RENT    PERSONAL          16.02                 0.49   \n",
       "1                   OWN   EDUCATION          11.14                 0.08   \n",
       "2              MORTGAGE     MEDICAL          12.87                 0.44   \n",
       "3                  RENT     MEDICAL          15.23                 0.44   \n",
       "4                  RENT     MEDICAL          14.27                 0.53   \n",
       "\n",
       "   cb_person_cred_hist_length  credit_score previous_loan_defaults_on_file  \\\n",
       "0                         3.0           561                             No   \n",
       "1                         2.0           504                            Yes   \n",
       "2                         3.0           635                             No   \n",
       "3                         2.0           675                             No   \n",
       "4                         4.0           586                             No   \n",
       "\n",
       "   loan_status  person_income_log  loan_amnt_log  \n",
       "0            1          11.183699      10.463103  \n",
       "1            0           9.415890       6.907755  \n",
       "2            1           9.428512       8.612503  \n",
       "3            1          11.286690      10.463103  \n",
       "4            1          11.099453      10.463103  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('loan_approval/loan_data_refined.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c31f29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_age</th>\n",
       "      <th>person_emp_exp</th>\n",
       "      <th>loan_int_rate</th>\n",
       "      <th>loan_percent_income</th>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>person_income_log</th>\n",
       "      <th>loan_amnt_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>44973.000000</td>\n",
       "      <td>44973.000000</td>\n",
       "      <td>44973.000000</td>\n",
       "      <td>44973.000000</td>\n",
       "      <td>44973.000000</td>\n",
       "      <td>44973.000000</td>\n",
       "      <td>44973.000000</td>\n",
       "      <td>44973.000000</td>\n",
       "      <td>44973.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>27.734596</td>\n",
       "      <td>5.380784</td>\n",
       "      <td>11.007052</td>\n",
       "      <td>0.139779</td>\n",
       "      <td>5.860272</td>\n",
       "      <td>632.568185</td>\n",
       "      <td>0.222356</td>\n",
       "      <td>11.121199</td>\n",
       "      <td>8.940998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.862090</td>\n",
       "      <td>5.878450</td>\n",
       "      <td>2.979142</td>\n",
       "      <td>0.087194</td>\n",
       "      <td>3.864655</td>\n",
       "      <td>50.389675</td>\n",
       "      <td>0.415833</td>\n",
       "      <td>0.553790</td>\n",
       "      <td>0.710942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>390.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.987197</td>\n",
       "      <td>6.214608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.590000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>601.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.761641</td>\n",
       "      <td>8.517193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>11.010000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>640.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.113089</td>\n",
       "      <td>8.987197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>670.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.469412</td>\n",
       "      <td>9.413281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>784.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.945418</td>\n",
       "      <td>10.463103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         person_age  person_emp_exp  loan_int_rate  loan_percent_income  \\\n",
       "count  44973.000000    44973.000000   44973.000000         44973.000000   \n",
       "mean      27.734596        5.380784      11.007052             0.139779   \n",
       "std        5.862090        5.878450       2.979142             0.087194   \n",
       "min       20.000000        0.000000       5.420000             0.000000   \n",
       "25%       24.000000        1.000000       8.590000             0.070000   \n",
       "50%       26.000000        4.000000      11.010000             0.120000   \n",
       "75%       30.000000        8.000000      13.000000             0.190000   \n",
       "max       70.000000       50.000000      20.000000             0.660000   \n",
       "\n",
       "       cb_person_cred_hist_length  credit_score   loan_status  \\\n",
       "count                44973.000000  44973.000000  44973.000000   \n",
       "mean                     5.860272    632.568185      0.222356   \n",
       "std                      3.864655     50.389675      0.415833   \n",
       "min                      2.000000    390.000000      0.000000   \n",
       "25%                      3.000000    601.000000      0.000000   \n",
       "50%                      4.000000    640.000000      0.000000   \n",
       "75%                      8.000000    670.000000      0.000000   \n",
       "max                     30.000000    784.000000      1.000000   \n",
       "\n",
       "       person_income_log  loan_amnt_log  \n",
       "count       44973.000000   44973.000000  \n",
       "mean           11.121199       8.940998  \n",
       "std             0.553790       0.710942  \n",
       "min             8.987197       6.214608  \n",
       "25%            10.761641       8.517193  \n",
       "50%            11.113089       8.987197  \n",
       "75%            11.469412       9.413281  \n",
       "max            13.945418      10.463103  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01451e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (35978, 25) (8995, 25)\n",
      "Value range (train): -4.823352093774259 to 7.602000771513218\n",
      "Bool cols mapped to ±1: ['previous_loan_defaults_on_file']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "TARGET = \"loan_status\"\n",
    "\n",
    "X = df.drop(columns=[TARGET]).copy()\n",
    "y = df[TARGET].copy()\n",
    "\n",
    "X_train_df, X_test_df, y_train_s, y_test_s = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "for _df in (X_train_df, X_test_df):\n",
    "    obj_cols = _df.select_dtypes(include=\"object\").columns\n",
    "    _df[obj_cols] = _df[obj_cols].replace(\"\", np.nan)\n",
    "\n",
    "def coerce_bool_like(dfx: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = dfx.copy()\n",
    "    for c in out.columns:\n",
    "        if out[c].dtype == \"bool\":\n",
    "            continue\n",
    "        if out[c].dtype == \"object\":\n",
    "            lower = out[c].str.strip().str.lower()\n",
    "            if lower.dropna().isin({\"yes\",\"no\",\"true\",\"false\",\"y\",\"n\",\"t\",\"f\",\"0\",\"1\"}).all():\n",
    "                out[c] = lower.isin({\"yes\",\"true\",\"y\",\"t\",\"1\"})\n",
    "    return out\n",
    "\n",
    "X_train_df = coerce_bool_like(X_train_df)\n",
    "X_test_df  = coerce_bool_like(X_test_df)\n",
    "\n",
    "num_cols  = X_train_df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "bool_cols = X_train_df.select_dtypes(include=[\"bool\"]).columns.tolist()\n",
    "cat_cols  = [c for c in X_train_df.columns if c not in num_cols + bool_cols]\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "bool_pipe = Pipeline([\n",
    "    (\"to_float\", FunctionTransformer(lambda X: X.astype(float))),\n",
    "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"to_pm1\", FunctionTransformer(lambda X: np.where(X > 0.5, 1.0, -1.0))),\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "    (\"to_pm1\", FunctionTransformer(lambda X: 2.0 * X - 1.0)),\n",
    "])\n",
    "\n",
    "preproc = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\",  num_pipe,  num_cols),\n",
    "        (\"bool\", bool_pipe, bool_cols),\n",
    "        (\"cat\",  cat_pipe,  cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "X_train = preproc.fit_transform(X_train_df)\n",
    "X_test  = preproc.transform(X_test_df)\n",
    "\n",
    "X_train = X_train.astype(np.float64, copy=False)\n",
    "X_test  = X_test.astype(np.float64, copy=False)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train_s.astype(str))\n",
    "y_test  = le.transform(y_test_s.astype(str))\n",
    "\n",
    "K = len(le.classes_)\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "print(\"Shapes:\", X_train.shape, X_test.shape)\n",
    "print(\"Value range (train):\", float(X_train.min()), \"to\", float(X_train.max()))\n",
    "print(\"Bool cols mapped to ±1:\", bool_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c590716",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = len(np.unique(y))                 # number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1d7fbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity checks before training\n",
    "assert X_train.ndim == 2 and X_train.dtype.kind in \"fc\", \"X_train must be float matrix\"\n",
    "assert y_train.ndim == 1 and np.issubdtype(y_train.dtype, np.integer), \"y_train must be int labels\"\n",
    "assert y_train.min() == 0 and y_train.max() == K-1, \"labels must be in [0..K-1]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d7f6d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | train acc: 0.9069 | train loss: 0.2150\n",
      "Epoch 001 | train acc: 0.9046 | train loss: 0.2114\n",
      "Epoch 002 | train acc: 0.9171 | train loss: 0.1950\n",
      "Epoch 003 | train acc: 0.9031 | train loss: 0.2108\n",
      "Epoch 004 | train acc: 0.9183 | train loss: 0.1850\n",
      "Epoch 005 | train acc: 0.9206 | train loss: 0.1855\n",
      "Epoch 006 | train acc: 0.9190 | train loss: 0.1872\n",
      "Epoch 007 | train acc: 0.9192 | train loss: 0.1877\n",
      "Epoch 008 | train acc: 0.9194 | train loss: 0.1854\n",
      "Epoch 009 | train acc: 0.9173 | train loss: 0.1914\n",
      "Epoch 010 | train acc: 0.9215 | train loss: 0.1828\n",
      "Epoch 011 | train acc: 0.9228 | train loss: 0.1810\n",
      "Epoch 012 | train acc: 0.9228 | train loss: 0.1785\n",
      "Epoch 013 | train acc: 0.9223 | train loss: 0.1783\n",
      "Epoch 014 | train acc: 0.9228 | train loss: 0.1771\n",
      "Epoch 015 | train acc: 0.9221 | train loss: 0.1803\n",
      "Epoch 016 | train acc: 0.9224 | train loss: 0.1857\n",
      "Epoch 017 | train acc: 0.9208 | train loss: 0.1792\n",
      "Epoch 018 | train acc: 0.9245 | train loss: 0.1776\n",
      "Epoch 019 | train acc: 0.9246 | train loss: 0.1742\n",
      "Epoch 020 | train acc: 0.9222 | train loss: 0.1767\n",
      "Epoch 021 | train acc: 0.9231 | train loss: 0.1809\n",
      "Epoch 022 | train acc: 0.9234 | train loss: 0.1801\n",
      "Epoch 023 | train acc: 0.9245 | train loss: 0.1758\n",
      "Epoch 024 | train acc: 0.9229 | train loss: 0.1783\n",
      "Epoch 025 | train acc: 0.9224 | train loss: 0.1807\n",
      "Epoch 026 | train acc: 0.9173 | train loss: 0.1893\n",
      "Epoch 027 | train acc: 0.9249 | train loss: 0.1771\n",
      "Epoch 028 | train acc: 0.9238 | train loss: 0.1842\n",
      "Epoch 029 | train acc: 0.9217 | train loss: 0.1897\n",
      "Epoch 030 | train acc: 0.9243 | train loss: 0.1788\n",
      "Epoch 031 | train acc: 0.9210 | train loss: 0.1796\n",
      "Epoch 032 | train acc: 0.9186 | train loss: 0.1866\n",
      "Epoch 033 | train acc: 0.9231 | train loss: 0.1789\n",
      "Epoch 034 | train acc: 0.9250 | train loss: 0.1755\n",
      "Epoch 035 | train acc: 0.9227 | train loss: 0.1801\n",
      "Epoch 036 | train acc: 0.9253 | train loss: 0.1756\n",
      "Epoch 037 | train acc: 0.9240 | train loss: 0.1790\n",
      "Epoch 038 | train acc: 0.9248 | train loss: 0.1743\n",
      "Epoch 039 | train acc: 0.9230 | train loss: 0.1779\n",
      "Epoch 040 | train acc: 0.9231 | train loss: 0.1777\n",
      "Epoch 041 | train acc: 0.9251 | train loss: 0.1726\n",
      "Epoch 042 | train acc: 0.9245 | train loss: 0.1733\n",
      "Epoch 043 | train acc: 0.9251 | train loss: 0.1768\n",
      "Epoch 044 | train acc: 0.9257 | train loss: 0.1743\n",
      "Epoch 045 | train acc: 0.9253 | train loss: 0.1769\n",
      "Epoch 046 | train acc: 0.9263 | train loss: 0.1727\n",
      "Epoch 047 | train acc: 0.9252 | train loss: 0.1873\n",
      "Epoch 048 | train acc: 0.9246 | train loss: 0.1747\n",
      "Epoch 049 | train acc: 0.9245 | train loss: 0.1761\n",
      "Epoch 050 | train acc: 0.9257 | train loss: 0.1718\n",
      "Epoch 051 | train acc: 0.9258 | train loss: 0.1740\n",
      "Epoch 052 | train acc: 0.9258 | train loss: 0.1750\n",
      "Epoch 053 | train acc: 0.9228 | train loss: 0.1825\n",
      "Epoch 054 | train acc: 0.9246 | train loss: 0.1750\n",
      "Epoch 055 | train acc: 0.9256 | train loss: 0.1789\n",
      "Epoch 056 | train acc: 0.9262 | train loss: 0.1739\n",
      "Epoch 057 | train acc: 0.9267 | train loss: 0.1726\n",
      "Epoch 058 | train acc: 0.9246 | train loss: 0.1764\n",
      "Epoch 059 | train acc: 0.9270 | train loss: 0.1708\n",
      "Epoch 060 | train acc: 0.9269 | train loss: 0.1713\n",
      "Epoch 061 | train acc: 0.9256 | train loss: 0.1725\n",
      "Epoch 062 | train acc: 0.9264 | train loss: 0.1742\n",
      "Epoch 063 | train acc: 0.9268 | train loss: 0.1711\n",
      "Epoch 064 | train acc: 0.9249 | train loss: 0.1751\n",
      "Epoch 065 | train acc: 0.9274 | train loss: 0.1725\n",
      "Epoch 066 | train acc: 0.9268 | train loss: 0.1714\n",
      "Epoch 067 | train acc: 0.9276 | train loss: 0.1737\n",
      "Epoch 068 | train acc: 0.9270 | train loss: 0.1728\n",
      "Epoch 069 | train acc: 0.9273 | train loss: 0.1716\n",
      "Epoch 070 | train acc: 0.9256 | train loss: 0.1757\n",
      "Epoch 071 | train acc: 0.9253 | train loss: 0.1751\n",
      "Epoch 072 | train acc: 0.9263 | train loss: 0.1725\n",
      "Epoch 073 | train acc: 0.9258 | train loss: 0.1726\n",
      "Epoch 074 | train acc: 0.9273 | train loss: 0.1706\n",
      "Epoch 075 | train acc: 0.9232 | train loss: 0.1743\n",
      "Epoch 076 | train acc: 0.9276 | train loss: 0.1698\n",
      "Epoch 077 | train acc: 0.9268 | train loss: 0.1704\n",
      "Epoch 078 | train acc: 0.9272 | train loss: 0.1795\n",
      "Epoch 079 | train acc: 0.9275 | train loss: 0.1712\n",
      "Epoch 080 | train acc: 0.9270 | train loss: 0.1729\n",
      "Epoch 081 | train acc: 0.9260 | train loss: 0.1716\n",
      "Epoch 082 | train acc: 0.9248 | train loss: 0.1783\n",
      "Epoch 083 | train acc: 0.9267 | train loss: 0.1727\n",
      "Epoch 084 | train acc: 0.9271 | train loss: 0.1702\n",
      "Epoch 085 | train acc: 0.9269 | train loss: 0.1694\n",
      "Epoch 086 | train acc: 0.9254 | train loss: 0.1747\n",
      "Epoch 087 | train acc: 0.9275 | train loss: 0.1715\n",
      "Epoch 088 | train acc: 0.9273 | train loss: 0.1760\n",
      "Epoch 089 | train acc: 0.9279 | train loss: 0.1694\n",
      "Epoch 090 | train acc: 0.9283 | train loss: 0.1751\n",
      "Epoch 091 | train acc: 0.9275 | train loss: 0.1708\n",
      "Epoch 092 | train acc: 0.9273 | train loss: 0.1719\n",
      "Epoch 093 | train acc: 0.9267 | train loss: 0.1730\n",
      "Epoch 094 | train acc: 0.9288 | train loss: 0.1684\n",
      "Epoch 095 | train acc: 0.9274 | train loss: 0.1723\n",
      "Epoch 096 | train acc: 0.9282 | train loss: 0.1691\n",
      "Epoch 097 | train acc: 0.9279 | train loss: 0.1724\n",
      "Epoch 098 | train acc: 0.9283 | train loss: 0.1702\n",
      "Epoch 099 | train acc: 0.9278 | train loss: 0.1710\n",
      "Final W2: [[ 0.17825221 -1.11316562 -0.0630799   0.03049519  0.08702985 -0.17634887\n",
      "   0.74809647 -0.47550082  0.23208     0.25362372  0.11388464  0.27599717\n",
      "   0.11101295 -0.51795133  0.09755601 -0.21662105]\n",
      " [ 0.12848122  1.31370221  0.18250083  0.57864738 -0.00755605  0.01214924\n",
      "  -0.42793708  0.14452428  0.20359611  0.25245143  0.14636825 -0.49109305\n",
      "   0.02786237  1.03155836  0.03797504 -0.09118245]] Final b2: [-0.25492834  0.25492834]\n",
      "Layer 1 weights:\n",
      "[[-5.29256787e-01  1.26837233e-01  3.73751475e+00 -6.62918035e-01\n",
      "   1.09773621e-01  3.77606480e-01 -9.02992947e-02  1.08121713e+00\n",
      "  -3.34958637e+00 -3.07418567e-01  4.83701043e-02  8.53333652e-01\n",
      "   8.67079468e-01  9.33644928e-01  1.23360042e+00  3.16745724e-01\n",
      "   9.70871388e-01 -4.68045352e-01  6.46307453e-01  3.47935254e+00\n",
      "  -9.92854252e-01 -1.36829436e-01  3.01341230e+00 -7.34927622e-01\n",
      "  -1.12172129e+00]\n",
      " [-1.04515256e+00  5.46986670e-01 -1.54964420e+00  4.33752690e-01\n",
      "   9.21940927e-01  3.98134004e-01  1.24236399e+00 -3.96874418e-01\n",
      "   2.56942426e+00 -5.13043992e-02  1.02803051e-02 -4.49310281e+00\n",
      "  -3.98516549e+00 -4.74565395e+00 -4.51965525e+00 -4.59704717e+00\n",
      "  -3.27582162e+00 -1.30997606e+00 -2.93529258e+00 -5.97697103e+00\n",
      "  -4.81926741e+00 -5.88383975e+00 -5.97088111e+00 -4.54460557e+00\n",
      "  -4.80217496e+00]\n",
      " [-8.11911591e-01  3.14430907e-01 -2.47691268e+00 -2.56120282e+00\n",
      "  -3.67337487e-01  2.61507822e-01 -6.26215191e-01 -1.29967944e+00\n",
      "   4.26053171e+00  1.25696564e-02 -2.79881650e-02  6.24389540e-01\n",
      "   3.48559387e-01  1.45106700e+00  3.02497079e-01  1.18364910e+00\n",
      "   1.06114198e+00  1.83633319e+00 -7.30859943e-01 -4.85147683e-01\n",
      "   1.20571396e+00  1.72701607e+00 -2.72512356e-01  1.61566356e+00\n",
      "   7.22127858e-01]\n",
      " [ 3.97621457e-01 -3.19022836e-01  6.18478828e+00  4.45066044e-01\n",
      "  -4.11629050e-01 -7.35887897e-01 -3.85484527e-01  2.15204742e-01\n",
      "   1.37138972e+00 -2.75834591e-01  8.33222168e-02 -1.07055778e+00\n",
      "  -1.52488699e+00 -1.34375064e+00 -1.43581789e+00 -1.53879672e+00\n",
      "   8.14463245e-01 -2.87777801e+00 -4.03780469e-01 -1.69257276e+00\n",
      "  -1.84303441e+00 -2.15695556e+00 -1.34243882e+00 -1.69295677e+00\n",
      "  -1.32456666e+00]\n",
      " [-5.30209899e-01 -1.80748509e-01 -2.50238920e+00 -3.55610725e+00\n",
      "   6.05545383e-01  7.59040647e-01  3.99143319e+00  1.53067513e+00\n",
      "   6.90842172e+00 -4.96158351e-01  3.06265411e-01  1.93781957e+00\n",
      "   2.71015474e+00  1.93809651e+00  3.17601439e+00  2.23411012e+00\n",
      "   1.79689116e+00  2.03965761e+00 -2.84867936e-01  2.90769311e+00\n",
      "   3.52239424e+00 -1.33467446e-01  3.41809636e+00  3.16842400e+00\n",
      "   3.28332826e+00]\n",
      " [-4.79879589e-01  3.33318671e-01  4.44599360e-02 -3.58846032e-01\n",
      "   2.31949625e-01 -3.51836037e-01 -2.19669226e-01 -2.24329763e+00\n",
      "   2.04404450e+00  4.12113323e-01 -4.69897759e-02 -2.34799011e+00\n",
      "  -2.26999923e+00 -1.87701178e+00 -1.98463129e+00 -2.32319401e+00\n",
      "  -1.42446497e+00 -9.55517933e-01 -1.21321216e+00 -2.16200796e+00\n",
      "  -2.80147069e+00 -1.09395134e+00 -2.44030488e+00 -2.23085702e+00\n",
      "  -2.32070322e+00]\n",
      " [ 2.56491459e-01 -9.57087476e-02  5.02130602e-02  1.85948529e-01\n",
      "  -1.62488796e-01 -4.42245620e-02  1.76966022e-02  9.94510107e-02\n",
      "   8.69644189e-02  1.73693446e-01  1.84146746e-01 -7.72317141e+00\n",
      "  -7.76741534e+00 -7.86122856e+00 -7.72950652e+00 -7.69763135e+00\n",
      "  -4.31958714e+00 -4.19613070e+00 -4.03300902e+00 -8.74055523e+00\n",
      "  -8.69240221e+00 -8.85545843e+00 -8.78814549e+00 -8.82759597e+00\n",
      "  -8.55330894e+00]\n",
      " [ 1.83606349e-01  4.64974927e-02 -4.48078884e-01 -7.81688794e-01\n",
      "  -1.00390509e-01  7.79254928e-01  1.60895006e+00  3.94223138e-01\n",
      "   3.08999983e+00 -1.66663905e-01 -2.42910727e-01 -4.68630697e+00\n",
      "  -5.01786786e+00 -5.01346044e+00 -4.98065852e+00 -4.98875994e+00\n",
      "  -3.32489080e+00 -2.04985170e+00 -3.18601139e+00 -5.73391513e+00\n",
      "  -5.47900180e+00 -6.32395777e+00 -4.96781139e+00 -5.65790778e+00\n",
      "  -4.52032197e+00]\n",
      " [ 3.81264609e-01 -7.23679401e-01 -1.49772041e-02 -5.93900818e-01\n",
      "   2.11552620e-01  1.24395227e-01  1.38152769e+00  8.36072500e-01\n",
      "   5.20644822e+00 -5.25096472e-02 -3.64731992e-01  2.11277479e-02\n",
      "  -4.89981648e-02 -3.85311804e-01  3.40621485e-01 -1.17783552e-01\n",
      "   6.81940744e-01  1.24182835e+00 -2.13530139e+00  1.44498304e-01\n",
      "  -5.75675112e-01  2.99162037e-01 -1.30568131e+00  1.80646400e-01\n",
      "   4.82175069e-01]\n",
      " [-3.05401910e-01  2.15466728e-01  2.66831906e-02  2.11815804e-01\n",
      "   2.02728428e-01  6.87533720e-02  3.11691553e-02 -1.43447690e-01\n",
      "   9.28349698e-02 -2.48939581e-01 -1.70280450e-01 -7.94772478e+00\n",
      "  -7.92726556e+00 -8.12919982e+00 -7.86057381e+00 -7.81845716e+00\n",
      "  -4.21591147e+00 -4.19358465e+00 -4.06420575e+00 -8.61186539e+00\n",
      "  -8.77568902e+00 -8.79903479e+00 -8.57078483e+00 -8.79352438e+00\n",
      "  -8.56201241e+00]\n",
      " [-3.31976381e-01 -4.37579692e-02 -2.04348810e-01 -4.39104792e+00\n",
      "   4.21284263e-02  1.50033985e-01  2.17041515e+00 -1.32200632e-01\n",
      "   4.46914143e+00 -5.86948392e-03  1.21138380e-01  1.30848595e+00\n",
      "   1.52545598e+00  1.36146654e+00  1.74177955e+00  9.78478935e-01\n",
      "   1.82203265e+00  5.99617571e-01 -1.52209508e-01  2.63934671e+00\n",
      "   2.70038457e+00 -1.66053843e+00  1.84203405e+00  2.00994477e+00\n",
      "   2.23803896e+00]\n",
      " [-6.48843168e-02  5.91268041e-01 -3.02195831e-01 -2.10159132e+00\n",
      "   5.66887253e-02  3.91446568e-01  6.67264808e-01 -5.55775630e-01\n",
      "   2.86353288e+00  1.09637354e-01  2.59423081e-01 -9.86263045e-01\n",
      "  -1.01257180e+00 -1.70241856e+00 -1.45913180e+00 -1.72717017e+00\n",
      "   6.84868992e-01 -3.31977486e-01 -2.86844033e+00 -1.64868716e+00\n",
      "  -1.35988047e+00 -1.55623759e+00 -1.47886865e+00 -1.82481170e+00\n",
      "  -1.09994225e+00]\n",
      " [ 2.50456969e-01 -4.91561239e-01  1.37460748e+00 -1.06011994e+00\n",
      "   3.12754158e-01 -1.10202141e+00 -2.45638973e-03  5.04139419e-01\n",
      "  -2.28799437e+00 -2.47126281e-01 -3.10083804e-01  2.88601165e+00\n",
      "   2.30299789e+00  3.46669763e+00  2.69004177e+00  3.25914103e+00\n",
      "   2.10241397e+00  1.18973932e+00  1.37116473e+00  4.22762200e+00\n",
      "   2.59759780e+00  1.73593407e+00  4.13011508e+00  3.08403299e+00\n",
      "   2.45767672e+00]\n",
      " [-1.75595877e-01  2.48148794e-01  5.20535153e-01  7.95749245e+00\n",
      "  -1.49097548e-01 -2.71589978e-01 -2.92974456e+00  3.70380089e+00\n",
      "  -6.22008167e+00 -1.67334396e-01 -7.69958157e-02 -1.71473952e+00\n",
      "  -1.84088500e+00 -2.37992652e+00 -1.79564009e+00 -1.76040221e+00\n",
      "  -4.53615565e+00 -4.90292385e+00  5.90194679e+00 -2.63697551e+00\n",
      "  -2.50942167e+00 -1.93566679e-01 -2.61300383e+00 -2.39575109e+00\n",
      "  -2.79491851e+00]\n",
      " [-3.38657362e-01  1.34922583e-01  1.92496660e+00 -9.64733454e-01\n",
      "   3.36923221e-01 -3.32525119e-01 -8.46785261e-01  6.00884690e-01\n",
      "  -2.77370084e+00  2.44917358e-02 -2.35897544e-01  5.09426031e+00\n",
      "   5.20787129e+00  5.28773181e+00  5.21972865e+00  5.15791679e+00\n",
      "   3.04674087e+00  1.69605834e+00  3.50834758e+00  6.37598722e+00\n",
      "   6.05421975e+00  4.99871696e+00  6.18184250e+00  5.96799201e+00\n",
      "   5.47806315e+00]\n",
      " [-1.38654763e-01 -5.23134466e-02 -1.86567675e+00 -2.03669460e-01\n",
      "   1.32774586e-01  7.42553158e-01  3.60315632e-01 -5.85396635e-02\n",
      "   3.39478648e+00 -9.49906856e-02 -1.46373949e-01 -3.87370510e+00\n",
      "  -4.04596441e+00 -4.87303332e+00 -3.83044394e+00 -3.79425620e+00\n",
      "  -2.68135942e+00 -1.36343092e+00 -3.17144166e+00 -5.09882621e+00\n",
      "  -3.94494131e+00 -5.16619827e+00 -4.76470536e+00 -4.53702969e+00\n",
      "  -3.98495607e+00]]\n",
      "Layer 1 bias:\n",
      "[ -8.63315918 -31.93922845  18.220331   -16.3806642   34.82917946\n",
      " -13.48724151 -59.86476716 -33.89262081   3.74706542 -60.00028507\n",
      "  18.88524152  -3.82339415  14.95711862 -47.61965598  36.57917798\n",
      " -28.70775278]\n",
      "Layer 2 weights:\n",
      "[[-8.27034614e-03  8.08587613e-01 -1.34082086e+00 -1.12847673e+00\n",
      "  -9.63952776e-01  2.15915606e-01 -7.95950540e-01  7.67389864e-02\n",
      "  -1.77454857e+00 -5.32285361e-01 -1.96670316e+00 -2.78946504e+00\n",
      "   2.81666714e-01  1.59645815e+00 -2.44826683e-01 -1.09133064e+00]\n",
      " [ 6.19602277e+00 -6.60543430e+00 -4.09865400e+00 -2.49426204e-01\n",
      "  -5.79851201e+00 -1.82782592e-01 -6.50374574e+00 -5.92263182e+00\n",
      "  -6.79228136e-02 -5.14268283e+00 -1.45863685e-03 -3.59194437e+00\n",
      "   5.27447775e+00  4.95723034e+00  6.47348199e+00 -4.52537627e+00]\n",
      " [-2.59781474e-01 -6.05146236e-01 -8.95890629e-02 -7.06912723e-01\n",
      "  -4.01767040e-01  2.15873778e-01 -2.73746188e+00  1.33280954e+00\n",
      "   1.98863091e-01 -2.01472105e+00 -1.96110792e+00  1.62089562e+00\n",
      "  -1.02428316e+00 -6.53596078e-01  7.10947498e-01  2.45554825e+00]\n",
      " [ 1.50166492e-01 -3.14143438e-01 -7.41867232e-01 -3.29320407e-01\n",
      "  -5.56139190e-01  1.62802603e+00 -1.63470920e+00 -1.28236158e+00\n",
      "  -2.06408690e+00 -8.75925375e-01 -1.76958516e+00 -1.84682074e+00\n",
      "   5.52986845e-01  1.86556326e+00 -1.76078116e-01 -1.00655297e-01]\n",
      " [-8.43173829e-01 -4.09955675e-01  7.23416723e-01 -6.00547737e+00\n",
      "   1.31593922e+00 -2.74817897e-01 -2.04273773e+00 -8.49560556e-01\n",
      "   4.82521731e-01 -2.06170187e+00  7.53792368e-01 -3.99223752e-01\n",
      "  -7.89116270e-01  2.44737341e-01 -1.03675471e+00  3.25030238e+00]\n",
      " [ 6.35549511e-01 -1.05532168e+00 -1.20655476e+00 -1.06278075e+00\n",
      "   4.67012929e-01 -9.57814253e-01  1.29797599e+00 -1.18138314e+00\n",
      "  -1.18033326e+00  1.02971470e+00 -1.25324996e+00 -4.02678188e-01\n",
      "   8.23773377e-01 -3.60387322e-01  2.79607155e-01  1.30488991e-01]\n",
      " [ 3.54377774e+00 -2.77231665e+00 -4.13223583e-01 -3.92943232e-01\n",
      "  -2.37055030e+00  2.43912944e+00 -4.90073253e+00 -2.96189579e+00\n",
      "  -1.36879283e+00 -3.46273668e+00 -3.41577893e-01  3.04159264e-01\n",
      "   3.48892813e+00 -9.42791332e-02  2.81735086e+00  3.01501901e-01]\n",
      " [ 7.10002997e+00 -6.40353667e+00 -5.04365279e+00 -2.22969740e-01\n",
      "  -4.40477062e+00 -2.25104460e-01 -7.37454865e+00 -6.24982179e+00\n",
      "  -3.77749228e-03 -9.18791230e+00  4.99883908e-02 -4.29376422e+00\n",
      "   7.16281446e+00  5.28074739e+00  6.58509060e+00 -5.32475681e+00]\n",
      " [ 3.76252998e+00 -4.55387082e-01  9.24166698e-01  3.67344423e-01\n",
      "   8.86600952e-01  5.12286828e-01 -5.73906770e+00  4.12770458e-01\n",
      "   5.94052993e+00 -7.43965089e+00  6.85874604e+00  7.48265657e+00\n",
      "   5.25467500e+00 -8.73009762e+00  4.12646475e-01 -7.85954489e-01]\n",
      " [ 2.23555258e+00 -2.56677911e+00 -1.44366961e+00 -1.59707515e-01\n",
      "  -1.16995491e+00 -5.09202024e-01 -2.29570794e+00 -1.28681613e+00\n",
      "   2.27043571e-02 -1.45291013e+00 -1.04272501e-02 -2.25009997e+00\n",
      "   1.54038568e+00  2.01552661e+00  1.93247704e+00 -1.99349469e+00]\n",
      " [-1.62941146e+00  1.57134798e+00  8.93431411e-01  5.42463234e-01\n",
      "   1.13873853e+00  9.25130580e-01  1.40689759e+00  8.73393681e-01\n",
      "   3.70294710e-02  1.22612805e+00  3.88142855e-01  1.80142547e-01\n",
      "  -2.08580805e+00 -1.13106051e+00 -1.61855968e+00  1.71318190e+00]\n",
      " [ 1.21495616e+00 -7.92382654e-01 -1.76190408e+00 -3.31404253e-01\n",
      "  -1.44009050e+00 -1.19654379e-01 -4.93600265e-01 -1.15246624e+00\n",
      "  -8.34227185e-01  2.83339185e-01 -1.37424601e+00 -3.13819812e-01\n",
      "   5.62663566e-02  6.03161547e-01  6.63126079e-01 -1.97839623e-01]\n",
      " [ 2.48040949e+00 -7.54233453e-01 -6.56443420e-01 -4.40674809e-01\n",
      "   9.52892246e-01  6.28964077e-01 -1.55389837e-01  3.06357585e-01\n",
      "   2.48465147e+00 -6.86578364e-01  3.19403259e+00  8.75119930e-01\n",
      "   1.28241151e+00  5.39874761e-01  4.88512435e-01 -1.66085576e-01]\n",
      " [ 1.73799901e-01  1.92838391e+00 -5.11359947e-01 -5.99614221e-01\n",
      "   3.51432509e-01  4.95452343e-02 -3.08257766e+00  2.47778690e-01\n",
      "   2.46846665e+00 -3.37357503e+00  2.46320610e+00  4.23882015e-01\n",
      "  -2.17886487e-01 -4.63718794e-01  3.56393170e-01 -5.91616880e-02]\n",
      " [-2.16515262e+00  2.64934396e+00  1.06126344e+00  3.29614866e-01\n",
      "   2.45154238e+00 -5.03319944e-01  2.76420497e+00  3.52028386e+00\n",
      "   1.25160185e+00  1.69907239e+00  1.05217036e+00 -1.38484017e-01\n",
      "  -2.62344731e+00 -1.72784296e+00 -2.75359301e+00  3.80197544e-01]\n",
      " [ 3.25606473e+00  7.47345599e-01 -1.55044335e+00  1.91828931e-01\n",
      "   2.23907361e+00 -4.37935823e+00 -2.73959082e+00  2.00259495e+00\n",
      "   3.38330094e+00 -3.28587565e+00  3.29336051e+00 -1.69474283e+00\n",
      "   3.16526554e+00 -3.11507965e-01  4.14803354e-01 -1.99457254e+00]]\n",
      "Layer 2 bias:\n",
      "[  5.22097004  62.40216064   4.27538585   5.72317925  11.93562858\n",
      "  -1.10170719  20.77820792  71.5933239   49.4567643   19.58183751\n",
      " -12.14603767   5.76221325   8.65333131   7.04848891 -20.81802053\n",
      "  24.0906012 ]\n",
      "Layer 3 weights:\n",
      "[[ 8.08078757e-02  6.23467275e-02  1.31574864e-02 -5.89613559e-03\n",
      "   3.34424616e-02 -1.66330760e-01 -1.59388140e-02 -7.26754716e-02\n",
      "  -3.05974165e-02 -1.14831426e-01 -1.15614280e-01  2.24250861e-05\n",
      "   2.43344457e-02 -6.67969666e-02  2.97683627e-02  8.70460366e-02]\n",
      " [ 2.14851174e+00  6.93188720e+00 -3.37702394e+00  3.92676933e+00\n",
      "  -2.74161516e+00  3.60624301e+00  5.06757900e+00  7.19221496e+00\n",
      "  -2.37478585e+00  5.22291835e+00 -4.69457917e+00  3.81834775e+00\n",
      "  -2.99303807e+00 -2.28774687e+00 -4.95460896e+00 -2.66433073e+00]\n",
      " [-7.98851820e-01  5.36301929e-01 -1.33710552e-01  2.24026125e-01\n",
      "  -5.90959600e-01  2.84361781e-01  5.45270213e-01  9.37048853e-03\n",
      "  -2.37375725e-01 -1.81568694e-01 -2.38915986e-01 -3.40473685e-01\n",
      "  -3.25336817e-01 -3.06555989e-01  3.95243043e-02  3.90902234e-02]\n",
      " [-1.08710688e+00  2.74943147e-01 -1.82791357e-01  2.40821123e-01\n",
      "  -2.05710646e-01  2.47225782e-01  3.84603627e-01  1.72531055e-01\n",
      "  -2.27921741e-01 -6.73492622e-02 -7.30365367e-01 -7.55073317e-01\n",
      "   1.63276441e-01 -6.45155938e-01 -6.57782027e-01 -7.42985801e-01]\n",
      " [ 4.16997086e-01 -2.88216305e-01  7.54057536e-02 -1.67878351e-02\n",
      "   2.68101327e-02 -1.27952072e-01  1.78307661e-02 -1.36263857e-01\n",
      "   2.35381230e-01  2.56836346e-01  1.88391012e-01 -1.55405274e-02\n",
      "  -2.65145080e-01  4.11755801e-01  1.45610025e-01  3.85441290e-01]\n",
      " [-6.94976709e-01  6.88457626e-01 -6.51006132e-03  2.38944913e-01\n",
      "  -7.73876577e-01  1.14265139e-01  2.86424058e-01  5.78191827e-01\n",
      "  -4.22715238e-01  5.40957184e-01 -5.81937578e-01 -1.01828734e+00\n",
      "  -3.79122175e-01 -5.27422447e-01 -2.65196459e-02  5.47853250e-01]\n",
      " [ 7.09146975e-01 -7.88872624e-01  4.68045646e-01  1.17481013e+00\n",
      "   1.00807024e+00 -1.48333720e+00  1.54136398e+00 -4.71704074e-01\n",
      "   2.44599755e+00 -1.40783721e+00  6.88655079e-01 -1.27267434e+00\n",
      "  -1.84092741e+00  1.51752350e+00  1.15419791e+00  7.93624448e-02]\n",
      " [ 2.53750629e-01  8.77321323e-02 -1.06680330e+00  7.60264515e-01\n",
      "  -1.11586069e+00  1.64244494e-01 -4.19646171e-01  6.35751446e-01\n",
      "  -1.03544321e+00  1.03169594e+00 -7.35802726e-01  5.54721299e-01\n",
      "   1.58834046e+00 -1.78110475e-01 -8.29872524e-01 -6.14735405e-01]\n",
      " [ 3.32023290e-01 -1.81064981e-01 -5.19213412e-02 -2.58145491e-01\n",
      "   1.09184728e-01 -2.17929748e-02  3.19503939e-02 -1.38939112e-01\n",
      "   2.40285423e-01  9.22764996e-02  1.27641684e-01  6.74111103e-02\n",
      "  -1.50047828e-01  2.33970081e-01 -2.85214806e-02  1.12929238e-01]\n",
      " [ 2.72128579e-01 -4.14817185e-01 -5.54008395e-02 -9.30388246e-02\n",
      "   1.14326612e-01 -3.96833270e-01  5.26049014e-02 -1.79611432e-02\n",
      "  -6.12725205e-02  2.56364224e-01  1.74276120e-01  3.66275184e-01\n",
      "   3.23716757e-02  1.31358619e-01  2.14780401e-01 -1.28751536e-01]\n",
      " [-1.44560767e-01 -8.46670590e-02  1.45273130e-02  3.79664049e-02\n",
      "  -6.83817997e-02 -6.21500863e-02 -3.76451984e-02  9.65830244e-02\n",
      "  -3.32672571e-01  5.66345200e-02 -1.37006651e-04  5.07841090e-02\n",
      "   3.09536815e-02 -9.77258934e-02 -7.92348075e-02 -1.30060041e-01]\n",
      " [ 3.50408661e-01  8.38196656e-01  1.08223778e+00 -1.04783811e+00\n",
      "   1.46232997e+00  7.68756457e-01 -1.12091351e+00 -1.93768855e-02\n",
      "  -1.15533560e+00  8.00485340e-01 -3.37293371e-01  5.52648455e-02\n",
      "  -1.75298807e-01 -3.79499160e-01  7.07499196e-01  1.58497133e+00]\n",
      " [ 3.62148028e-01 -1.79285202e-01  9.54528578e-02 -1.62946874e-01\n",
      "   8.67900887e-02  1.55402524e-02  2.38376913e-03 -4.97361295e-02\n",
      "   1.92540152e-01  1.46516110e-02  3.29142224e-02  2.16283519e-02\n",
      "  -1.13399562e-01  2.99292753e-01  2.52308478e-02  3.52906758e-01]\n",
      " [ 2.38133808e+00  3.70939210e-02  1.22695794e-01  3.99024998e+00\n",
      "   1.32506562e-01  6.04225588e-01  2.70814533e-01 -6.43721268e-01\n",
      "  -4.72264020e+00 -7.50279229e-01  9.37182952e-02  1.89739804e+00\n",
      "  -4.47663334e+00 -2.46655159e+00 -2.97287248e+00 -4.40907180e+00]\n",
      " [ 3.79955486e-01  1.21839484e-02  9.69291879e-02 -1.72450332e-01\n",
      "   1.55926432e-02 -2.14286016e-01  3.64782998e-03 -1.55756907e-01\n",
      "   1.81345346e-01  1.36382523e-01  8.84519676e-02 -8.20360623e-02\n",
      "   1.76914786e-01 -2.28356017e-01  1.25646709e-01  3.67195084e-01]\n",
      " [-4.79047566e-01 -4.23297151e-02 -1.75028379e-01 -2.14193748e-02\n",
      "  -1.70975714e-01  5.38062743e-02  2.57522922e-02  3.02437100e-01\n",
      "  -4.39596512e-01 -2.10888619e-01 -1.15242858e-01  7.31640537e-02\n",
      "  -6.50568485e-02 -3.22125349e-01 -2.34601141e-01 -2.86603996e-01]]\n",
      "Layer 3 bias:\n",
      "[-6.09301897e-02  6.27337793e+01  4.87236033e-01  1.12176754e+00\n",
      " -3.11124140e-01 -8.08905347e-01  5.50334398e+00 -3.20371457e+00\n",
      " -2.39007888e-01  2.88965171e-01  4.22749896e-01  1.91324085e-01\n",
      " -4.61044142e-01 -1.65315541e+01 -4.94827891e-01  7.47233357e-01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# === Funções auxiliares ===\n",
    "def glorot_uniform(fan_in, fan_out):\n",
    "    limit = np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return np.random.uniform(-limit, limit, size=(fan_out, fan_in))\n",
    "\n",
    "def init_vector_glorot(fan_out):\n",
    "    return np.zeros(fan_out)\n",
    "\n",
    "def softmax(z):\n",
    "    z = z - np.max(z)\n",
    "    e = np.exp(z)\n",
    "    return e / np.sum(e)\n",
    "\n",
    "def one_hot(y_i, K):\n",
    "    v = np.zeros(K, dtype=float)\n",
    "    v[y_i] = 1.0\n",
    "    return v\n",
    "\n",
    "def cross_entropy(y_true_one_hot, p):\n",
    "    return -np.sum(y_true_one_hot * np.log(p + 1e-12))\n",
    "\n",
    "tanh = np.tanh\n",
    "tanhp = lambda a: (1.0 - a**2)\n",
    "\n",
    "# === Classe da camada ===\n",
    "class HiddenLayer:\n",
    "    def __init__(self, fan_in, fan_out):\n",
    "        self.W = glorot_uniform(fan_in, fan_out)\n",
    "        self.b = init_vector_glorot(fan_out)\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "        self.prev_a = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.prev_a = x\n",
    "        self.z = self.W @ x + self.b\n",
    "        self.a = tanh(self.z)\n",
    "        return self.a\n",
    "\n",
    "# === Forward total ===\n",
    "def forward_probs(x, layers, W2, b2):\n",
    "    a = x\n",
    "    for layer in layers:\n",
    "        a = layer.forward(a)\n",
    "    z2 = W2 @ a + b2\n",
    "    p = softmax(z2)\n",
    "    return p, a\n",
    "\n",
    "\n",
    "# === Configuração da rede ===\n",
    "K = len(np.unique(y))                 \n",
    "input_dim = X_train.shape[1]\n",
    "H = 16\n",
    "NLayers = 3\n",
    "\n",
    "HiddenLayers = []\n",
    "HiddenLayers.append(HiddenLayer(input_dim, H))\n",
    "for _ in range(NLayers - 1):\n",
    "    HiddenLayers.append(HiddenLayer(H, H))\n",
    "\n",
    "W2 = glorot_uniform(H, K)\n",
    "b2 = np.zeros(K)\n",
    "\n",
    "eta = 0.01     \n",
    "epochs = 100\n",
    "clip_value = 5.0\n",
    "\n",
    "N = len(X_train)\n",
    "indices = np.arange(N)\n",
    "\n",
    "# === Histórico para gráficos ===\n",
    "train_acc_history = []\n",
    "train_loss_history = []\n",
    "\n",
    "# === Loop de treino ===\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for idx in indices:\n",
    "        x_i = X_train[idx]\n",
    "        y_i = int(y_train[idx])\n",
    "\n",
    "        # forward\n",
    "        p, a_last = forward_probs(x_i, HiddenLayers, W2, b2)\n",
    "        y_one = one_hot(y_i, K)\n",
    "        total_loss += cross_entropy(y_one, p)\n",
    "\n",
    "        delta_out = p - y_one\n",
    "        W2_old = W2.copy()\n",
    "\n",
    "        # grads output\n",
    "        dW2 = np.outer(delta_out, a_last)\n",
    "        db2 = delta_out\n",
    "\n",
    "        next_delta = delta_out\n",
    "        next_W = W2_old\n",
    "\n",
    "        # backward hidden\n",
    "        for layer in reversed(HiddenLayers):\n",
    "            g = next_W.T @ next_delta\n",
    "            delta = g * tanhp(layer.a)\n",
    "            dW = np.outer(delta, layer.prev_a)\n",
    "            db = delta\n",
    "\n",
    "            W_old = layer.W.copy()\n",
    "\n",
    "            # gradient clipping\n",
    "            if np.linalg.norm(dW) > clip_value:\n",
    "                dW *= clip_value / (np.linalg.norm(dW) + 1e-12)\n",
    "            if np.linalg.norm(db) > clip_value:\n",
    "                db *= clip_value / (np.linalg.norm(db) + 1e-12)\n",
    "\n",
    "            layer.W -= eta * dW\n",
    "            layer.b -= eta * db\n",
    "\n",
    "            next_delta = delta\n",
    "            next_W = W_old\n",
    "\n",
    "        # update output layer\n",
    "        if np.linalg.norm(dW2) > clip_value:\n",
    "            dW2 *= clip_value / (np.linalg.norm(dW2) + 1e-12)\n",
    "        if np.linalg.norm(db2) > clip_value:\n",
    "            db2 *= clip_value / (np.linalg.norm(db2) + 1e-12)\n",
    "\n",
    "        W2 -= eta * dW2\n",
    "        b2 -= eta * db2\n",
    "\n",
    "    # === Avaliação após cada época ===\n",
    "    probs = np.array([forward_probs(x_i, HiddenLayers, W2, b2)[0] for x_i in X_train])\n",
    "    y_pred = np.argmax(probs, axis=1)\n",
    "\n",
    "    acc = accuracy_score(y_train, y_pred)\n",
    "    loss = np.mean([-np.log(probs[i, y_train[i]] + 1e-12) for i in range(len(y_train))])\n",
    "\n",
    "    train_acc_history.append(acc)\n",
    "    train_loss_history.append(loss)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | train acc: {acc:.4f} | train loss: {loss:.4f}\")\n",
    "\n",
    "\n",
    "print(\"Final W2:\", W2, \"Final b2:\", b2)\n",
    "for i, layer in enumerate(HiddenLayers, 1):\n",
    "    print(f\"Layer {i} weights:\\n{layer.W}\\nLayer {i} bias:\\n{layer.b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab577c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.9168426903835464\n"
     ]
    }
   ],
   "source": [
    "\n",
    "probs = np.array([forward_probs(x_i, HiddenLayers, W2, b2)[0] for x_i in X_test])  # (N, K)\n",
    "y_pred = np.argmax(probs, axis=1)\n",
    "print(\"Testing Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e56a576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Set Metrics ===\n",
      "Accuracy: 0.9168426903835464\n",
      "Precision: 0.9148169231833273\n",
      "Recall: 0.9168426903835464\n",
      "F1-score: 0.914907759420784\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.95      6995\n",
      "           1       0.86      0.75      0.80      2000\n",
      "\n",
      "    accuracy                           0.92      8995\n",
      "   macro avg       0.89      0.86      0.87      8995\n",
      "weighted avg       0.91      0.92      0.91      8995\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "probs_test = np.array([forward_probs(x_i, HiddenLayers, W2, b2)[0] for x_i in X_test])\n",
    "y_pred_test = np.argmax(probs_test, axis=1)\n",
    "\n",
    "print(\"=== Test Set Metrics ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_test, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_test, average='weighted'))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred_test, average='weighted'))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05577b8",
   "metadata": {},
   "source": [
    "\n",
    "## Model Evaluation\n",
    "\n",
    "### Metrics\n",
    "\n",
    "| Metric    | Value     |\n",
    "| --------- | --------- |\n",
    "| Accuracy  | **0.917** |\n",
    "| Precision | **0.915** |\n",
    "| Recall    | **0.917** |\n",
    "| F1-score  | **0.915** |\n",
    "\n",
    "These results indicate that the model performs **well overall**, maintaining a **balanced trade-off between precision and recall**, which shows consistent behavior when identifying both positive and negative classes.\n",
    "\n",
    "---\n",
    "\n",
    "### Training Loss and Training Accuracy Curves\n",
    "\n",
    "```python\n",
    "# === Plots ===\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "# Loss curve\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_loss_history, label='Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy curve\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_acc_history, label='Accuracy', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "![Training Loss and Accuracy Curves](./images/output.png)\n",
    "\n",
    "During training, we can observe that:\n",
    "\n",
    "* The **loss curve** decreases rapidly during the first epochs and stabilizes around 0.17, indicating that the model **converged successfully**.\n",
    "* The **accuracy curve** rises quickly and remains stable around 0.917, suggesting that the model **learned consistently**.\n",
    "\n",
    "These curves demonstrate that the training process was **stable and efficient**.\n",
    "\n",
    "---\n",
    "\n",
    "### Class-wise Performance\n",
    "\n",
    "| Class | Precision | Recall | F1-score | Support |\n",
    "| ----- | --------- | ------ | -------- | ------- |\n",
    "| **0** | 0.93      | 0.96   | 0.95     | 6995    |\n",
    "| **1** | 0.91      | 0.92   | 0.91     | 2000    |\n",
    "\n",
    "The model shows **excellent performance for both classes**, though class 1 still has slightly lower metrics than class 0. Overall, the model is **well-balanced** in handling both majority and minority classes.\n",
    "\n",
    "---\n",
    "\n",
    "### Discussion\n",
    "\n",
    "Overall, the model demonstrates effective learning and good generalization on the test set.\n",
    "It maintains consistent performance across accuracy, precision, recall, and F1-score, indicating stable training behavior.\n",
    "The performance on the minority class (class 1) has **improved compared to previous results**, suggesting that the model is now more sensitive to less frequent samples.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The model achieved **strong and consistent performance**, reaching about **91.7% accuracy** on the test set.\n",
    "The learning curves indicate **successful convergence** with **no clear signs of overfitting**.\n",
    "With a **balanced handling of both classes**, the model demonstrates **robust generalization ability** and provides a reliable baseline for further improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b5a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
