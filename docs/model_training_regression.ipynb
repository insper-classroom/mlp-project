{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1018c4cc",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron\n",
    "\n",
    "This is the Notebook used for the Implementation, Training and Testing of a numpy-based Multi Layer Perceptron classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4184a4d8",
   "metadata": {},
   "source": [
    "## Importing Data - Preparation\n",
    "\n",
    "Separating data into training, validation and testing. This process uses the Scikit-Learn [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba2c5dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1500, 4) y shape: (1500,)\n",
      "Feature means (first 5): [0.01363166 0.00552647 0.02209796 0.03360056]\n",
      "Target stats: mean=22.389, std=296.309\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Reproducibility\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "def sample_regression_subset(\n",
    "    n_needed: int,\n",
    "    n_features: int,\n",
    "    seed: int,\n",
    "    *,\n",
    "    n_informative: int = None,\n",
    "    bias: float = 0.0,\n",
    "    noise: float = 10.0,\n",
    "    coef_range: tuple = (1.0, 5.0),\n",
    "    cluster_shift: float = 0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a synthetic regression subset with a controllable shift/noise pattern.\n",
    "    Each call can represent one 'region' or 'cluster' of data with its own difficulty.\n",
    "    \"\"\"\n",
    "    if n_informative is None:\n",
    "        n_informative = n_features\n",
    "\n",
    "    X, y, coef = make_regression(\n",
    "        n_samples=n_needed,\n",
    "        n_features=n_features,\n",
    "        n_informative=n_informative,\n",
    "        noise=noise,\n",
    "        bias=bias,\n",
    "        coef=True,\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "    # Add random coefficient scaling to diversify clusters\n",
    "    coef_scale = rng.uniform(*coef_range, size=n_features)\n",
    "    y = X @ (coef * coef_scale) + bias + rng.randn(n_needed) * noise\n",
    "\n",
    "    # Shift features to form separated clusters\n",
    "    X += cluster_shift\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# ---------- Build an asymmetric multi-cluster regression dataset ----------\n",
    "N = 1500\n",
    "n_features = 4\n",
    "clusters = [\n",
    "    dict(n_needed=500, noise=5.0,  bias=20.0,  cluster_shift=-2.0, seed=42),\n",
    "    dict(n_needed=500, noise=10.0, bias=-15.0, cluster_shift=0.0,  seed=1337),\n",
    "    dict(n_needed=500, noise=15.0, bias=35.0,  cluster_shift=+2.0, seed=2027),\n",
    "]\n",
    "\n",
    "Xs, ys = [], []\n",
    "for cfg in clusters:\n",
    "    Xi, yi = sample_regression_subset(n_features=n_features, **cfg)\n",
    "    Xs.append(Xi)\n",
    "    ys.append(yi)\n",
    "\n",
    "# Combine and shuffle\n",
    "X = np.vstack(Xs)\n",
    "y = np.concatenate(ys)\n",
    "perm = rng.permutation(len(y))\n",
    "X = X[perm]\n",
    "y = y[perm]\n",
    "\n",
    "print(\"X shape:\", X.shape, \"y shape:\", y.shape)\n",
    "print(\"Feature means (first 5):\", X.mean(axis=0)[:5])\n",
    "print(\"Target stats: mean={:.3f}, std={:.3f}\".format(y.mean(), y.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01451e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (1200, 4) (300, 4)\n",
      "Value range (train): -5.241267340069072 to 5.202304125686403\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Shapes:\", X_train.shape, X_test.shape)\n",
    "print(\"Value range (train):\", float(X_train.min()), \"to\", float(X_train.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d7f6d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | RMSE: 287.1662 | MAE: 213.2945 | R2: 0.0294\n",
      "Epoch 001 | RMSE: 284.8587 | MAE: 206.6814 | R2: 0.0449\n",
      "Epoch 002 | RMSE: 283.6301 | MAE: 203.6634 | R2: 0.0531\n",
      "Epoch 003 | RMSE: 280.8880 | MAE: 196.0130 | R2: 0.0714\n",
      "Epoch 004 | RMSE: 278.5997 | MAE: 193.2958 | R2: 0.0864\n",
      "Epoch 005 | RMSE: 276.1665 | MAE: 190.0861 | R2: 0.1023\n",
      "Epoch 006 | RMSE: 275.0305 | MAE: 186.5900 | R2: 0.1097\n",
      "Epoch 007 | RMSE: 273.8199 | MAE: 183.6943 | R2: 0.1175\n",
      "Epoch 008 | RMSE: 272.5194 | MAE: 179.2736 | R2: 0.1259\n",
      "Epoch 009 | RMSE: 266.8472 | MAE: 174.2831 | R2: 0.1619\n",
      "Epoch 010 | RMSE: 256.3436 | MAE: 165.5892 | R2: 0.2266\n",
      "Epoch 011 | RMSE: 251.6058 | MAE: 159.9309 | R2: 0.2549\n",
      "Epoch 012 | RMSE: 247.7754 | MAE: 154.0929 | R2: 0.2774\n",
      "Epoch 013 | RMSE: 250.4832 | MAE: 151.8115 | R2: 0.2615\n",
      "Epoch 014 | RMSE: 239.2455 | MAE: 151.5356 | R2: 0.3263\n",
      "Epoch 015 | RMSE: 215.1983 | MAE: 133.4564 | R2: 0.4549\n",
      "Epoch 016 | RMSE: 201.1432 | MAE: 123.6979 | R2: 0.5238\n",
      "Epoch 017 | RMSE: 200.7460 | MAE: 119.8873 | R2: 0.5257\n",
      "Epoch 018 | RMSE: 185.6689 | MAE: 100.7903 | R2: 0.5942\n",
      "Epoch 019 | RMSE: 189.2024 | MAE: 100.5728 | R2: 0.5787\n",
      "Epoch 020 | RMSE: 188.5939 | MAE: 109.0983 | R2: 0.5814\n",
      "Epoch 021 | RMSE: 180.4913 | MAE: 92.7858 | R2: 0.6166\n",
      "Epoch 022 | RMSE: 179.6159 | MAE: 102.3254 | R2: 0.6203\n",
      "Epoch 023 | RMSE: 172.0733 | MAE: 78.9603 | R2: 0.6515\n",
      "Epoch 024 | RMSE: 176.5039 | MAE: 86.3380 | R2: 0.6333\n",
      "Epoch 025 | RMSE: 173.3568 | MAE: 85.2347 | R2: 0.6463\n",
      "Epoch 026 | RMSE: 174.7087 | MAE: 92.5173 | R2: 0.6407\n",
      "Epoch 027 | RMSE: 172.1930 | MAE: 85.1264 | R2: 0.6510\n",
      "Epoch 028 | RMSE: 179.9872 | MAE: 95.9185 | R2: 0.6187\n",
      "Epoch 029 | RMSE: 170.6413 | MAE: 86.0192 | R2: 0.6573\n",
      "Epoch 030 | RMSE: 170.6526 | MAE: 77.8289 | R2: 0.6572\n",
      "Epoch 031 | RMSE: 172.7415 | MAE: 86.6276 | R2: 0.6488\n",
      "Epoch 032 | RMSE: 168.5865 | MAE: 79.5085 | R2: 0.6655\n",
      "Epoch 033 | RMSE: 173.0107 | MAE: 84.9194 | R2: 0.6477\n",
      "Epoch 034 | RMSE: 169.8839 | MAE: 82.8367 | R2: 0.6603\n",
      "Epoch 035 | RMSE: 169.6431 | MAE: 83.6289 | R2: 0.6613\n",
      "Epoch 036 | RMSE: 168.5386 | MAE: 81.3992 | R2: 0.6657\n",
      "Epoch 037 | RMSE: 169.7221 | MAE: 78.1726 | R2: 0.6610\n",
      "Epoch 038 | RMSE: 165.0717 | MAE: 73.4156 | R2: 0.6793\n",
      "Epoch 039 | RMSE: 165.2349 | MAE: 75.9745 | R2: 0.6786\n",
      "Epoch 040 | RMSE: 164.3005 | MAE: 76.0738 | R2: 0.6823\n",
      "Epoch 041 | RMSE: 167.4303 | MAE: 80.8375 | R2: 0.6700\n",
      "Epoch 042 | RMSE: 165.0707 | MAE: 73.4089 | R2: 0.6793\n",
      "Epoch 043 | RMSE: 162.5824 | MAE: 70.2413 | R2: 0.6889\n",
      "Epoch 044 | RMSE: 166.5658 | MAE: 76.2702 | R2: 0.6734\n",
      "Epoch 045 | RMSE: 169.8878 | MAE: 85.4071 | R2: 0.6603\n",
      "Epoch 046 | RMSE: 165.1262 | MAE: 75.2488 | R2: 0.6791\n",
      "Epoch 047 | RMSE: 163.1928 | MAE: 72.6977 | R2: 0.6865\n",
      "Epoch 048 | RMSE: 160.8501 | MAE: 70.6776 | R2: 0.6955\n",
      "Epoch 049 | RMSE: 161.7308 | MAE: 70.3560 | R2: 0.6921\n",
      "Epoch 050 | RMSE: 164.2877 | MAE: 75.3825 | R2: 0.6823\n",
      "Epoch 051 | RMSE: 166.1303 | MAE: 77.0447 | R2: 0.6752\n",
      "Epoch 052 | RMSE: 162.7698 | MAE: 71.3417 | R2: 0.6882\n",
      "Epoch 053 | RMSE: 160.3215 | MAE: 72.9709 | R2: 0.6975\n",
      "Epoch 054 | RMSE: 163.8629 | MAE: 74.6813 | R2: 0.6840\n",
      "Epoch 055 | RMSE: 161.8805 | MAE: 73.1667 | R2: 0.6916\n",
      "Epoch 056 | RMSE: 157.1414 | MAE: 64.9046 | R2: 0.7094\n",
      "Epoch 057 | RMSE: 162.1127 | MAE: 64.6875 | R2: 0.6907\n",
      "Epoch 058 | RMSE: 164.0490 | MAE: 65.6782 | R2: 0.6832\n",
      "Epoch 059 | RMSE: 162.1293 | MAE: 68.7164 | R2: 0.6906\n",
      "Epoch 060 | RMSE: 158.0842 | MAE: 64.2897 | R2: 0.7059\n",
      "Epoch 061 | RMSE: 156.5180 | MAE: 63.5877 | R2: 0.7117\n",
      "Epoch 062 | RMSE: 161.2367 | MAE: 66.7580 | R2: 0.6940\n",
      "Epoch 063 | RMSE: 159.1301 | MAE: 71.1970 | R2: 0.7020\n",
      "Epoch 064 | RMSE: 158.1698 | MAE: 69.0374 | R2: 0.7055\n",
      "Epoch 065 | RMSE: 167.4965 | MAE: 72.6868 | R2: 0.6698\n",
      "Epoch 066 | RMSE: 163.8711 | MAE: 68.9543 | R2: 0.6839\n",
      "Epoch 067 | RMSE: 159.4107 | MAE: 68.9002 | R2: 0.7009\n",
      "Epoch 068 | RMSE: 158.0298 | MAE: 59.9378 | R2: 0.7061\n",
      "Epoch 069 | RMSE: 160.3202 | MAE: 74.8614 | R2: 0.6975\n",
      "Epoch 070 | RMSE: 156.1166 | MAE: 60.8804 | R2: 0.7131\n",
      "Epoch 071 | RMSE: 162.5661 | MAE: 75.8986 | R2: 0.6889\n",
      "Epoch 072 | RMSE: 153.5674 | MAE: 61.8629 | R2: 0.7224\n",
      "Epoch 073 | RMSE: 155.2980 | MAE: 62.5012 | R2: 0.7161\n",
      "Epoch 074 | RMSE: 159.0835 | MAE: 64.3923 | R2: 0.7021\n",
      "Epoch 075 | RMSE: 166.4672 | MAE: 75.0591 | R2: 0.6738\n",
      "Epoch 076 | RMSE: 158.7872 | MAE: 67.6143 | R2: 0.7032\n",
      "Epoch 077 | RMSE: 160.3668 | MAE: 63.0948 | R2: 0.6973\n",
      "Epoch 078 | RMSE: 156.3799 | MAE: 60.4188 | R2: 0.7122\n",
      "Epoch 079 | RMSE: 160.0201 | MAE: 62.4142 | R2: 0.6986\n",
      "Epoch 080 | RMSE: 167.0431 | MAE: 62.1401 | R2: 0.6716\n",
      "Epoch 081 | RMSE: 153.2703 | MAE: 60.1212 | R2: 0.7235\n",
      "Epoch 082 | RMSE: 167.7066 | MAE: 74.9309 | R2: 0.6690\n",
      "Epoch 083 | RMSE: 155.1948 | MAE: 63.5081 | R2: 0.7165\n",
      "Epoch 084 | RMSE: 155.2499 | MAE: 56.5390 | R2: 0.7163\n",
      "Epoch 085 | RMSE: 164.9753 | MAE: 77.2320 | R2: 0.6797\n",
      "Epoch 086 | RMSE: 153.6932 | MAE: 55.0913 | R2: 0.7220\n",
      "Epoch 087 | RMSE: 156.5245 | MAE: 64.0873 | R2: 0.7116\n",
      "Epoch 088 | RMSE: 168.7486 | MAE: 83.6712 | R2: 0.6648\n",
      "Epoch 089 | RMSE: 154.0572 | MAE: 55.2867 | R2: 0.7207\n",
      "Epoch 090 | RMSE: 153.8936 | MAE: 55.8463 | R2: 0.7212\n",
      "Epoch 091 | RMSE: 153.4758 | MAE: 56.0364 | R2: 0.7228\n",
      "Epoch 092 | RMSE: 157.7867 | MAE: 65.3941 | R2: 0.7070\n",
      "Epoch 093 | RMSE: 152.3183 | MAE: 58.2043 | R2: 0.7269\n",
      "Epoch 094 | RMSE: 154.6565 | MAE: 59.4449 | R2: 0.7185\n",
      "Epoch 095 | RMSE: 152.1840 | MAE: 57.9893 | R2: 0.7274\n",
      "Epoch 096 | RMSE: 157.3408 | MAE: 63.3941 | R2: 0.7086\n",
      "Epoch 097 | RMSE: 162.7244 | MAE: 68.9336 | R2: 0.6883\n",
      "Epoch 098 | RMSE: 155.8694 | MAE: 64.4491 | R2: 0.7140\n",
      "Epoch 099 | RMSE: 152.5110 | MAE: 52.1342 | R2: 0.7262\n",
      "Final W2: [[-41.0229384  -47.7719647  -43.79362131  45.57548202  61.99900275\n",
      "   43.95538233  43.80513168  24.75424028  48.70842175 -43.83232856\n",
      "  -46.04901545  36.69349428  36.13725789 -51.83671315  45.57408554\n",
      "   23.5909675 ]] Final b2: [27.39539407]\n",
      "Layer 1 weights:\n",
      "[[ 2.89883001e+00  1.52483869e+00  2.06086944e+00  2.03834705e+00]\n",
      " [-7.02171393e-01 -1.13575621e+00 -3.06137237e+00 -2.81479298e+00]\n",
      " [-9.40426007e-01 -1.11766390e+00 -3.41564898e+00 -3.12477999e+00]\n",
      " [-9.46709545e-02 -1.10737117e+00 -4.78399840e-01 -1.62186017e+00]\n",
      " [ 1.17244853e-01  4.54966411e-01  3.03644491e-01  9.43655504e-01]\n",
      " [ 4.98792031e-01  7.93071676e-01  1.53302735e+00  1.55285633e+00]\n",
      " [ 1.66925298e+00  1.64035455e+00  1.16409746e+00  1.32228833e+00]\n",
      " [ 7.64067533e-01  3.15674649e-01  1.30547691e+00  2.00342781e-01]\n",
      " [-6.19596968e-01 -8.13230377e-01 -2.90011598e+00 -2.64822142e+00]\n",
      " [-4.65671723e-01 -7.21752482e-01 -2.04681361e+00 -1.83856677e+00]\n",
      " [ 1.07311532e+00  1.79614825e+00  1.47414201e+00  2.04852606e+00]\n",
      " [ 1.10169861e+00  4.71533677e-01  2.01980145e+00  4.51086611e-01]\n",
      " [-1.01549329e+00 -1.31240655e+00 -2.34514912e+00 -2.69187972e+00]\n",
      " [-2.09124064e-03  7.27041957e-01 -7.70917424e-01 -1.14398132e-01]\n",
      " [ 3.81611685e-01  1.54722204e+00  8.50625789e-01  2.69086890e+00]\n",
      " [-9.06186524e-01 -4.54471047e-02 -4.10342188e-01  1.25847561e+00]]\n",
      "Layer 1 bias:\n",
      "[  7.10110367  -0.19263796  -3.02926766   4.28752676  -3.33989965\n",
      "  -2.78087811  -5.93717454   4.66895163  -4.39064439   1.40789921\n",
      "   3.70133554  10.39474483   0.05271836  -1.15788106 -14.65514646\n",
      "   6.86086959]\n",
      "Layer 2 weights:\n",
      "[[ 7.76167528e-02  1.47771043e+00  7.05571275e-01  4.32346555e-01\n",
      "  -1.75792893e-01 -7.74169934e-01 -9.31175891e-02 -5.12029269e-01\n",
      "   9.47871602e-02  1.38479584e+00  3.55434601e-01 -2.18601385e-01\n",
      "   1.50329015e+00  3.18563563e-01  2.97155194e-01  4.44485125e-01]\n",
      " [ 4.96069899e-03  1.75508466e+00  1.22302758e+00  6.32498927e-01\n",
      "  -1.93524318e-01 -2.27945601e-01  1.07981654e-01 -3.71510503e-01\n",
      "   3.86761054e-02  8.47736929e-01  3.66101165e-01  4.57993716e-01\n",
      "   1.40780387e+00  5.72917634e-01  5.13073111e-02  1.75943375e-02]\n",
      " [ 1.92306671e+00 -7.17396426e-02 -1.62835887e-01 -6.68990607e-01\n",
      "  -2.44066825e+00  6.99510080e-02  1.29390446e+00 -4.87161028e+00\n",
      "  -8.48040930e-01 -1.19177091e-01  1.87588296e+00 -5.87504409e+00\n",
      "  -1.26517860e-01  4.03420209e-01 -4.63809814e+00 -2.89108939e+00]\n",
      " [ 1.32736815e+00 -5.44943725e-01 -5.12225972e-01  3.84491370e-02\n",
      "  -3.94080249e-01 -4.44357343e-02 -3.44100712e-01  9.62025117e-01\n",
      "  -7.26695106e-01 -2.10939713e-01  8.35479566e-01  3.33057427e-01\n",
      "  -3.96613085e-01 -4.41119679e-01 -3.52634030e-01  1.15154335e-01]\n",
      " [-3.78939952e-01  1.69947515e+00  1.93321081e+00  1.56611744e-01\n",
      "  -3.36180685e-01 -7.92233504e-01 -3.15711071e-02 -2.54843535e-01\n",
      "   7.73803492e-01  9.00015503e-01 -1.09714516e-01  6.50191906e-02\n",
      "   1.28329846e+00  1.39409559e-01  5.02110615e-02  1.76786214e-01]\n",
      " [ 1.55655029e+00  4.52126240e-01 -3.55115654e-02  1.28481147e+00\n",
      "  -3.70612251e+00 -1.17432245e+00  6.44661618e-01 -7.17764915e-01\n",
      "  -4.67704732e-01  7.73145505e-01  1.25651004e+00 -2.88582087e+00\n",
      "  -2.77168822e-01 -2.84932066e-02 -5.50027182e+00  1.46706788e+00]\n",
      " [ 1.84531131e-01 -6.53550806e-01 -3.07735951e-01 -4.84109673e-01\n",
      "   1.59840978e-01  5.57472079e-01  4.44327576e-01  4.46851784e-01\n",
      "  -1.12026510e+00 -9.01581129e-01 -1.74911821e-01 -1.01586782e-01\n",
      "  -8.14816419e-01 -5.65864032e-01  5.47493547e-01 -2.86999114e-01]\n",
      " [ 1.96718895e+00 -6.90471245e-01  2.81473482e-01  2.26398309e+00\n",
      "  -5.68134382e+00  7.72953391e-01  2.54770077e+00  1.09703263e+00\n",
      "   6.15852515e-01 -7.96733947e-01  7.60153298e-01 -2.92033920e-01\n",
      "   1.90633789e-01  6.22323169e-01 -6.25111453e+00  1.71815686e+00]\n",
      " [-5.46531207e-01 -5.51394089e-02  7.67524504e-01  6.03189243e-01\n",
      "  -3.10729507e-01 -3.98820006e-01  3.02325584e-01 -1.00638043e+00\n",
      "   2.00292414e+00  8.90158045e-01 -7.52178467e-01 -1.46571208e-01\n",
      "   4.61929956e-01  8.99604783e-01 -3.42210603e-01 -7.40947878e-02]\n",
      " [ 4.31008604e-01 -5.80745717e-01 -1.25494763e+00 -5.25548437e-01\n",
      "  -1.22077228e-01  6.07519481e-01 -4.96926320e-02  1.60738239e-01\n",
      "  -1.57291730e+00 -1.55362580e-01  5.49914304e-01  2.94121841e-01\n",
      "  -1.33034018e-01 -4.53771225e-02  6.06638393e-02  1.30485335e-01]\n",
      " [ 2.44451161e-01  6.36112309e-01  1.25434630e+00 -1.03537667e+00\n",
      "  -3.07577714e+00  8.20063853e-01  2.71145068e+00 -4.56552022e-01\n",
      "   1.27908625e+00 -5.24245421e-01 -6.14601416e-02 -3.12867712e+00\n",
      "   1.06321663e+00 -3.35067539e-01 -2.78851069e+00 -4.56169342e-01]\n",
      " [ 3.72807185e+00  1.51092542e+00  6.97622784e-01 -2.30701320e+00\n",
      "  -2.00255765e+00 -1.69710485e+00  2.65972393e+00 -1.23755365e+00\n",
      "  -1.04575290e+00  1.86668161e+00  2.03680719e+00 -1.99226831e+00\n",
      "   1.07390302e+00  1.19725441e-01 -4.10230113e+00 -1.63957547e+00]\n",
      " [ 5.17973890e-01 -7.55906199e-01 -6.82975091e-01 -2.99497053e-01\n",
      "  -1.67186349e-02  3.78499453e-01 -2.28518448e-01  6.28918929e-01\n",
      "  -1.40949318e+00 -5.40172093e-01  9.97234849e-01  3.66669824e-01\n",
      "  -3.47323667e-01 -9.48844594e-01 -1.17780090e-01  1.48441862e-01]\n",
      " [-3.43379366e+00 -4.85821903e-01 -3.44141070e-01  1.07277439e+00\n",
      "   2.62606138e+00  2.58071499e+00 -1.31887260e+00  6.34361291e-01\n",
      "   1.22144955e+00 -1.74893517e+00 -2.24192953e+00  2.10277193e+00\n",
      "  -4.02272544e-01 -3.44518595e-02  4.71768718e+00 -1.86764016e-01]\n",
      " [ 2.49379507e+00  5.08911202e-01  1.12568664e-01 -1.10557471e+00\n",
      "  -2.15766601e+00 -1.96266769e+00  1.41435394e+00 -2.05756922e+00\n",
      "  -9.62547047e-01  1.27895428e+00  2.14688658e+00 -3.01070803e+00\n",
      "   7.12570470e-01 -9.28039140e-03 -3.29146069e+00 -9.03888801e-01]\n",
      " [-3.99449838e-01  1.11026241e+00  1.01692433e+00  9.37894091e-01\n",
      "   1.56460602e-02 -1.23584661e+00 -1.07010468e-01 -1.31399066e-01\n",
      "   4.77396046e-01  9.44206597e-01  2.61295672e-01  1.73068638e-01\n",
      "   8.88950725e-01 -2.77815766e-01 -4.07767242e-01  2.75494863e-01]]\n",
      "Layer 2 bias:\n",
      "[  1.79894202   0.06870332  12.35298199   1.63291812  -1.35375377\n",
      "  -4.92459262   1.37110617 -19.30018527  -4.16313305   1.98351222\n",
      "   1.83817896  -0.05327883   2.5205749    4.17373175   1.21701871\n",
      "   1.14871422]\n",
      "Layer 3 weights:\n",
      "[[ 8.96758389e-01  8.29745695e-01  2.29889655e+00  3.76675680e-01\n",
      "   1.26161600e+00  4.53871723e+00 -1.18860942e-01  4.98134950e+00\n",
      "  -3.55063371e-01 -2.84966881e-01  2.36893114e+00  1.92927728e+00\n",
      "  -3.39693363e-02 -2.06754053e+00  1.28186922e+00  9.42822427e-02]\n",
      " [-1.71306131e-01  7.51117610e-01  4.52580631e+00 -6.10436092e-01\n",
      "   1.15213492e+00  5.90398242e+00 -1.07916081e+00  7.68213017e+00\n",
      "   1.62720282e+00 -1.38788762e+00  4.76427968e+00  4.79213751e-01\n",
      "  -1.12722066e+00 -7.73312264e-01  1.65464326e+00  2.39370796e-01]\n",
      " [ 6.99499931e-01  4.87201467e-01  3.34511785e+00 -3.39769432e-01\n",
      "  -3.67356671e-01  1.62148276e+00  5.90672572e-01  5.36462638e-01\n",
      "   3.77090775e-01  5.03957235e-01  2.05787167e+00  2.48895347e+00\n",
      "   2.23788837e-02 -2.74153451e+00  2.35903904e+00  2.25034678e-01]\n",
      " [ 5.76951901e-02 -7.27094109e-03 -3.79779570e+00 -5.86628857e-01\n",
      "   5.16380622e-01 -1.02552604e-01 -2.37084973e-01 -6.19351157e-01\n",
      "   7.53593254e-01 -6.98783421e-01 -1.42155871e+00 -5.25209675e+00\n",
      "  -6.62914420e-01  3.16277000e+00 -2.31967902e+00  2.33587063e-01]\n",
      " [-1.49571737e+00 -9.63131704e-01 -8.21421964e+00  8.19567054e-01\n",
      "  -6.70949324e-01 -2.37780702e+00  3.05637645e-01 -1.13968030e-01\n",
      "  -2.50830213e-02  7.24382601e-01 -4.25154636e+00 -6.53505248e+00\n",
      "   2.96370973e-01  2.68683332e+00 -3.66375900e+00 -1.03232440e+00]\n",
      " [-3.68844551e-01  5.32844663e-02 -4.30919503e-01 -1.61282222e+00\n",
      "   2.40757692e-01  3.36386889e-01 -5.69104574e-01 -8.35899007e-01\n",
      "   8.19335173e-01 -6.50710818e-01 -1.54267818e+00 -1.82824731e+00\n",
      "  -1.05528114e+00  2.22739122e+00 -1.80891465e+00 -8.45819530e-01]\n",
      " [ 9.28768038e-03  7.80194085e-01 -1.32381455e+00 -1.19978582e+00\n",
      "   5.74746437e-01 -5.74799598e-01 -8.53559041e-01 -1.19777772e+00\n",
      "   1.21892032e+00 -4.99202955e-01 -9.18168294e-01 -2.44057503e+00\n",
      "  -5.55477479e-01  3.42284296e+00 -1.61469237e+00  6.44979254e-01]\n",
      " [-4.28757451e-01  7.84436425e-03 -5.32571404e-01  2.19828025e+00\n",
      "  -1.40818494e+00 -2.24526398e-02  1.31139303e+00 -7.85831608e-01\n",
      "  -2.80078403e+00  1.41398795e+00  3.27336555e-01  5.11069831e-01\n",
      "   1.88943101e+00  3.15982514e-01  3.69188721e-01 -2.09548781e-01]\n",
      " [ 9.38127433e-01  1.02475052e+00 -1.87869933e+00  1.52483443e-01\n",
      "  -1.06320692e-01 -4.00705804e+00  3.94431733e-02 -5.21190650e+00\n",
      "  -8.08484259e-01  1.83479376e-01 -1.38545509e+00 -2.50743365e+00\n",
      "   5.64780848e-01  2.34848569e+00 -1.75960463e+00  7.62637394e-01]\n",
      " [-3.83375119e-02  4.85057079e-01  1.57309299e+00 -7.79392353e-01\n",
      "   8.81097296e-01  3.03018220e+00 -4.66686136e-01  3.64234195e+00\n",
      "   1.19283714e+00 -7.13112449e-01  1.74593237e+00  6.62708436e-01\n",
      "  -9.21856984e-01 -7.43415348e-01  6.44927165e-01 -1.01174458e-01]\n",
      " [-1.47686959e+00 -1.00701161e-01  6.42322949e-02  1.44243970e+00\n",
      "   6.59748190e-02  2.07372886e+00 -5.23255862e-01  3.27396246e+00\n",
      "  -2.36586324e-01 -1.17592058e+00  2.17497263e+00  2.87944437e+00\n",
      "  -1.38564572e-01 -1.71794579e+00  1.48711380e+00 -3.00142231e-01]\n",
      " [-5.27278234e-01 -1.12146838e-01 -2.31133366e+00  3.30699374e-01\n",
      "  -5.55701018e-01 -1.23820150e+00  4.21456235e-01 -5.13230217e-01\n",
      "  -7.49785537e-01  4.78701795e-01 -1.54327935e+00 -1.72845285e+00\n",
      "   3.82553853e-01  1.87235560e+00 -1.73818601e+00 -2.23327639e-01]\n",
      " [-7.19561737e-01 -3.96550760e-01 -7.14915137e-01 -8.27424912e-01\n",
      "  -1.50964865e-02 -3.86396932e-01 -1.76350556e-01 -2.43521779e+00\n",
      "   9.36786375e-01 -6.26111107e-01 -2.10508609e+00 -2.12177214e+00\n",
      "  -2.01283902e-01  1.41016426e+00 -1.49902626e+00 -1.01161618e+00]\n",
      " [-1.60061539e+00 -1.57541565e+00  1.00048004e+00 -4.47829414e-01\n",
      "  -3.18438843e-01  1.63645215e+00 -2.83274978e-01  2.72468373e+00\n",
      "   1.07847791e+00 -2.06327127e-01  2.30687401e+00  3.04823977e+00\n",
      "  -3.81086618e-01 -1.22260573e+00  1.89370278e+00 -1.38173116e+00]\n",
      " [-8.37750500e-01 -1.24240273e+00 -1.89728083e+00  4.42314298e-01\n",
      "  -9.20820093e-02 -2.28568185e+00  7.26775352e-01 -1.72635111e+00\n",
      "  -4.50532138e-02  2.50481021e-01 -3.52606670e+00 -1.59012000e+00\n",
      "   4.39848315e-01  9.20220399e-01 -4.15748734e-01 -5.79599477e-01]\n",
      " [-5.40325164e-01 -5.86641517e-01  1.77610564e-01  1.51937387e+00\n",
      "  -1.27085309e-01 -1.47050412e-01  5.58106636e-01 -3.45386374e-01\n",
      "  -1.11036062e+00  7.19506662e-01  4.73548424e-01  3.18506884e-01\n",
      "   1.23385597e+00 -1.46971417e-01  4.45323943e-01 -7.20960261e-01]]\n",
      "Layer 3 bias:\n",
      "[ -5.98499225 -12.41943972   7.44188108  -5.68387592 -27.97464099\n",
      "   4.04226842   3.54129874   5.64238136   9.0295359   -0.53272123\n",
      "  -6.58938544  -0.7475244    3.84184328  -7.27290641  -0.22193374\n",
      "   1.52968662]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# === Funções auxiliares ===\n",
    "def glorot_uniform(fan_in, fan_out):\n",
    "    limit = np.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return np.random.uniform(-limit, limit, size=(fan_out, fan_in))\n",
    "\n",
    "def init_vector_glorot(fan_out):\n",
    "    return np.zeros(fan_out)\n",
    "\n",
    "# Usada apenas para classificação multiclasse\n",
    "# def softmax(z):\n",
    "#     z = z - np.max(z)\n",
    "#     e = np.exp(z)\n",
    "#     return e / np.sum(e)\n",
    "\n",
    "# Função de perda para regressão\n",
    "def mse(y_true, y_pred):\n",
    "    # both shaped (K,) where K=1 for single target\n",
    "    return 0.5 * np.mean((y_pred - y_true)**2)\n",
    "\n",
    "def one_hot(y_i, K):\n",
    "    v = np.zeros(K, dtype=float)\n",
    "    v[y_i] = 1.0\n",
    "    return v\n",
    "\n",
    "def cross_entropy(y_true_one_hot, p):\n",
    "    return -np.sum(y_true_one_hot * np.log(p + 1e-12))\n",
    "\n",
    "tanh = np.tanh\n",
    "tanhp = lambda a: (1.0 - a**2)\n",
    "\n",
    "# === Classe da camada ===\n",
    "class HiddenLayer:\n",
    "    def __init__(self, fan_in, fan_out):\n",
    "        self.W = glorot_uniform(fan_in, fan_out)\n",
    "        self.b = init_vector_glorot(fan_out)\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "        self.prev_a = None\n",
    "        self.tanh = np.tanh\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.prev_a = x\n",
    "        self.z = self.W @ x + self.b\n",
    "        self.a = self.tanh(self.z)\n",
    "        return self.a\n",
    "\n",
    "# === Forward total ===\n",
    "def forward_probs(x, layers, W2, b2):\n",
    "    a = x\n",
    "    for layer in layers:\n",
    "        a = layer.forward(a)\n",
    "    z2 = W2 @ a + b2\n",
    "    # Para problemas de classificação, usar softmax na saída\n",
    "    # p = softmax(z2\n",
    "    p = z2\n",
    "    return p, a\n",
    "\n",
    "\n",
    "# === Configuração da rede ===\n",
    "K = 1                 \n",
    "input_dim = X_train.shape[1]\n",
    "H = 16\n",
    "NLayers = 3\n",
    "\n",
    "HiddenLayers = []\n",
    "HiddenLayers.append(HiddenLayer(input_dim, H))\n",
    "for _ in range(NLayers - 1):\n",
    "    HiddenLayers.append(HiddenLayer(H, H))\n",
    "\n",
    "W2 = glorot_uniform(H, K)\n",
    "b2 = np.zeros(K)\n",
    "\n",
    "eta = 0.01     \n",
    "epochs = 100\n",
    "clip_value = 5.0\n",
    "\n",
    "N = len(X_train)\n",
    "indices = np.arange(N)\n",
    "\n",
    "# === Histórico para gráficos ===\n",
    "\n",
    "# Apenas para classificação multiclasse\n",
    "# train_acc_history = []\n",
    "\n",
    "train_loss_history = []\n",
    "\n",
    "# === Loop de treino ===\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for idx in indices:\n",
    "        \n",
    "        # pegar amostra\n",
    "        x_i = X_train[idx]\n",
    "        y_i = np.asarray(y_train[idx], dtype=float).reshape(-1) \n",
    "\n",
    "        # forward classficação multiclasses\n",
    "        # p, a_last = forward_probs(x_i, HiddenLayers, W2, b2)\n",
    "        # y_one = one_hot(y_i, K)\n",
    "        # total_loss += cross_entropy(y_one, p)\n",
    "        \n",
    "        #  forward regressão\n",
    "        y_hat, a_last = forward_probs(x_i, HiddenLayers, W2, b2)\n",
    "        Tdim = y_i.size  # = 1\n",
    "        delta_out = (y_hat - y_i) / Tdim  \n",
    "        \n",
    "        \n",
    "        # grads output\n",
    "        dW2 = np.outer(delta_out, a_last)\n",
    "        db2 = delta_out\n",
    "        W2_old = W2.copy()\n",
    "\n",
    "        next_delta = delta_out\n",
    "        next_W = W2_old\n",
    "\n",
    "        # backward hidden\n",
    "        for layer in reversed(HiddenLayers):\n",
    "            g = next_W.T @ next_delta\n",
    "            delta = g * tanhp(layer.a)\n",
    "            dW = np.outer(delta, layer.prev_a)\n",
    "            db = delta\n",
    "\n",
    "            W_old = layer.W.copy()\n",
    "\n",
    "            # gradient clipping\n",
    "            if np.linalg.norm(dW) > clip_value:\n",
    "                dW *= clip_value / (np.linalg.norm(dW) + 1e-12)\n",
    "            if np.linalg.norm(db) > clip_value:\n",
    "                db *= clip_value / (np.linalg.norm(db) + 1e-12)\n",
    "\n",
    "            layer.W -= eta * dW\n",
    "            layer.b -= eta * db\n",
    "\n",
    "            next_delta = delta\n",
    "            next_W = W_old\n",
    "\n",
    "        # update output layer\n",
    "        if np.linalg.norm(dW2) > clip_value:\n",
    "            dW2 *= clip_value / (np.linalg.norm(dW2) + 1e-12)\n",
    "        if np.linalg.norm(db2) > clip_value:\n",
    "            db2 *= clip_value / (np.linalg.norm(db2) + 1e-12)\n",
    "\n",
    "        W2 -= eta * dW2\n",
    "        b2 -= eta * db2\n",
    "\n",
    "    # === Avaliação após cada época ===\n",
    "\n",
    "    # == Para modelos de classificação multiclasses\n",
    "    \n",
    "    # probs = np.array([forward_probs(x_i, HiddenLayers, W2, b2)[0] for x_i in X_train])\n",
    "    # y_pred = np.argmax(probs, axis=1)\n",
    "    \n",
    "    # acc = accuracy_score(y_train, y_pred)\n",
    "    # loss = np.mean([-np.log(probs[i, y_train[i]] + 1e-12) for i in range(len(y_train))])\n",
    "    \n",
    "    # train_acc_history.append(acc)\n",
    "    # train_loss_history.append(loss)\n",
    "    \n",
    "    # print(f\"Epoch {epoch:03d} | train acc: {acc:.4f} | train loss: {loss:.4f}\")\n",
    "    \n",
    "    # == para modelos de regressão\n",
    "    \n",
    "    preds = np.array([forward_probs(x_i, HiddenLayers, W2, b2)[0][0] for x_i in X_train])\n",
    "    y_true = y_train.astype(float)\n",
    "    \n",
    "    rmse = np.sqrt(np.mean((preds - y_true)**2))\n",
    "    mae  = np.mean(np.abs(preds - y_true))\n",
    "    \n",
    "    # R**2 (against zero variance)\n",
    "    var = np.var(y_true)\n",
    "    r2 = 1.0 - np.mean((preds - y_true)**2) / (var + 1e-12)\n",
    "\n",
    "    train_loss_history.append(rmse)\n",
    "    print(f\"Epoch {epoch:03d} | RMSE: {rmse:.4f} | MAE: {mae:.4f} | R2: {r2:.4f}\")\n",
    "\n",
    "\n",
    "print(\"Final W2:\", W2, \"Final b2:\", b2)\n",
    "for i, layer in enumerate(HiddenLayers, 1):\n",
    "    print(f\"Layer {i} weights:\\n{layer.W}\\nLayer {i} bias:\\n{layer.b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab577c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE: 152.5109930811942\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preds = np.array([forward_probs(x_i, HiddenLayers, W2, b2)[0][0] for x_i in X_train])\n",
    "y_true = y_train.astype(float)\n",
    "\n",
    "rmse = np.sqrt(np.mean((preds - y_true)**2))\n",
    "print(\"Training RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9781557e",
   "metadata": {},
   "source": [
    "## Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e56a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# probs_test = np.array([forward_probs(x_i, HiddenLayers, W2, b2)[0] for x_i in X_test])\n",
    "# y_pred_test = np.argmax(probs_test, axis=1)\n",
    "\n",
    "# print(\"=== Test Set Metrics ===\")\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "# print(\"Precision:\", precision_score(y_test, y_pred_test, average='weighted'))\n",
    "# print(\"Recall:\", recall_score(y_test, y_pred_test, average='weighted'))\n",
    "# print(\"F1-score:\", f1_score(y_test, y_pred_test, average='weighted'))\n",
    "# print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# cm = confusion_matrix(y_test, y_pred_test)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "# disp.plot(cmap='Blues')\n",
    "# plt.title('Confusion Matrix - Test Set')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785055e5",
   "metadata": {},
   "source": [
    "## Regression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e86b5a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Regression Metrics ===\n",
      "Samples (n): 300 | Features (p): 4\n",
      "MAE   (Mean Absolute Error):       56.270572\n",
      "MSE   (Mean Squared Error):        26091.874329\n",
      "RMSE  (Root Mean Squared Error):   161.529794\n",
      "MAPE  (Mean Abs % Error):          52.805427%   (handled zeros in y_true: 0)\n",
      "R²    (Coeff. of Determination):   0.736460\n",
      "Adj R² (Adjusted R-squared):       0.732886\n",
      "MedAE (Median Absolute Error):     20.474108\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    median_absolute_error,\n",
    ")\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def regression_report(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    *,\n",
    "    n_features: int,\n",
    "    sample_weight: np.ndarray | None = None,\n",
    "    mape_epsilon: float = 1e-8,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute and print a set of regression metrics:\n",
    "      - MAE, MSE, RMSE\n",
    "      - MAPE (safe for zeros in y_true via epsilon)\n",
    "      - R^2 (coefficient of determination)\n",
    "      - Adjusted R^2\n",
    "      - Median Absolute Error (MedAE)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like, shape (n_samples,)\n",
    "    y_pred : array-like, shape (n_samples,)\n",
    "    n_features : int\n",
    "        Number of predictors used by the model (for Adjusted R^2).\n",
    "    sample_weight : array-like or None\n",
    "        Optional sample weights for all metrics that support it.\n",
    "    mape_epsilon : float\n",
    "        Small constant to avoid division by zero in MAPE denominator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    metrics : dict\n",
    "        Dictionary with all computed metrics.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(f\"Shape mismatch: y_true {y_true.shape} vs y_pred {y_pred.shape}\")\n",
    "\n",
    "    n = y_true.size\n",
    "    p = int(n_features)\n",
    "\n",
    "    # Core metrics\n",
    "    mae = mean_absolute_error(y_true, y_pred, sample_weight=sample_weight)\n",
    "    mse = mean_squared_error(y_true, y_pred, sample_weight=sample_weight)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    # Safe MAPE (handles zeros by clamping denominator with epsilon)\n",
    "    denom = np.maximum(np.abs(y_true), mape_epsilon)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / denom)) * 100.0\n",
    "\n",
    "    # R² and Adjusted R²\n",
    "    r2 = r2_score(y_true, y_pred, sample_weight=sample_weight)\n",
    "\n",
    "    # Guard for small n relative to p\n",
    "    if n > p + 1:\n",
    "        adj_r2 = 1.0 - (1.0 - r2) * (n - 1) / (n - p - 1)\n",
    "    else:\n",
    "        adj_r2 = np.nan  # Not defined if n <= p + 1\n",
    "\n",
    "    # Median absolute error\n",
    "    medae = median_absolute_error(y_true, y_pred)\n",
    "\n",
    "    # Print nicely\n",
    "    print(\"\\n=== Regression Metrics ===\")\n",
    "    print(f\"Samples (n): {n} | Features (p): {p}\")\n",
    "    print(f\"MAE   (Mean Absolute Error):       {mae:.6f}\")\n",
    "    print(f\"MSE   (Mean Squared Error):        {mse:.6f}\")\n",
    "    print(f\"RMSE  (Root Mean Squared Error):   {rmse:.6f}\")\n",
    "    zero_count = int(np.sum(np.isclose(y_true, 0.0)))\n",
    "    print(f\"MAPE  (Mean Abs % Error):          {mape:.6f}%   \"\n",
    "          f\"(handled zeros in y_true: {zero_count})\")\n",
    "    print(f\"R²    (Coeff. of Determination):   {r2:.6f}\")\n",
    "    print(f\"Adj R² (Adjusted R-squared):       {adj_r2:.6f}\")\n",
    "    print(f\"MedAE (Median Absolute Error):     {medae:.6f}\")\n",
    "\n",
    "    # return {\n",
    "    #     \"n\": n,\n",
    "    #     \"p\": p,\n",
    "    #     \"MAE\": mae,\n",
    "    #     \"MSE\": mse,\n",
    "    #     \"RMSE\": rmse,\n",
    "    #     \"MAPE_percent\": mape,\n",
    "    #     \"R2\": r2,\n",
    "    #     \"Adj_R2\": adj_r2,\n",
    "    #     \"MedAE\": medae,\n",
    "    #     \"zeros_in_y_true\": zero_count,\n",
    "    # }\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "y_pred = np.array([forward_probs(x_i, HiddenLayers, W2, b2)[0][0] for x_i in X_test])\n",
    "y_true = y_test.astype(float)\n",
    "regression_report(y_true, y_pred, n_features=X.shape[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
