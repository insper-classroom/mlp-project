{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "622fc0db",
   "metadata": {},
   "source": [
    "## Exercise 1: Manual Calculation of MLP Steps\n",
    "\n",
    "Consider a simple MLP with 2 input features, 1 hidden layer containing 2 neurons, and 1 output neuron. Use the hyperbolic tangent (tanh) function as the activation for both the hidden layer and the output layer. The loss function is mean squared error (MSE): $ L = \\frac{1}{N} (y - \\hat{y})^2 $, where $ \\hat{y} $ is the network's output.\n",
    "\n",
    "For this exercise, use the following specific values:\n",
    "\n",
    "- Input and output vectors:\n",
    "\n",
    "    $ \\mathbf{x} = [0.5, -0.2] $\n",
    "\n",
    "    $ y = 1.0 $\n",
    "\n",
    "- Hidden layer weights:\n",
    "\n",
    "    $ \\mathbf{W}^{(1)} = \\begin{bmatrix} 0.3 & -0.1 \\\\ 0.2 & 0.4 \\end{bmatrix} $\n",
    "\n",
    "- Hidden layer biases:\n",
    "\n",
    "    $ \\mathbf{b}^{(1)} = [0.1, -0.2] $\n",
    "\n",
    "- Output layer weights:\n",
    "\n",
    "    $ \\mathbf{W}^{(2)} = [0.5, -0.3] $\n",
    "\n",
    "- Output layer bias:\n",
    "\n",
    "    $ b^{(2)} = 0.2 $\n",
    "\n",
    "- Learning rate: $ \\eta = 0.3 $\n",
    "\n",
    "- Activation function: $ \\tanh $\n",
    "\n",
    "\n",
    "\n",
    "Values were defined as follows :\n",
    "\n",
    "```py\n",
    "\n",
    "x  = np.array([0.5, -0.2])                 # input\n",
    "y  = 1.0                                   # target\n",
    "\n",
    "\n",
    "W1 = np.array([[0.3, -0.1],                # hidden layer weights (2x2)\n",
    "               [0.2,  0.4]])\n",
    "\n",
    "\n",
    "b1 = np.array([0.1, -0.2])                 # hidden biases\n",
    "\n",
    "W2 = np.array([0.5, -0.3])                 # output layer weights\n",
    "b2 = 0.2                                   # output bias\n",
    "\n",
    "eta = 0.1                                  # learning rate, used for the update step\n",
    "tanh = np.tanh\n",
    "tanhp = lambda z: 1.0 - np.tanh(z)**2      # derivative of tanh\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Perform the following steps explicitly, showing all mathematical derivations and calculations with the provided values:\n",
    "\n",
    "1. **Forward Pass**:\n",
    "\n",
    "    - Compute the hidden layer pre-activations: $ \\mathbf{z}^{(1)} = \\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)} $.\n",
    "    - Apply tanh to get hidden activations: $ \\mathbf{a}^{(1)} = \\tanh(\\mathbf{z}^{(1)}) $.\n",
    "    - Compute the output pre-activation: $ z^{(2)} = \\mathbf{W}^{(2)} \\mathbf{a}^{(1)} + b^{(2)} $.\n",
    "    - Compute the final output: $ \\hat{y} = \\tanh(z^{(2)}) $.\n",
    "\n",
    "```py\n",
    "\n",
    "# ----- 1) Forward pass -----\n",
    "z1 = W1 @ x + b1            # hidden-layer pre-activations\n",
    "a1 = tanh(z1)               # hidden activations (2,)\n",
    "z2 = W2 @ a1 + b2           # pre-activation output \n",
    "y_hat = tanh(z2)            # final output\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "2. **Loss Calculation**:\n",
    "\n",
    "    - Compute the MSE loss:\n",
    "\n",
    "        $ L = \\frac{1}{N} (y - \\hat{y})^2 $.\n",
    "\n",
    "```py\n",
    "\n",
    "# ----- 2) Loss (MSE, N=1) -----\n",
    "L = (y - y_hat)**2\n",
    "\n",
    "```\n",
    "\n",
    "3. **Backward Pass (Backpropagation)**: Compute the gradients of the loss with respect to all weights and biases. Start with $ \\frac{\\partial L}{\\partial \\hat{y}} $, then compute:\n",
    "\n",
    "    - $ \\frac{\\partial L}{\\partial z^{(2)}} $ (using the tanh derivative: $ \\frac{d}{dz} \\tanh(z) = 1 - \\tanh^2(z) $).\n",
    "    - Gradients for output layer: $ \\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}} $, $ \\frac{\\partial L}{\\partial b^{(2)}} $.\n",
    "    - Propagate to hidden layer: $ \\frac{\\partial L}{\\partial \\mathbf{a}^{(1)}} $, $ \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} $.\n",
    "    - Gradients for hidden layer: $ \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} $, $ \\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}} $.\n",
    "    \n",
    "    Show all intermediate steps and calculations.\n",
    "\n",
    "```py\n",
    "\n",
    "# ----- 3) Backward pass -----\n",
    "# dL/dy_hat\n",
    "dL_dyhat = 2.0 * (y_hat - y)              # since N=1\n",
    "# dL/dz2\n",
    "dL_dz2 = dL_dyhat * tanhp(z2)\n",
    "\n",
    "# Output layer grads\n",
    "# dL/dW2 = dL/dz2 * a1\n",
    "dL_dW2 = dL_dz2 * a1\n",
    "# dL/db2 = dL/dz2\n",
    "dL_db2 = dL_dz2\n",
    "\n",
    "# Backprop to hidden\n",
    "# dL/da1 = dL/dz2 * W2\n",
    "dL_da1 = dL_dz2 * W2\n",
    "# dL/dz1 = dL/da1 ⊙ tanh'(z1)\n",
    "dL_dz1 = dL_da1 * tanhp(z1)\n",
    "\n",
    "# Hidden layer grads\n",
    "# dL/dW1 = (dL/dz1)[:, None] @ x[None, :]\n",
    "dL_dW1 = np.outer(dL_dz1, x)\n",
    "# dL/db1 = dL/dz1\n",
    "dL_db1 = dL_dz1\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "4. **Parameter Update**: Using the learning rate $ \\eta = 0.1 $, update all weights and biases via gradient descent:\n",
    "\n",
    "    - $ \\mathbf{W}^{(2)} \\leftarrow \\mathbf{W}^{(2)} - \\eta \\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}} $\n",
    "    - $ b^{(2)} \\leftarrow b^{(2)} - \\eta \\frac{\\partial L}{\\partial b^{(2)}} $\n",
    "    - $ \\mathbf{W}^{(1)} \\leftarrow \\mathbf{W}^{(1)} - \\eta \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} $\n",
    "    - $ \\mathbf{b}^{(1)} \\leftarrow \\mathbf{b}^{(1)} - \\eta \\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}} $\n",
    "\n",
    "    Provide the numerical values for all updated parameters.\n",
    "\n",
    "\n",
    "```py\n",
    "# ----- 4) Parameter update (gradient descent, eta = 0.1) -----\n",
    "\n",
    "W2_new = W2 - eta * dL_dW2\n",
    "b2_new = b2 - eta * dL_db2\n",
    "W1_new = W1 - eta * dL_dW1\n",
    "b1_new = b1 - eta * dL_db1\n",
    "\n",
    "```\n",
    "\n",
    "**Submission Requirements**: Show all mathematical steps explicitly, including intermediate calculations (e.g., matrix multiplications, tanh applications, gradient derivations). Use exact numerical values throughout and avoid rounding excessively to maintain precision (at least 4 decimal places).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95334afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z^(1) = [ 0.27 -0.18]\n",
      "a^(1) = [ 0.26362484 -0.17808087]\n",
      "z^(2) = 0.38523667817130075\n",
      "y_hat = 0.36724656264510797\n",
      "Loss L = 0.4003769124844312\n",
      "\n",
      "dL/dy_hat = -1.265506874709784\n",
      "dL/dz^(2) = -1.0948279147135995\n",
      "dL/dW^(2) = [-0.28862383  0.19496791]\n",
      "dL/db^(2) = -1.0948279147135995\n",
      "dL/da^(1) = [-0.54741396  0.32844837]\n",
      "dL/dz^(1) = [-0.50936975  0.31803236]\n",
      "dL/dW^(1) =\n",
      " [[-0.25468488  0.10187395]\n",
      " [ 0.15901618 -0.06360647]]\n",
      "dL/db^(1) = [-0.50936975  0.31803236]\n",
      "\n",
      "Updated parameters:\n",
      "W^(2)_new = [ 0.52886238 -0.31949679]\n",
      "b^(2)_new = 0.30948279147136\n",
      "W^(1)_new =\n",
      " [[ 0.32546849 -0.1101874 ]\n",
      " [ 0.18409838  0.40636065]]\n",
      "b^(1)_new = [ 0.15093698 -0.23180324]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=8, suppress=False)\n",
    "\n",
    "# ----- Given values -----\n",
    "x  = np.array([0.5, -0.2])                # input\n",
    "y  = 1.0                                   # target\n",
    "\n",
    "W1 = np.array([[0.3, -0.1],                # hidden layer weights (2x2)\n",
    "               [0.2,  0.4]])\n",
    "b1 = np.array([0.1, -0.2])                 # hidden biases (2,)\n",
    "\n",
    "W2 = np.array([0.5, -0.3])                 # output layer weights (2,)\n",
    "b2 = 0.2                                   # output bias (scalar)\n",
    "\n",
    "eta = 0.1                                   # learning rate for the update step\n",
    "tanh = np.tanh\n",
    "tanhp = lambda z: 1.0 - np.tanh(z)**2       # derivative of tanh\n",
    "\n",
    "# ----- 1) Forward pass -----\n",
    "z1 = W1 @ x + b1            # pre-activations hidden (2,)\n",
    "a1 = tanh(z1)               # activations hidden (2,)\n",
    "z2 = W2 @ a1 + b2           # pre-activation output (scalar)\n",
    "y_hat = tanh(z2)            # final output (scalar)\n",
    "\n",
    "print(\"z^(1) =\", z1)\n",
    "print(\"a^(1) =\", a1)\n",
    "print(\"z^(2) =\", z2)\n",
    "print(\"y_hat =\", y_hat)\n",
    "\n",
    "\n",
    "# ----- 2) Loss (MSE, N=1) -----\n",
    "L = (y - y_hat)**2\n",
    "print(\"Loss L =\", L)\n",
    "\n",
    "\n",
    "# ----- 3) Backward pass -----\n",
    "# dL/dy_hat\n",
    "dL_dyhat = 2.0 * (y_hat - y)              # since N=1\n",
    "# dL/dz2\n",
    "dL_dz2 = dL_dyhat * tanhp(z2)\n",
    "\n",
    "# Output layer grads\n",
    "# dL/dW2 = dL/dz2 * a1\n",
    "dL_dW2 = dL_dz2 * a1\n",
    "# dL/db2 = dL/dz2\n",
    "dL_db2 = dL_dz2\n",
    "\n",
    "# Backprop to hidden\n",
    "# dL/da1 = dL/dz2 * W2\n",
    "dL_da1 = dL_dz2 * W2\n",
    "# dL/dz1 = dL/da1 ⊙ tanh'(z1)\n",
    "dL_dz1 = dL_da1 * tanhp(z1)\n",
    "\n",
    "# Hidden layer grads\n",
    "# dL/dW1 = (dL/dz1)[:, None] @ x[None, :]\n",
    "dL_dW1 = np.outer(dL_dz1, x)\n",
    "# dL/db1 = dL/dz1\n",
    "dL_db1 = dL_dz1\n",
    "\n",
    "print(\"\\ndL/dy_hat =\", dL_dyhat)\n",
    "print(\"dL/dz^(2) =\", dL_dz2)\n",
    "print(\"dL/dW^(2) =\", dL_dW2)\n",
    "print(\"dL/db^(2) =\", dL_db2)\n",
    "print(\"dL/da^(1) =\", dL_da1)\n",
    "print(\"dL/dz^(1) =\", dL_dz1)\n",
    "print(\"dL/dW^(1) =\\n\", dL_dW1)\n",
    "print(\"dL/db^(1) =\", dL_db1)\n",
    "\n",
    "\n",
    "# ----- 4) Parameter update (gradient descent, eta = 0.1) -----\n",
    "W2_new = W2 - eta * dL_dW2\n",
    "b2_new = b2 - eta * dL_db2\n",
    "W1_new = W1 - eta * dL_dW1\n",
    "b1_new = b1 - eta * dL_db1\n",
    "\n",
    "print(\"\\nUpdated parameters:\")\n",
    "print(\"W^(2)_new =\", W2_new)\n",
    "print(\"b^(2)_new =\", b2_new)\n",
    "print(\"W^(1)_new =\\n\", W1_new)\n",
    "print(\"b^(1)_new =\", b1_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5dbca8",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Exercise 2: Binary Classification with Synthetic Data and Scratch MLP\n",
    "\n",
    "Using the `make_classification` function from scikit-learn ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)), generate a synthetic dataset with the following specifications:\n",
    "\n",
    "- Number of samples: 1000\n",
    "- Number of classes: 2\n",
    "- Number of clusters per class: Use the `n_clusters_per_class` parameter creatively to achieve 1 cluster for one class and 2 for the other (hint: you may need to generate subsets separately and combine them, as the function applies the same number of clusters to all classes by default).\n",
    "- Other parameters: Set `n_features=2` for easy visualization, `n_informative=2`, `n_redundant=0`, `random_state=42` for reproducibility, and adjust `class_sep` or `flip_y` as needed for a challenging but separable dataset.\n",
    "\n",
    "Implement an MLP from scratch (without using libraries like TensorFlow or PyTorch for the model itself; you may use NumPy for array operations) to classify this data. You have full freedom to choose the architecture, including:\n",
    "\n",
    "- Number of hidden layers (at least 1)\n",
    "- Number of neurons per layer\n",
    "- Activation functions (e.g., sigmoid, ReLU, tanh)\n",
    "- Loss function (e.g., binary cross-entropy)\n",
    "- Optimizer (e.g., gradient descent, with a chosen learning rate)\n",
    "\n",
    "Steps to follow:\n",
    "\n",
    "1. Generate and split the data into training (80%) and testing (20%) sets.\n",
    "2. Implement the forward pass, loss computation, backward pass, and parameter updates in code.\n",
    "3. Train the model for a reasonable number of epochs (e.g., 100-500), tracking training loss.\n",
    "4. Evaluate on the test set: Report accuracy, and optionally plot decision boundaries or confusion matrix.\n",
    "5. Submit your code and results, including any visualizations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bffe45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1000, 2) y shape: (1000,)\n",
      "Class counts: [500 500]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Reproducibility\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "def biased_weights_for(target_class, K=2, high=0.8):\n",
    "    \"\"\"\n",
    "    Return a weight vector of length K that heavily favors target_class.\n",
    "    Helps ensure enough samples of the desired class per call.\n",
    "    \"\"\"\n",
    "    low = (1.0 - high) / (K - 1)\n",
    "    w = np.full(K, low, dtype=float)\n",
    "    w[target_class] = high\n",
    "    return w\n",
    "\n",
    "def sample_class_subset(\n",
    "    n_needed: int,\n",
    "    target_class: int,\n",
    "    n_clusters_per_class: int,\n",
    "    seed: int,\n",
    "    *,\n",
    "    n_features: int = 2,\n",
    "    n_informative: int = 2,\n",
    "    n_redundant: int = 0,\n",
    "    class_sep: float = 1.5,\n",
    "    flip_y: float = 0.0,\n",
    "    max_tries: int = 20,\n",
    "    K: int = 2\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate samples with make_classification and keep only rows of 'target_class'.\n",
    "    We over-generate with biased 'weights' so we can downsample exactly n_needed.\n",
    "    \"\"\"\n",
    "    tries = 0\n",
    "    local_seed = seed\n",
    "    # Over-generate to boost the chance of hitting n_needed for target_class\n",
    "    n_generate = max(4 * n_needed, 2000)\n",
    "\n",
    "    while tries < max_tries:\n",
    "        X_tmp, y_tmp = make_classification(\n",
    "            n_samples=n_generate,\n",
    "            n_features=n_features,\n",
    "            n_informative=n_informative,\n",
    "            n_redundant=n_redundant,\n",
    "            n_repeated=0,\n",
    "            n_classes=K,\n",
    "            n_clusters_per_class=n_clusters_per_class,\n",
    "            class_sep=class_sep,\n",
    "            flip_y=flip_y,\n",
    "            weights=biased_weights_for(target_class, K=K, high=0.8),\n",
    "            random_state=local_seed,\n",
    "        )\n",
    "\n",
    "        idx = np.flatnonzero(y_tmp == target_class)\n",
    "        if idx.size >= n_needed:\n",
    "            chosen = rng.choice(idx, size=n_needed, replace=False)\n",
    "            return X_tmp[chosen], np.full(n_needed, target_class, dtype=int)\n",
    "\n",
    "        # Try again with a different seed\n",
    "        tries += 1\n",
    "        local_seed += 1\n",
    "\n",
    "    raise RuntimeError(\n",
    "        f\"Could not obtain {n_needed} samples for class={target_class} \"\n",
    "        f\"with n_clusters_per_class={n_clusters_per_class} after {max_tries} tries.\"\n",
    "    )\n",
    "\n",
    "# ---------- Build the asymmetric dataset ----------\n",
    "N = 1000\n",
    "K = 2\n",
    "n_per_class = [N // K] * K\n",
    "n_per_class[0] += N - sum(n_per_class)  # handle remainder if any (keeps total = N)\n",
    "\n",
    "# Assign distinct cluster counts per class\n",
    "clusters_per_class = {\n",
    "    0: 1,  # class 0 -> 1 clusters\n",
    "    1: 2,  # class 1 -> 2 clusters\n",
    "}\n",
    "\n",
    "# Different seeds per class for variety\n",
    "base_seeds = {0: 42, 1: 1337}\n",
    "\n",
    "Xs = []\n",
    "ys = []\n",
    "for c in range(K):\n",
    "    Xi, yi = sample_class_subset(\n",
    "        n_needed=n_per_class[c],\n",
    "        target_class=c,\n",
    "        n_clusters_per_class=clusters_per_class[c],\n",
    "        seed=base_seeds[c],\n",
    "        n_features=2,\n",
    "        n_informative=2,\n",
    "        n_redundant=0,\n",
    "        class_sep=1.6,   # tweak for difficulty vs. separability\n",
    "        flip_y=0.0,\n",
    "        K=K,\n",
    "    )\n",
    "    Xs.append(Xi)\n",
    "    ys.append(yi)\n",
    "\n",
    "# Combine and shuffle\n",
    "X = np.vstack(Xs)\n",
    "y = np.concatenate(ys)\n",
    "perm = rng.permutation(len(y))\n",
    "X = X[perm]\n",
    "y = y[perm]\n",
    "\n",
    "print(\"X shape:\", X.shape, \"y shape:\", y.shape)\n",
    "print(\"Class counts:\", np.bincount(y))\n",
    "# Expect: (1500, 4) and roughly balanced counts (exactly 500 each by construction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a64d3d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final W2: [ 0.63446301 -0.83237545] Final b2: 0.45908843162023505\n",
      "Layer 1 weights:\n",
      "[[-0.3443021   0.88894019]\n",
      " [-0.09855254 -0.86185418]]\n",
      "Layer 1 bias:\n",
      "[-0.6156105   0.16952364]\n",
      "Layer 2 weights:\n",
      "[[ 0.72297534 -0.12167491]\n",
      " [-0.45737613  1.00072866]]\n",
      "Layer 2 bias:\n",
      "[0.72930515 0.43186679]\n",
      "Accuracy: 0.737\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "class HiddenLayer():\n",
    "    \n",
    "    def __init__(self, w_size, b_size):\n",
    "        self.W = np.random.uniform(-1, 1, size=w_size)\n",
    "        self.b = np.random.uniform(-1, 1, size=b_size)\n",
    "        self.z = 0                # pre-activation value\n",
    "        self.a = 0                # activation value \n",
    "        self.prev_a = 0\n",
    "\n",
    "    def set_z(self, z):\n",
    "        self.z = z\n",
    "        \n",
    "    def set_a(self, a):\n",
    "        self.a = a\n",
    "    \n",
    "    def set_W(self, W):\n",
    "        self.W = W    \n",
    "        \n",
    "    def set_b(self, b):\n",
    "        self.b = b\n",
    "        \n",
    "    def set_prev_a(self, value):\n",
    "        self.prev_a = value\n",
    "        \n",
    "        \n",
    "    def update_W(self, eta, dW):\n",
    "        self.W -= eta * dW\n",
    "        \n",
    "    def update_b(self, eta, db):\n",
    "        self.b -= eta * db\n",
    "\n",
    "np.set_printoptions(precision=8, suppress=False)\n",
    "\n",
    "# ----- Given values -----\n",
    "\n",
    "# Mapping for -1 and 1\n",
    "y = 2*y - 1\n",
    "\n",
    "HiddenLayers = []\n",
    "\n",
    "NLayers = 2\n",
    "\n",
    "for _ in range(NLayers):\n",
    "    HiddenLayers.append(HiddenLayer(w_size=(2,2), b_size=(2,)))\n",
    "\n",
    "\n",
    "W2 = np.array([0.5, -0.3])                 # output layer weights (2,)\n",
    "b2 = 0.2                                   # output bias\n",
    "\n",
    "eta = 0.1                                   # learning rate for the update step\n",
    "tanh = np.tanh\n",
    "tanhp = lambda z: 1.0 - np.tanh(z)**2       # derivative of tanh\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    wrong = False\n",
    "    \n",
    "    for index in range(len(X)):\n",
    "        \n",
    "        x_i = X[index]\n",
    "        y_i = y[index]\n",
    "\n",
    "        prev_a = x_i\n",
    "        for layer in HiddenLayers:\n",
    "            layer.set_prev_a(prev_a)\n",
    "            layer.set_z(layer.W @ prev_a + layer.b)            # pre-activations hidden (2,)\n",
    "            a = (tanh(layer.z))                                # activations hidden (2,)\n",
    "            layer.set_a(a)\n",
    "            prev_a = a\n",
    "            \n",
    "            \n",
    "        z2 = W2 @ prev_a + b2           # pre-activation output\n",
    "        y_hat = tanh(z2)            # final output\n",
    "\n",
    "\n",
    "        L = ((y_i - y_hat)**2)\n",
    "\n",
    "        if L > 1e-12:\n",
    "            \n",
    "            wrong = True\n",
    "\n",
    "            delta2 = (2.0 / N) * (y_hat - y_i) * (1.0 - y_hat**2)   # scalar\n",
    "\n",
    "            # Gradients for output layer\n",
    "            dW2 = delta2 * prev_a   \n",
    "            db2 = delta2\n",
    "\n",
    "            next_delta = delta2                      \n",
    "            next_W = W2                              \n",
    "\n",
    "            # Updatiung Hidden Layers\n",
    "            first_back = True\n",
    "            for layer in reversed(HiddenLayers):\n",
    "\n",
    "                if first_back:\n",
    "                    # Output Layer has a Bias (not a matrix)\n",
    "                    g = next_W * next_delta                   \n",
    "                    first_back = False\n",
    "                else:\n",
    "                    g = next_W.T @ next_delta                  \n",
    "\n",
    "                delta = g * (1.0 - layer.a**2)   \n",
    "\n",
    "                dW = np.outer(delta, layer.prev_a)     \n",
    "                db = delta                     \n",
    "\n",
    "                # Update hidden layer\n",
    "                layer.update_W(eta, dW)\n",
    "                layer.update_b(eta, db)\n",
    "\n",
    "                # Prepare for next layer\n",
    "                next_delta = delta\n",
    "                next_W = layer.W    \n",
    "\n",
    "            # Output Layer Update\n",
    "            W2 -= eta * dW2\n",
    "            b2 -= eta * db2\n",
    "                    \n",
    "    if not wrong:            \n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "def forward(x, HiddenLayers, W2, b2):\n",
    "    x_ = x\n",
    "    for layer in HiddenLayers:\n",
    "        z = layer.W @ x_ + layer.b\n",
    "        a = np.tanh(z)\n",
    "        x_ = a\n",
    "    z2 = W2 @ x_ + b2\n",
    "    y_hat = np.tanh(z2)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "\n",
    "y_pred = np.array([forward(x_i, HiddenLayers, W2, b2) for x_i in X])\n",
    "\n",
    "y_pred_labels = np.where(y_pred >= 0.0, 1, -1)\n",
    "\n",
    "print(\"Final W2:\", W2, \"Final b2:\", b2)\n",
    "for i, layer in enumerate(HiddenLayers, 1):\n",
    "    print(f\"Layer {i} weights:\\n{layer.W}\\nLayer {i} bias:\\n{layer.b}\")\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y, y_pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4769f4b6",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "## Exercise 3: Multi-Class Classification with Synthetic Data and Reusable MLP\n",
    "\n",
    "Similar to Exercise 2, but with increased complexity.\n",
    "\n",
    "Use `make_classification` to generate a synthetic dataset with:\n",
    "\n",
    "- Number of samples: 1500\n",
    "- Number of classes: 3\n",
    "- Number of features: 4\n",
    "- Number of clusters per class: Achieve 2 clusters for one class, 3 for another, and 4 for the last (again, you may need to generate subsets separately and combine them, as the function doesn't directly support varying clusters per class).\n",
    "- Other parameters: `n_features=4`, `n_informative=4`, `n_redundant=0`, `random_state=42`.\n",
    "\n",
    "Implement an MLP from scratch to classify this data. You may choose the architecture freely, but for an extra point (bringing this exercise to 4 points), reuse the exact same MLP implementation code from Exercise 2, modifying only hyperparameters (e.g., output layer size for 3 classes, loss function to categorical cross-entropy if needed) without changing the core structure.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Generate and split the data (80/20 train/test).\n",
    "2. Train the model, tracking loss.\n",
    "3. Evaluate on test set: Report accuracy, and optionally visualize (e.g., scatter plot of data with predicted labels).\n",
    "4. Submit code and results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4b8c106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1500, 4) y shape: (1500,)\n",
      "Class counts: [500 500 500]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Reproducibility\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "def biased_weights_for(target_class, K=3, high=0.8):\n",
    "    \"\"\"\n",
    "    Return a weight vector of length K that heavily favors target_class.\n",
    "    Helps ensure enough samples of the desired class per call.\n",
    "    \"\"\"\n",
    "    low = (1.0 - high) / (K - 1)\n",
    "    w = np.full(K, low, dtype=float)\n",
    "    w[target_class] = high\n",
    "    return w\n",
    "\n",
    "def sample_class_subset(\n",
    "    n_needed: int,\n",
    "    target_class: int,\n",
    "    n_clusters_per_class: int,\n",
    "    seed: int,\n",
    "    *,\n",
    "    n_features: int = 4,\n",
    "    n_informative: int = 4,\n",
    "    n_redundant: int = 0,\n",
    "    class_sep: float = 1.5,\n",
    "    flip_y: float = 0.0,\n",
    "    max_tries: int = 20,\n",
    "    K: int = 3\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate samples with make_classification and keep only rows of 'target_class'.\n",
    "    We over-generate with biased 'weights' so we can downsample exactly n_needed.\n",
    "    \"\"\"\n",
    "    tries = 0\n",
    "    local_seed = seed\n",
    "    # Over-generate to boost the chance of hitting n_needed for target_class\n",
    "    n_generate = max(4 * n_needed, 2000)\n",
    "\n",
    "    while tries < max_tries:\n",
    "        X_tmp, y_tmp = make_classification(\n",
    "            n_samples=n_generate,\n",
    "            n_features=n_features,\n",
    "            n_informative=n_informative,\n",
    "            n_redundant=n_redundant,\n",
    "            n_repeated=0,\n",
    "            n_classes=K,\n",
    "            n_clusters_per_class=n_clusters_per_class,\n",
    "            class_sep=class_sep,\n",
    "            flip_y=flip_y,\n",
    "            weights=biased_weights_for(target_class, K=K, high=0.8),\n",
    "            random_state=local_seed,\n",
    "        )\n",
    "\n",
    "        idx = np.flatnonzero(y_tmp == target_class)\n",
    "        if idx.size >= n_needed:\n",
    "            chosen = rng.choice(idx, size=n_needed, replace=False)\n",
    "            return X_tmp[chosen], np.full(n_needed, target_class, dtype=int)\n",
    "\n",
    "        # Try again with a different seed\n",
    "        tries += 1\n",
    "        local_seed += 1\n",
    "\n",
    "    raise RuntimeError(\n",
    "        f\"Could not obtain {n_needed} samples for class={target_class} \"\n",
    "        f\"with n_clusters_per_class={n_clusters_per_class} after {max_tries} tries.\"\n",
    "    )\n",
    "\n",
    "# ---------- Build the asymmetric dataset ----------\n",
    "N = 1500\n",
    "K = 3\n",
    "n_per_class = [N // K] * K\n",
    "n_per_class[0] += N - sum(n_per_class)  # handle remainder if any (keeps total = N)\n",
    "\n",
    "# Assign distinct cluster counts per class\n",
    "clusters_per_class = {\n",
    "    0: 2,  # class 0 -> 2 clusters\n",
    "    1: 3,  # class 1 -> 3 clusters\n",
    "    2: 4,  # class 2 -> 4 clusters\n",
    "}\n",
    "\n",
    "# Different seeds per class for variety\n",
    "base_seeds = {0: 42, 1: 1337, 2: 2027}\n",
    "\n",
    "Xs = []\n",
    "ys = []\n",
    "for c in range(K):\n",
    "    Xi, yi = sample_class_subset(\n",
    "        n_needed=n_per_class[c],\n",
    "        target_class=c,\n",
    "        n_clusters_per_class=clusters_per_class[c],\n",
    "        seed=base_seeds[c],\n",
    "        n_features=4,\n",
    "        n_informative=4,\n",
    "        n_redundant=0,\n",
    "        class_sep=1.6,   # tweak for difficulty vs. separability\n",
    "        flip_y=0.0,\n",
    "        K=K,\n",
    "    )\n",
    "    Xs.append(Xi)\n",
    "    ys.append(yi)\n",
    "\n",
    "# Combine and shuffle\n",
    "X = np.vstack(Xs)\n",
    "y = np.concatenate(ys)\n",
    "perm = rng.permutation(len(y))\n",
    "X = X[perm]\n",
    "y = y[perm]\n",
    "\n",
    "print(\"X shape:\", X.shape, \"y shape:\", y.shape)\n",
    "print(\"Class counts:\", np.bincount(y))\n",
    "# Expect: (1500, 4) and roughly balanced counts (exactly 500 each by construction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c973566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final W2: [[-1.30150104  0.74545721  0.40928816]\n",
      " [ 1.46128138  0.86141196 -0.66732008]\n",
      " [ 1.12006045 -1.26491417  1.06276974]] Final b2: [-1.23020757 -0.18742653  1.4176341 ]\n",
      "Layer 1 weights:\n",
      "[[ 14.87125433   4.96105463  -6.61067946   9.02410079]\n",
      " [ -1.54703905   0.30016101  19.98290756   3.37650402]\n",
      " [  6.28185814 -12.02192067   8.7366937   -3.54556444]]\n",
      "Layer 1 bias:\n",
      "[-0.44119326 -6.82410816 -7.93830413]\n",
      "Layer 2 weights:\n",
      "[[-0.75370642  2.52967955  1.27164864]\n",
      " [-4.13644548 -2.32339937 -1.9046856 ]\n",
      " [-1.02515081 -1.44959544  2.82048605]]\n",
      "Layer 2 bias:\n",
      "[-0.94398869  2.25227241  1.17658551]\n",
      "Accuracy: 0.7086666666666667\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    z = z - np.max(z)\n",
    "    e = np.exp(z)\n",
    "    return e / np.sum(e)\n",
    "\n",
    "def one_hot(y_i, K):\n",
    "    v = np.zeros(K, dtype=float)\n",
    "    v[y_i] = 1.0\n",
    "    return v\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class HiddenLayer():\n",
    "    \n",
    "    def __init__(self, w_size, b_size):\n",
    "        self.W = np.random.uniform(-1, 1, size=w_size)\n",
    "        self.b = np.random.uniform(-1, 1, size=b_size)\n",
    "        self.z = 0                # pre-activation value\n",
    "        self.a = 0                # activation value \n",
    "        self.prev_a = 0\n",
    "\n",
    "    def set_z(self, z):\n",
    "        self.z = z\n",
    "        \n",
    "    def set_a(self, a):\n",
    "        self.a = a\n",
    "    \n",
    "    def set_W(self, W):\n",
    "        self.W = W    \n",
    "        \n",
    "    def set_b(self, b):\n",
    "        self.b = b\n",
    "        \n",
    "    def set_prev_a(self, value):\n",
    "        self.prev_a = value\n",
    "        \n",
    "        \n",
    "    def update_W(self, eta, dW):\n",
    "        self.W -= eta * dW\n",
    "        \n",
    "    def update_b(self, eta, db):\n",
    "        self.b -= eta * db\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def forward_probs(x, HiddenLayers, W2, b2):\n",
    "    prev_a = x\n",
    "    for layer in HiddenLayers:\n",
    "        layer.set_prev_a(prev_a)\n",
    "        layer.set_z(layer.W @ prev_a + layer.b)            # pre-activations hidden (2,)\n",
    "        a = (tanh(layer.z))                                # activations hidden (2,)\n",
    "        layer.set_a(a)\n",
    "        prev_a = a\n",
    "    z2 = W2 @ prev_a + b2          # logits, shape (K,)\n",
    "    p = softmax(z2)           # probs, shape (K,)\n",
    "    return p, prev_a               # also return last hidden activation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "K = len(np.unique(y))                 # number of classes\n",
    "input_dim = X.shape[1]                # = 4\n",
    "H = 3                                 # hidden width (it is up to personal prefeerence)\n",
    "NLayers = 2\n",
    "\n",
    "HiddenLayers = []\n",
    "# First hidden layer: (H, input_dim)\n",
    "HiddenLayers.append(HiddenLayer(w_size=(H, input_dim), b_size=(H,)))\n",
    "\n",
    "# Remaining hidden layers: (H, H)\n",
    "for _ in range(NLayers - 1):\n",
    "    HiddenLayers.append(HiddenLayer(w_size=(H, H), b_size=(H,)))\n",
    "\n",
    "# Output layer: (K, H) and (K,)\n",
    "W2 = np.random.uniform(-1, 1, size=(K, H))\n",
    "b2 = np.zeros(K)                \n",
    "\n",
    "\n",
    "\n",
    "eta = 0.1                                   # learning rate for the update step\n",
    "tanh = np.tanh\n",
    "tanhp = lambda z: 1.0 - np.tanh(z)**2       # derivative of tanh\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    wrong = False\n",
    "    \n",
    "    for index in range(len(X)):\n",
    "        \n",
    "        x_i = X[index]\n",
    "        y_i = y[index]\n",
    "\n",
    "        p, a_last = forward_probs(x_i, HiddenLayers, W2, b2)  # p: (K,), a_last: (H,)\n",
    "        L = -np.log(p[y_i])   \n",
    "\n",
    "        if L > 1e-12:\n",
    "            \n",
    "            wrong = True\n",
    "\n",
    "            # output layer calculations\n",
    "            y_one = one_hot(y_i, K)               # (K,)\n",
    "            delta_out = p - y_one                 # (K,)\n",
    "            dW2 = np.outer(delta_out, a_last)     # (K, H)\n",
    "            db2 = delta_out                       # (K,)\n",
    "\n",
    "            # save for hidden backprop (use current W2, not yet updated)\n",
    "            next_delta = delta_out                # (K,)\n",
    "            next_W = W2                           # (K, H)\n",
    "\n",
    "            # updating hidden layers from reversed order\n",
    "            for layer in reversed(HiddenLayers):\n",
    "                g = next_W.T @ next_delta         # (H_prev,) where H_prev = layer.a.size\n",
    "                delta = g * (1.0 - layer.a**2)    # tanh'(z) = 1 - a^2\n",
    "\n",
    "                dW = np.outer(delta, layer.prev_a)\n",
    "                db = delta\n",
    "\n",
    "                layer.update_W(eta, dW)\n",
    "                layer.update_b(eta, db)\n",
    "\n",
    "                next_delta = delta                # (H_prev,)\n",
    "                next_W = layer.W                  # (H_prev, input_dim_prev)\n",
    "\n",
    "            # Updatye Output Layer\n",
    "            W2 -= eta * dW2\n",
    "            b2 -= eta * db2\n",
    "                    \n",
    "    if not wrong:            \n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "def forward(x, HiddenLayers, W2, b2):\n",
    "    x_ = x\n",
    "    for layer in HiddenLayers:\n",
    "        z = layer.W @ x_ + layer.b\n",
    "        a = np.tanh(z)\n",
    "        x_ = a\n",
    "    z2 = W2 @ x_ + b2\n",
    "    y_hat = np.tanh(z2)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "\n",
    "print(\"Final W2:\", W2, \"Final b2:\", b2)\n",
    "for i, layer in enumerate(HiddenLayers, 1):\n",
    "    print(f\"Layer {i} weights:\\n{layer.W}\\nLayer {i} bias:\\n{layer.b}\")\n",
    "\n",
    "probs = np.array([forward_probs(x_i, HiddenLayers, W2, b2)[0] for x_i in X])  # (N, K)\n",
    "y_pred = np.argmax(probs, axis=1)\n",
    "print(\"Accuracy:\", accuracy_score(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceb3f15",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "\n",
    "## Exercise 4: Multi-Class Classification with Deeper MLP\n",
    "\n",
    "Repeat Exercise 3 exactly, but now ensure your MLP has **at least 2 hidden layers**. You may adjust the number of neurons per layer as needed for better performance. Reuse code from Exercise 3 where possible, but the focus is on demonstrating the deeper architecture. Submit updated code, training results, and test evaluation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5dd27e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final W2: [[-0.84814546  0.53060569 -0.02411378]\n",
      " [-0.84814546  0.53060569 -0.02411379]\n",
      " [-0.84814546  0.53060569 -0.02411378]] Final b2: [-3.47440006e-05 -2.60865489e-01  2.60900233e-01]\n",
      "Layer 1 weights:\n",
      "[[-0.84338989  0.61003512  0.19936643  0.24624894]\n",
      " [-0.3643678  -0.15606804  0.51052957 -0.82817205]\n",
      " [ 0.78685002 -0.21631564  0.33439671  0.32546589]]\n",
      "Layer 1 bias:\n",
      "[0.71062903 0.57992    0.99769994]\n",
      "Layer 2 weights:\n",
      "[[ 0.92091437  0.1590592  -0.92093574]\n",
      " [ 0.26167634 -0.37245274 -0.51969802]\n",
      " [ 0.52611436  0.27042889  0.56091939]]\n",
      "Layer 2 bias:\n",
      "[-0.94674719 -0.50250292 -0.88996456]\n",
      "Layer 3 weights:\n",
      "[[-0.48557048 -0.89524249  0.23999672]\n",
      " [ 0.11486781  0.22516431  0.4767062 ]\n",
      " [ 0.72083311  0.25566272 -0.04362071]]\n",
      "Layer 3 bias:\n",
      "[-0.87372412  0.31243484 -0.1821496 ]\n",
      "Layer 4 weights:\n",
      "[[-0.86679364 -0.74903253 -0.59972829]\n",
      " [-0.41471671 -0.93718506  0.99308781]\n",
      " [-0.90901745 -0.38878322 -0.66119217]]\n",
      "Layer 4 bias:\n",
      "[ 0.84725279 -0.47609917 -0.78251266]\n",
      "Layer 5 weights:\n",
      "[[ 0.9030442   0.58482892 -0.43559953]\n",
      " [-0.41552525 -0.21690784  0.5429586 ]\n",
      " [-0.49544896  0.30894673 -0.67021008]]\n",
      "Layer 5 bias:\n",
      "[-0.65896629 -0.61186907  0.5217721 ]\n",
      "Layer 6 weights:\n",
      "[[-0.55519639 -0.21116559  0.92635114]\n",
      " [-0.8295121   0.74982627 -0.5204944 ]\n",
      " [ 0.58045514 -0.11925096  0.91129031]]\n",
      "Layer 6 bias:\n",
      "[ 0.63632091  0.69050396 -0.34407802]\n",
      "Layer 7 weights:\n",
      "[[ 0.84706671 -0.74466787 -0.93739272]\n",
      " [ 0.06601499 -0.76135376  0.19264094]\n",
      " [ 0.66664853  0.971907   -0.12379642]]\n",
      "Layer 7 bias:\n",
      "[0.6458024  0.0991511  0.56741277]\n",
      "Layer 8 weights:\n",
      "[[-0.49792103 -0.33986334  0.64831763]\n",
      " [-0.67284879 -0.0911487  -0.43292653]\n",
      " [ 0.69657043 -0.92378961 -0.46666754]]\n",
      "Layer 8 bias:\n",
      "[0.96021288 0.52926684 0.43696608]\n",
      "Layer 9 weights:\n",
      "[[-0.02879622 -0.09095586  0.10260813]\n",
      " [-0.18308815  0.88207993 -0.81538387]\n",
      " [ 0.07355429  0.90624164  0.44765395]]\n",
      "Layer 9 bias:\n",
      "[-0.61947583  0.41456654  0.15003394]\n",
      "Layer 10 weights:\n",
      "[[-0.38670175  0.37181863  0.74573708]\n",
      " [-0.58474177  0.17708032 -0.05597755]\n",
      " [-0.1046401   0.18846164 -0.62779381]]\n",
      "Layer 10 bias:\n",
      "[ 0.778389   -0.11774065 -0.58456906]\n",
      "Layer 11 weights:\n",
      "[[ 0.29578964  0.70808643 -0.71902162]\n",
      " [-0.2780659  -0.15756649 -0.52731511]\n",
      " [-0.81969092 -0.99186894 -0.27165441]]\n",
      "Layer 11 bias:\n",
      "[ 0.21351485 -0.50396314 -0.60744244]\n",
      "Layer 12 weights:\n",
      "[[ 0.20899755  0.188431    0.9039075 ]\n",
      " [ 0.73453118  0.72469553  0.67449315]\n",
      " [-0.43761628  0.50199741 -0.00176604]]\n",
      "Layer 12 bias:\n",
      "[-0.47384312 -0.41409483  0.34247563]\n",
      "Layer 13 weights:\n",
      "[[ 0.85827974  0.26050644  0.99279375]\n",
      " [ 0.26630617  0.42527993 -0.98437894]\n",
      " [ 0.72432677  0.44440174 -0.07706986]]\n",
      "Layer 13 bias:\n",
      "[ 0.81259266 -0.65490135 -0.23438245]\n",
      "Layer 14 weights:\n",
      "[[-0.32600582 -0.14099059  0.7022149 ]\n",
      " [ 0.30027051  0.54585321  0.67737026]\n",
      " [-0.60452063  0.25479585 -0.1465644 ]]\n",
      "Layer 14 bias:\n",
      "[ 0.09187532 -0.93153263  0.82949807]\n",
      "Layer 15 weights:\n",
      "[[-0.07802049  0.78518034  0.81796399]\n",
      " [-0.38031279 -0.9721498   0.93982652]\n",
      " [-0.26769681 -0.99869141  0.82723497]]\n",
      "Layer 15 bias:\n",
      "[ 0.96983261 -0.19004694 -0.27436936]\n",
      "Layer 16 weights:\n",
      "[[ 0.56836158 -0.63261343  0.20244483]\n",
      " [-0.90876047  0.51555323 -0.60564177]\n",
      " [ 0.0925004   0.79601933 -0.84226849]]\n",
      "Layer 16 bias:\n",
      "[-0.39219228 -0.88461593  0.64174938]\n",
      "Layer 17 weights:\n",
      "[[-0.04223624 -0.0633029  -0.89187892]\n",
      " [ 0.47756719  0.55116613 -0.64863496]\n",
      " [ 0.24527797  0.18346612  0.69450946]]\n",
      "Layer 17 bias:\n",
      "[-0.55650148 -0.99207706 -0.06485824]\n",
      "Layer 18 weights:\n",
      "[[-0.39748889  0.16881872 -0.08851347]\n",
      " [ 0.35963021  0.70508191  0.74982946]\n",
      " [ 0.22187427 -0.37280188 -0.6715373 ]]\n",
      "Layer 18 bias:\n",
      "[ 0.07629452 -0.09305782  0.08393797]\n",
      "Layer 19 weights:\n",
      "[[ 0.07315602  0.16119645  0.06284187]\n",
      " [-0.85287028  0.70676101 -0.38580857]\n",
      " [ 0.20085256  0.97611168 -0.04219756]]\n",
      "Layer 19 bias:\n",
      "[ 0.83181074  0.85390894 -0.97626914]\n",
      "Layer 20 weights:\n",
      "[[ 0.09332611  0.54858288 -0.34970708]\n",
      " [-0.19122545  0.9336954  -0.9673755 ]\n",
      " [ 0.25725625  0.661262    0.17482299]]\n",
      "Layer 20 bias:\n",
      "[-0.93241732 -0.81781579 -0.22165913]\n",
      "Layer 21 weights:\n",
      "[[ 0.76171395 -0.09259759  0.02512992]\n",
      " [-0.77192088 -0.31653661  0.89888902]\n",
      " [-0.19789061  0.23023854 -0.79608376]]\n",
      "Layer 21 bias:\n",
      "[-0.37668546  0.84282715  0.72127689]\n",
      "Layer 22 weights:\n",
      "[[-0.63804925  0.34138177 -0.63317894]\n",
      " [-0.20310024 -0.93926575 -0.7620111 ]\n",
      " [ 0.30841294  0.97155075  0.24961694]]\n",
      "Layer 22 bias:\n",
      "[ 0.57131956 -0.48648027  0.63087965]\n",
      "Layer 23 weights:\n",
      "[[ 0.11101438  0.11443556 -0.62925282]\n",
      " [-0.83833369  0.81172015  0.37005955]\n",
      " [-0.68288991  0.75208239  0.47909997]]\n",
      "Layer 23 bias:\n",
      "[-0.22936561  0.34149771 -0.15579547]\n",
      "Layer 24 weights:\n",
      "[[ 0.39669916 -0.73288457 -0.21538437]\n",
      " [ 0.15164198 -0.24233378 -0.7001719 ]\n",
      " [ 0.92853424  0.82661268  0.96074571]]\n",
      "Layer 24 bias:\n",
      "[0.28035122 0.42092809 0.83142552]\n",
      "Layer 25 weights:\n",
      "[[ 0.09671779 -0.12517287  0.1166779 ]\n",
      " [ 0.3213752   0.40228999  0.33406393]\n",
      " [ 0.37184415 -0.77764378  0.12140295]]\n",
      "Layer 25 bias:\n",
      "[-0.94781249  0.87108809 -0.81741833]\n",
      "Layer 26 weights:\n",
      "[[-0.47884978  0.80867635  0.04036849]\n",
      " [-1.11134796  0.59463244  0.36112353]\n",
      " [-0.67632998 -0.56671283  0.49569751]]\n",
      "Layer 26 bias:\n",
      "[ 0.14444695  0.10312196 -0.83345851]\n",
      "Layer 27 weights:\n",
      "[[ 0.69581258  0.50586944 -0.96834487]\n",
      " [-1.07041915 -0.35764253 -0.61594017]\n",
      " [-0.21659356  0.79271223  0.74534188]]\n",
      "Layer 27 bias:\n",
      "[0.06538256 0.36249442 0.603774  ]\n",
      "Layer 28 weights:\n",
      "[[ 0.75887268 -0.25837588 -0.7857883 ]\n",
      " [ 0.31733201  0.65501039  0.09359379]\n",
      " [-0.64907728  0.07766857  0.45316614]]\n",
      "Layer 28 bias:\n",
      "[-0.45701711 -0.15716434  0.44452645]\n",
      "Accuracy: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    z = z - np.max(z)\n",
    "    e = np.exp(z)\n",
    "    return e / np.sum(e)\n",
    "\n",
    "def one_hot(y_i, K):\n",
    "    v = np.zeros(K, dtype=float)\n",
    "    v[y_i] = 1.0\n",
    "    return v\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class HiddenLayer():\n",
    "    \n",
    "    def __init__(self, w_size, b_size):\n",
    "        self.W = np.random.uniform(-1, 1, size=w_size)\n",
    "        self.b = np.random.uniform(-1, 1, size=b_size)\n",
    "        self.z = 0                # pre-activation value\n",
    "        self.a = 0                # activation value \n",
    "        self.prev_a = 0\n",
    "\n",
    "    def set_z(self, z):\n",
    "        self.z = z\n",
    "        \n",
    "    def set_a(self, a):\n",
    "        self.a = a\n",
    "    \n",
    "    def set_W(self, W):\n",
    "        self.W = W    \n",
    "        \n",
    "    def set_b(self, b):\n",
    "        self.b = b\n",
    "        \n",
    "    def set_prev_a(self, value):\n",
    "        self.prev_a = value\n",
    "        \n",
    "        \n",
    "    def update_W(self, eta, dW):\n",
    "        self.W -= eta * dW\n",
    "        \n",
    "    def update_b(self, eta, db):\n",
    "        self.b -= eta * db\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def forward_probs(x, HiddenLayers, W2, b2):\n",
    "    prev_a = x\n",
    "    for layer in HiddenLayers:\n",
    "        layer.set_prev_a(prev_a)\n",
    "        layer.set_z(layer.W @ prev_a + layer.b)            # pre-activations hidden (2,)\n",
    "        a = (tanh(layer.z))                                # activations hidden (2,)\n",
    "        layer.set_a(a)\n",
    "        prev_a = a\n",
    "    z2 = W2 @ prev_a + b2          # logits, shape (K,)\n",
    "    p = softmax(z2)           # probs, shape (K,)\n",
    "    return p, prev_a               # also return last hidden activation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "K = len(np.unique(y))                 # number of classes\n",
    "input_dim = X.shape[1]                # = 4\n",
    "H = 3                                 # hidden width (it is up to personal prefeerence)\n",
    "NLayers = 28\n",
    "\n",
    "HiddenLayers = []\n",
    "# First hidden layer: (H, input_dim)\n",
    "HiddenLayers.append(HiddenLayer(w_size=(H, input_dim), b_size=(H,)))\n",
    "\n",
    "# Remaining hidden layers: (H, H)\n",
    "for _ in range(NLayers - 1):\n",
    "    HiddenLayers.append(HiddenLayer(w_size=(H, H), b_size=(H,)))\n",
    "\n",
    "# Output layer: (K, H) and (K,)\n",
    "W2 = np.random.uniform(-1, 1, size=(K, H))\n",
    "b2 = np.zeros(K)                \n",
    "\n",
    "\n",
    "\n",
    "eta = 0.1                                   # learning rate for the update step\n",
    "tanh = np.tanh\n",
    "tanhp = lambda z: 1.0 - np.tanh(z)**2       # derivative of tanh\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    wrong = False\n",
    "    \n",
    "    for index in range(len(X)):\n",
    "        \n",
    "        x_i = X[index]\n",
    "        y_i = y[index]\n",
    "\n",
    "        p, a_last = forward_probs(x_i, HiddenLayers, W2, b2)  # p: (K,), a_last: (H,)\n",
    "        L = -np.log(p[y_i])   \n",
    "\n",
    "        if L > 1e-12:\n",
    "            \n",
    "            wrong = True\n",
    "\n",
    "            # output layer calculations\n",
    "            y_one = one_hot(y_i, K)               # (K,)\n",
    "            delta_out = p - y_one                 # (K,)\n",
    "            dW2 = np.outer(delta_out, a_last)     # (K, H)\n",
    "            db2 = delta_out                       # (K,)\n",
    "\n",
    "            # save for hidden backprop (use current W2, not yet updated)\n",
    "            next_delta = delta_out                # (K,)\n",
    "            next_W = W2                           # (K, H)\n",
    "\n",
    "            # updating hidden layers from reversed order\n",
    "            for layer in reversed(HiddenLayers):\n",
    "                g = next_W.T @ next_delta         # (H_prev,) where H_prev = layer.a.size\n",
    "                delta = g * (1.0 - layer.a**2)    # tanh'(z) = 1 - a^2\n",
    "\n",
    "                dW = np.outer(delta, layer.prev_a)\n",
    "                db = delta\n",
    "\n",
    "                layer.update_W(eta, dW)\n",
    "                layer.update_b(eta, db)\n",
    "\n",
    "                next_delta = delta                # (H_prev,)\n",
    "                next_W = layer.W                  # (H_prev, input_dim_prev)\n",
    "\n",
    "            # Updatye Output Layer\n",
    "            W2 -= eta * dW2\n",
    "            b2 -= eta * db2\n",
    "                    \n",
    "    if not wrong:            \n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "def forward(x, HiddenLayers, W2, b2):\n",
    "    x_ = x\n",
    "    for layer in HiddenLayers:\n",
    "        z = layer.W @ x_ + layer.b\n",
    "        a = np.tanh(z)\n",
    "        x_ = a\n",
    "    z2 = W2 @ x_ + b2\n",
    "    y_hat = np.tanh(z2)\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "\n",
    "print(\"Final W2:\", W2, \"Final b2:\", b2)\n",
    "for i, layer in enumerate(HiddenLayers, 1):\n",
    "    print(f\"Layer {i} weights:\\n{layer.W}\\nLayer {i} bias:\\n{layer.b}\")\n",
    "\n",
    "probs = np.array([forward_probs(x_i, HiddenLayers, W2, b2)[0] for x_i in X])  # (N, K)\n",
    "y_pred = np.argmax(probs, axis=1)\n",
    "print(\"Accuracy:\", accuracy_score(y, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
